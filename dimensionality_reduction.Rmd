---
title: "dimensionality_reduction"
author: "Carlos Gomez Sanchez"
date: "14 de abril de 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(dplyr)
library(ggplot2)
library(kmed)
library(Rtsne)
```


# Métodos de reducción de dimensionalidad

```{r}
housesTrain <- read.csv('base_train.csv')
```

## PCA


```{r}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```


```{r}
train <- housesTrain %>% select(-Price)
```

```{r}
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat, price_label_high)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=price_label_high)) + geom_point() + labs(title='PCA: price label')
```

```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=Regionname)) + geom_point() + labs(title='PCA: Regionname')
```
```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=sell_rate_cat)) + geom_point() + labs(title='PCA: sell_rate_cat')
```


## MDS
Usaremos la distancia euclídea para realizar el MDS (debería ser muy parecido a PCA). Al tener bastantes datos, vamos a muestrear el train primero.

```{r}
train_sample <- housesTrain[sample(nrow(housesNum), 2500),]
train_num_sample <- train_sample %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
```

```{r}
# Distancia euclídea
d <- dist(train_num_sample) 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```

```{r}
df_fit <- data.frame(fit$points)
# No estoy seguro de hacer el join bien, se supone que es por indice
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='MDS: price_label_high')
```
Con otras distancias:

```{r}
# Distancia euclídea
d <- dist(train_num_sample, method = 'manhattan') 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```

```{r}
df_fit <- data.frame(fit$points)
# No estoy seguro de hacer el join bien, se supone que es por indice
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='MDS: price_label_high')
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname')
```

## tSNE
Probamos tsne sobre nuestras variables numéricas escaladas, para que valores extremos no nos den problemas. El parámetro que se suele tocar es el perplexity, que es algo parecido al número de vecinos más cercanos cuando se comparan las distribuciones que utiliza tSNE.
Nota: hay varios duplicados en housesNum, algo un poco raro...
Fuente: https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

```{r}
houses_num_unique <- unique(housesNum)
tsne <- Rtsne(houses_num_unique, dims = 2, perplexity=50, verbose=TRUE, max_iter = 500)
```

```{r}
df_tsne <- data.frame(tsne$Y)
df_train_no_dup <- housesTrain[!duplicated(housesTrain[c("sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std")]),]
df_train_no_dup$X1 <- df_tsne$X1
df_train_no_dup$X2 <- df_tsne$X2
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='tSNE: price_label_high')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='tSNE: Regionname')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=sell_rate_cat)) + geom_point() + labs(title='tSNE: sell_rate_cat')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=rooms_cat)) + geom_point() + labs(title='tSNE: rooms_cat')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=year_built_cat)) + geom_point() + labs(title='tSNE: year_built_cat')
```

