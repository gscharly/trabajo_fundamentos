---
title: "Métodos de análisis de datos"
author: 
- name: Paula Santamaría Villaverde
- name: Manuel Jesús Pertejo Lope
- name: Carlos Gómez Sánchez
date: "19 de diciembre de 2019"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      section_divs: true
    theme: "sandstone"
    highlight: "zenburn"
    code_folding: "hide"
---

```{r setup, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(error = TRUE)
```

```{r imagenMelbourne, echo=FALSE}
knitr::include_graphics("melbourneModif.png")
```

```{r librerías, message=FALSE}
library(caret)
library(dplyr)
library(RColorBrewer)
library(Hmisc)
library(VIM)
library(tidyr)
library(knitr)
library(kableExtra)
library(Hmisc)
library(gmodels)
library(VIM)
library(mice)
library(wesanderson)
library(ggplot2)
library(GGally)
library(gridExtra)
library(car)
library(moments)
library(mplot)
library(glmnet)
library(plyr)
library(tidyverse)
library(leaps)
library(broom)
library(ggfortify)
library(olsrr)
library(nortest)
library(Metrics)
library(ggmap)
library(reshape2)
```
<br>

# Definición de objetivos

<br>

*Objetivo general*

- Predecir el precio de venta de una vivienda en la ciudad de Melbourne.

*Objetivos específicos*

- Realizar un anáilis exploratorio de datos univariante para las variables cualitativas y cuantitativas.
- Realizar un análisis explotratorio multivariante de todas las variables con respecto a la variable Price.
- Transformar las variables necesarias para su uso adecuado en un modelo de regresión lineal.
- Detectar y tratar los datos faltantes.
- Seleccionar las variables adecuadas al modelo.
- Ajustar el modelo de regresión lineal múltiple.

Enlace a Github: https://github.com/gscharly/trabajo_fundamentos

# Lectura y descripción del dataset

<br>
```{r Lectura, echo=FALSE}
houses <- read.csv('./datasets/melb_data.csv')
cols <- ncol(houses)
rows <- nrow(houses)
```

```{r}
kable(head(houses, 5)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = TRUE)
```

El presente dataset contiene información acerca del sector inmobiliario en la ciudad de Melbourne. Está compuesto por un total de 21 variables y 13580 observaciones.
A continuación se muestra una tabla con la descripción de cada variable.

```{r Descripción, echo=FALSE}
dfcol <- data.frame(
  'Variable' = c('Rooms', 'Price', 'Method', 'Type', 'SellerG', 'Date', 'Distance', 'Regionname', 'Propertycount', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'CouncilArea', 'Suburb', 'Address', 'Postcode', 'YearBuilt', 'Lattitude', 'Longtitude'),
  'Descripción' = c('Number of rooms', 'Price in dollars', 'S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available', 'br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential', 'Real Estate Agent', ' Date sold', 'Distance from CBD', 'General Region (West, North West, North, North east ...etc)', 'Number of properties that exist in the suburb', 'Scraped of Bedrooms (from different source)', 'Number of Bathrooms', 'Number of carspots', 'Land Size', 'Building Size', 'Governing council for the area', 'Neighborhood', 'House direction', 'Postcode', 'Construction year', 'Lattitude', 'Longtitude')
)

dfcol %>%
  kable() %>%
  kable_styling(bootstrap_options = c('hover'), position = 'center')
```

Se muestra a modo informativo las casas por cada región:

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10}
melbourne_map <- get_map(location = "melbourne", zoom = 10)
houses_map <- houses[,c("Regionname", "Lattitude", "Longtitude")]
houses_map$region_colours <- revalue(houses_map$Regionname, c("Southern Metropolitan" = "#1B9E77", "Western Metropolitan" = "#D95F02", "Northern Metropolitan" = "#7570B3",
                                                      "Eastern Metropolitan" = "#E7298A", "South-Eastern Metropolitan" = "#66A61E", "Eastern Victoria" = "#A6761D", 
                                                      "Northern Victoria" = "#1DA632", "Western Victoria" = "#1B849E"))
  
ggmap(melbourne_map) + geom_point(aes(x = houses_map$Longtitude, y = houses_map$Lattitude), colour=houses_map$region_colours , data = houses, alpha=0.5, size = 0.75)
```


<br>

# División de los datos

<br>

Separaremos el archivo en dos subconjuntos: datos de entrenamiento y datos de test. El primero de los grupos se llamará *housesTrain* y el segundo *housesTest*.

```{r DivisiónDatos}
set.seed(10)
trainIndex <- createDataPartition(houses$Price, p = .8, list = FALSE, times = 1)
housesTrain <- houses[ trainIndex,]
housesTest <- houses[-trainIndex,]
```
```{r DivisiónDatosTabla}
obsTrain <- housesTrain %>% nrow
obsTest <- housesTest %>% nrow
dfDivision <- data.frame(
  Subconjuntos = c('Train + Validation', 'Test'),
  Observaciones = c(obsTrain, obsTest))

dfDivision %>% kable() %>% kable_styling(bootstrap_options = c('hover'), position = 'center')
```


Ahora, el subconjunto *housesTrain*, con el que trabajaremos inicialmente, está formado por 10865 observaciones. Más adelante, subdividiremos este mismo archivo para testar el modelo de regresión lineal múltiple. 

<br>

# Análisis exploratorio de datos

<br>

## Resumen numérico variables cualitativas

<br>

En el dataset contamos con ocho variables categóricas: Suburb, Address, Type, Method, SellerG, Date, CouncilArea y Regionname. A continuación se muestra la esctructura de cada una de ellas. 

```{r ResumenCualitativas}
VarCualitativas <- housesTrain %>% select_if(Negate(is.numeric))
str(VarCualitativas)
```
<br>

### Tablas de frecuencias

<br>

Las siguientes tablas muestran la frecuencia para cada una de las variables cualitativas del dataset a excepción de la variable *Address*. Se decide no emplear esta variable ya que se cuenta con una dirección para cada vivienda y creemos que es mejor, para el anáilisis del conjunto de datos, emplear las otras variables de localización como son *Regionname*, *CouncilArea* y *Suburb*.

Además de esto, respecto a la variable *Date* nos parece más relevante mostrar las frecuencias por meses del año por lo que la separamos en tres variables distintas: *day*, *mounth* y *year*.

```{r DateSeparate}
VarCualitativas <- VarCualitativas %>% separate(Date, c('Day','Month', 'Year'), sep = '/')
```

```{r FrecuenciasCualitativas}
tabla_frecuencias <- function(df, columna1, columna2 = 'Frecuencia'){
    knitr::kable(df, col.names = c(columna1, columna2)) %>%
      kable_styling(bootstrap_options = 'hover', position = 'center',full_width = T)
}
```

```{r}
tabla_frecuencias(df = data.frame(table(VarCualitativas$Type)), columna1 = 'Type')
tabla_frecuencias(df = data.frame(table(VarCualitativas$Method)), columna1 = 'Method')
tabla_frecuencias(df = data.frame(table(VarCualitativas$Regionname)), columna1 = 'Regionname')
tabla_frecuencias(df = data.frame(table(VarCualitativas$Suburb)), columna1 = 'Suburb')%>%
      scroll_box(width = "100%", height = "350px")
tabla_frecuencias(df = data.frame(table(VarCualitativas$Year)), columna1 = 'Year')
tabla_frecuencias(df = data.frame(table(VarCualitativas$Month)), columna1 = 'Month')
```

<br>

### Gráficas de barras

<br>
En este apartado se muestran las gráficas de barras correspondientes a las frecuencias anteriores excepto para la variable *Suburb* ya que presenta 314 niveles y no ofrecería una gráfica legible.

```{r paleta32, echo=FALSE}
#Paleta creada a partir de Brewer Dark2
palette34 = c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D", "#666666","#1DA632","#9E1B84","#1B849E","#7CD902","#A61E22","#1EA6A2","#8AE729", "#AEB370","#E7E529","#70B397","#29E786","#292BE7","#E7292B","#B3708D","#AFE602", "#02E6AB","#02AFE6","#92A61D","#771B9E","#1DA676","#1D4DA6","#02D95F","#02D9CB","#D9CB02","#B37570","#B39670")
colorDensidad <- brewer.pal(n = 4, name = 'Dark2')[4]
colorNormal <- wes_palettes$GrandBudapest2[4]
colorBox <- brewer.pal(n = 4, name = 'Dark2')[1]
```

```{r GráficaBarras}
plot_barras <-function(df = VarCualitativas, x, bar, xlab, ylab = 'Frecuencia'){
  ggplot(data=df, aes(x=x, fill = bar)) + 
    geom_bar(stat="count",position=position_dodge()) +
    scale_fill_manual(values=palette34) +
    labs(x= xlab, y = ylab, fill=NULL) + 
    theme(axis.text.x=element_blank())
}

xlab_barras <- c('Tipo de vivienda', 'Método de venta', 'CouncilArea', 'Región')
var_barras <- colnames((VarCualitativas)[c(3,4,9,10)])
i <- 1

for (var in var_barras){
  xaes_var <- VarCualitativas %>% select(var)
  print(plot_barras(x = unlist(xaes_var),bar= unlist(xaes_var), xlab =  var))
  i <- i + 1
}

plot_barras(x = VarCualitativas$Month,bar= VarCualitativas$Year, xlab = 'Month')
```

Respecto a la variable *Type*, observamos que el factor más representado en el dataset es el h (house,cottage,villa, semi, terrace) y que no hay viviendas de tipo bedroom.

Siguiendo con la variable *Method*, podemos ver como está muy descompensada con factores que son practicamente nulos. Esta variable hace referencia a la forma en la que se ha vendido una casa. Es una variable muy propia del ámbito de la cual no sabemos leer bien su información y medir su relevancia. Es por este motivo por el que, si resulta ser una variable útil en el modelo, deberíamos informarnos de ella a través de alguna persona experta.

Atendiendo a la salida, tanto numérica como gráfica, observamos que la variable *CouncilArea* presenta una categoría vacía y otra llamada Unavailable. Los datos de dichas categorías (10%) los consideramos faltantes y serán imputados en el apartado dedicado a esto.

Por otra parte, de las 8 regiones de Melbourne, hay 4 que representan el 95% de casas vendidas (Southern Metropolitan, Northern Metropolitan, Western Metropolitan y Eastern Metropolitan en ese orden).

Por último, atendiendo a *Month* y *Year*, en este dataset solo están las viviendas vendidas en los años 2016 y 2017 donde observamos una frecuencia similar en ambos años pero con diferencias según el mes.

```{r UnavailableCouncil}
housesTrain$CouncilArea[which(housesTrain$CouncilArea == '')] <- 'Unavailable'
```


```{r}
water_councils <- c('Wyndham', 'Hobsons Bay', 'Port Phillip', 'Bayside', 'Kingston', 'Frankston', 'Stonnington')
housesTrain$may_have_water <- factor(ifelse(housesTrain$CouncilArea %in% water_councils, TRUE, FALSE))

housesTrain %>% ggplot(aes(x=log_price, color=may_have_water)) + geom_density()
```


<br>

## Resumen numérico variables cuantitativas

<br>

```{r ResumenCuantitativas}
VarCuantitativas <- housesTrain %>% select_if(is.numeric)
VarCuantitativas %>% describe()
```

<br>

### Histogramas, densidad, asimetría y apuntamiento

<br>

A continuación obserbamos las distribuciones para cada variable cuantitativa a través de sus histogramas.

```{r HistogramaDensidad, message=FALSE, warning=FALSE}
plot_histograma <- function(df = housesTrain, dat1, dat2, xlab, ylab = "Densidad", xaes){
  ggplot(df, aes(x=xaes, label = curt), na.omit = TRUE) + geom_histogram(aes(y=..density..), colour="black", fill="white") + geom_density(alpha=.2, fill= colorDensidad) + theme_light () +
  stat_function(fun=dnorm, color=colorNormal, lwd = 1.3,
  args=list(mean=mean(xaes), sd=sd(xaes)))  + labs(x= xlab, y = ylab) + 
  annotate(geom = 'text', label = dat1, x = Inf, y = Inf, hjust = 1.7, vjust = 2, parse = TRUE) +
  annotate(geom = 'text', label = dat2, x = Inf, y = Inf, hjust = 1.7, vjust = 3.5, parse = TRUE)
}

plot_box <- function(df = VarCuantitativas, ylab, yaes){
  ggplot(df, aes(y=yaes)) + geom_boxplot(na.rm = TRUE, colour = colorBox) + 
    theme_light () + labs(y = ylab)
}


variables_not_geo <- colnames(VarCuantitativas)
for (var in variables_not_geo){
  aes_var <- VarCuantitativas %>% select(var)
  curt <- round(kurtosis(VarCuantitativas[var], na.rm = TRUE), 2)
  asim <- round(skewness(VarCuantitativas[var], na.rm = TRUE), 2)
  dat1 <- paste0("Curtosis == ",curt)
  dat2 <- paste0("Asimetría == ",asim)
  p1<- plot_histograma(dat1 = dat1, dat2 = dat2, xlab = var, xaes = unlist(aes_var))
  p2 <- plot_box(ylab = var, yaes = unlist(aes_var))
  grid.arrange(p1, p2, nrow=1)
}
```

En primer lugar, atendiendo a las distribuciones de las variables *Rooms*, *Bedroom*, *Bathroom*, *Car* y *YearBuilt* se concentran en torno a unos pocos valores enteros, por lo que previsiblemente las convertiremos a variables categóricas. De estas variables, observando su Boxplot y los outliers, vemos que son más las viviendas que tienen un número bajo o medio de habitaciones.

Respecto a esta última variable, *YearBuilt*, vemos que hay un outlier que se corresponde con el año 1196 según la tabla describe. Este dato lo tomamos como un dato erróneo (faltante) y se establece que cualquier casa que tenga *YearBuilt* por debajo del año de fundación de la ciudad (1850) será eliminada.

Por otro lado, debido a que tenemos variables que representan habitaciones dentro de una casa (*Rooms*, *Bedroom2*, *Bathroom*), vamos a comprobar primero si *Rooms* incluye a las demás. 

```{r ComprobacionRooms, echo=FALSE}
housesTrain %>% select(Rooms, Bedroom2, Bathroom) %>%
  filter(Rooms < Bedroom2+Bathroom) %>%
  nrow

housesTrain %>% select(Rooms, Bedroom2, Bathroom) %>%
  filter(Rooms <= Bedroom2) %>%
  nrow
```

*Rooms* no incluye la suma de *Bedroom2* y *Bathroom*, ya que el 98% de las veces el valor es menor a la suma de ambos.

Por último,observamos que son varias las distribuciones con una fuerte curtósis y asimetría (*Price*, *Distance*, *Landsize*, *BuildingArea*, *YearBuilt*) por lo estas variables son candidatas a recibir una transformación. Las tres últimas variables tienen valores faltantes, por lo que se analizarán en un apartado diferente.

__Price__

Para la variable objetivo *Price*, observamos que tiene valores muy extremos, lo que hace que la distribución quede muy sesgada a la izquierda. Esta variable es claramente candidata a una transformación del tipo logaritmo, ya que además no presenta valores iguales a 0.

```{r transformacionPrice, message=FALSE}
housesTrain <- housesTrain %>% mutate(log_price = log10(Price))

p1 <- housesTrain %>%
  ggplot(aes(x=Price)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(fill= colorDensidad , alpha=.2)

p2 <- housesTrain %>%
  ggplot(aes(x=log_price)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(fill= colorDensidad , alpha=.2)
  
grid.arrange(p1, p2, nrow=1)
```

```{r transformacionPriceBox, message=FALSE}
p1 <- housesTrain %>%
  ggplot(aes(y=Price)) +
  geom_boxplot()

p2 <- housesTrain %>%
  ggplot(aes(y=log_price)) +
  geom_boxplot()
  
grid.arrange(p1, p2, nrow=1)
```

Comprobamos si sigue una distribución normal con un diagrama QQ y un test de normalidad. El diagrama QQ se asemeja al de una distribución normal; sin embargo el p-valor obtenido en el test de normalidad es muy bajo, con lo que se puede rechazar la hipótesis nula de normalidad de la variable *log_price*.

```{r normalidadPrice}
housesTrain %>% mutate(log_price = log10(Price)) %>%
ggplot(aes(sample=log_price)) + stat_qq() + stat_qq_line() + labs(title='qq-plot log_price')

lillie.test(housesTrain$log_price)
```

__Distance__

La variable *Distance* también podría verse favorecida por una transformación, ya que tiene valores extremos en la cola de su distribución. Sin embargo, esta variable presenta valores iguales a 0 (casas que están en el centro) por lo que sería más conveniente una transformación del tipo raíz cuadrada.

```{r transformacionDistance, message=FALSE}
housesTrain <- housesTrain %>% mutate(sqrt_distance = sqrt(Distance))

p1 <- housesTrain %>%
  ggplot(aes(x=Distance)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(fill= colorDensidad , alpha=.2)

p2 <- housesTrain %>%
  ggplot(aes(x=sqrt_distance)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(fill= colorDensidad , alpha=.2)
  
grid.arrange(p1, p2, nrow=1)
```

```{r transformacionDistanceBox, message=FALSE}
p1 <- housesTrain %>%
  ggplot(aes(y=Distance)) +
  geom_boxplot()

p2 <- housesTrain %>%
  ggplot(aes(y=sqrt_distance)) +
  geom_boxplot()
  
grid.arrange(p1, p2, nrow=1)
```

Con esta transformación conseguimos reducir los valores extremos de la distribución, pero no conseguimos una distribución normal de la variable a simple vista.


__Lattitude__ y __Longtitude__

A priori, las distribuciones de estas dos variables no son normales. Se comprueba mediante tests de normalidad:

```{r normalidadLattitude}
housesTrain %>% 
ggplot(aes(sample=Lattitude)) + stat_qq() + stat_qq_line() + labs(title='qq-plot Lattitude')
lillie.test(housesTrain$Lattitude)
```
```{r normalidadLongtitude}
housesTrain %>% 
ggplot(aes(sample=Longtitude)) + stat_qq() + stat_qq_line() + labs(title='qq-plot Longtitude')
lillie.test(housesTrain$Longtitude)
```

<br>

## Análisis multivariante

<br>

Siguiendo con nuestro objetivo principal, es decir predecir el precio de una vivienda en base a sus características, vamos a analizar la relación de las variables del dataset con respecto a la variable *Price*. Aunque se verá más adelante, se tomará el logaritmo del precio para su comparación con las variables categóricas, por presentar una distribución más cercana a una distribución normal.

<br>

### Variables cualitativas

<br>

Empezaremos examinando la variable Price en función de las variables categóricas que tenemos. 

__Regionname__

Distribución de precio en función de la región. Western Victoria parece que tiene unos precios más bajos, mientras que Southern Metropolitan y Eastern Metropolitan parece que tienen mayores precios.

```{r RegionPrice}
housesTrain %>% 
  ggplot(aes(x=log_price, colour=Regionname)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>%
  ggplot(aes(y=log_price, fill=Regionname)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

__Type__

Recordemos los distintos tipos de casas que hay:

* h: houses, cottage, villa, semi, terrace
* t: townhouse
* u: unit

Observando las gráficas podemos apreciar como las viviendas de tipo u (unit) y t townhouse tienen menor dispersión en el precio y son más baratas. Sin embargo, para las viviendas tipo h cuenta con precios más caros y la varianza es mayor, lo cual tiene sentido ya que este tipo engloba tipos de viviendas más diversos.
Presumiblemente, parece que el tipo de vivienda sí discrimina con respecto al precio de la vivienda.

```{r TypePrice}
housesTrain %>%
  ggplot(aes(x=log_price, colour=Type)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>% 
  ggplot(aes(y=log_price, fill=Type)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

Como comprobación, se compara la variable numérica *Rooms* con *Type*, para ver cuantas habitaciones tienen los distintos tipos de casa. Podemos observar que las casas de tipo u son las que menos habitaciones tienen (1, 2 o 3), seguidas de las casas tipo t (2, 3 y 4) y terminando con las h, que tienen todos los posibles valores de habitaciones.
```{r roomsType}
housesTrain %>% ggplot(aes(x=Rooms, fill=Type)) + geom_histogram()
```


__Method__

Los métodos de venta son los siguientes:

* S - property sold
* SP - property sold prior
* PI - property passed in
* VB - vendor bid
* SA - sold after auction

La gráfica tomando el logaritmo permite una visualización mejor donde vemos precios con bastante variabilidad para todos los métodos de venta excepto para SA. Estas viviendas se han vendido después de una subasta, lo cual puede explicar que el precio esté más regulado. Atendiendo a los Boxplots, se observan rangos intercuantiles parecidos para algunos métodos (por ejemplo: S y SA) y diferentes para otros (por ejemplo: SP y VB). En este sentido queda por ver si el método resulta útil en el modelo a la hora de diferenciar el precio.

```{r PriceMethod}
housesTrain %>% 
  ggplot(aes(x=log_price, colour=Method)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>%
  ggplot(aes(y=log_price, fill=Method)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

__CouncilArea__

Por el número de niveles que tiene esta variable, resulta dificil apreciar algunas diferencias con respecto al precio, pero sí observamos diferencias en cuanto a la variabilidad del precio. También llama la atención algunos de los outliers, donde hay viviendas con precios muy altos y otras muy bajos.

```{r fig.width=15, fig.height=4}
housesTrain %>% 
  ggplot(aes(y=log_price, fill=CouncilArea)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

<br>

### Variables cuantitativas

<br>

A continuación, examinaremos las distintas variables numéricas, y su relación con la que será nuestra columna objetivo (Price).
Lo primero que hacemos es comprobar las correlaciones de las variables numéricas.

```{r heatmapCorr, fig.width=15, fig.height=10, message=FALSE}
numeric_cols <- c("Rooms", "Distance", "sqrt_distance", "Bedroom2", "Bathroom", "Car", "Lattitude", "Longtitude", "YearBuilt", "Landsize", "BuildingArea", "Propertycount", 'log_price')
cormat <- round(cor(na.omit(housesTrain[,numeric_cols])), 2)

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  # Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

ggheatmap + geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```
Se puede observar lo siguiente:

- La variable de salida tiene alta correlación positiva con las variables *Rooms*, *Bathroom* y *Bedroom*. Además, estas variables están muy correladas entre ellas (parece lógico pensar que el número de baños y dormitorios esté correlado con el número de habitaciones). Sin embargo, ya se ha comentado que, debido a las distribuciones de estas variables, se van a convertir a categóricas.
- Los valores informados de *BuildingArea* tienen una alta correlación lineal con la variable objetivo.
- El año de construcción tiene una correlación negativa con el precio, es decir, las casas nuevas pueden ser más baratas que las antiguas. De nuevo, tenemos el mismo problema con la distribución de esta variable, además de su gran cantidad de valores faltantes.

Esta información puede ser completada con las distribuciones de las variables:

```{r message=FALSE}

numeric_cols <- c("Rooms", "sqrt_distance", "Bedroom2", "Bathroom", "Car", "log_price")

housesTrain %>% select(numeric_cols) %>%
  na.omit() %>%
  ggpairs(columns=1:6)

numeric_cols2 <- c("Landsize", "BuildingArea", "YearBuilt", 'Lattitude', 'Longtitude', "log_price")

housesTrain %>% select(numeric_cols2) %>%
  na.omit() %>%
  ggpairs(columns=1:6)

```


Aquí podemos ver el problema de las distribuciones de *Rooms*, *Bedroom2* y *Bathroom*, además de su relación con el precio.

Por otra parte, se observa que la variable *sqrt_distance* está debilmente correlada con el precio. Sin embargo, basándonos en el dominio, creemos que la distancia al centro de la ciudad sí puede ser una variable interesante a ver, posteriormente, qué papel juega en el modelo.

A continuación, se examinan más detenidamente las variables respecto al precio de venta de las viviendas, y respecto a otras variables categóricas.

__Rooms__

Rooms se concetra entre 2, 3 y 4 habitaciones. Parece que los precios más altos se alcanzan con 3, 4 y 5 habitaciones, pero los rangos son similares.

```{r RoomsPrice}
housesTrain %>%
  select(Rooms, log_price) %>%
  ggpairs
```

Dada la gáfica de distribución de la variable *Rooms* se observa como los valores se concentran en unos puntos. A la hora de discretizarla, probaremos a categorizarla en 3 y 4 grupos para observar cómo se comporta con respecto a *Price*.

Podemos probar a categorizar la variable Rooms, intentando que las categorías tengan aproximadamente el mismo número de registros. A la hora de categorizar, a pesar de intentar que los grupos estén equilibrados, siempre sería aconsejable el criterio de un experto del dominio.


```{r Rooms3Grupos}
housesTrain %>% mutate(rooms_cat = cut(housesTrain$Rooms, breaks = c(1,3,4,10), labels = c("De 1 a 2", "3", "De 4 a 10"), include.lowest = TRUE, right = FALSE)) %>% select(rooms_cat) %>%
  table()

housesTrain %>% mutate(rooms_cat = cut(housesTrain$Rooms, breaks = c(1,3,4,10), labels = c("De 1 a 2", "3", "De 4 a 10"), include.lowest = TRUE, right = FALSE)) %>%
  ggplot(aes(x=log_price, colour=rooms_cat)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>%
  mutate(rooms_cat = cut(housesTrain$Rooms, breaks = c(1,3,4,10), labels = c("De 1 a 2", "3", "De 4 a 10"), include.lowest = TRUE, right = FALSE)) %>% 
  ggplot(aes(y=log_price, fill=rooms_cat)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

```{r Rooms4Grupos}
housesTrain %>%
  mutate(rooms_cat = cut2(Rooms, g=5)) %>%
  select(rooms_cat) %>%
  table()

housesTrain %>% 
  mutate(rooms_cat = cut2(Rooms, g=5)) %>%
  ggplot(aes(x=log_price, colour=rooms_cat)) +
  geom_density() + scale_fill_manual(values=palette34)
```

En ambas categorizaciones se observa cómo son más numerosas las viviendas con 1, 2 o 3 habitaciones y cómo la cola derecha la la distribución es mayor cuanto mayor es el número de habitaciones. Nos centraremos en la división en 3 grupos, que corresponderían con casas Pequeñas, Medianas y Grandes.


__Bathrooms__

De nuevo, se categoriza la variable, esta vez en dos grupos: casas con hasta 1 baño, y casas con 2 o más. Se observa que esta distinción sí permite diferenciar en cierta medida el precio: casas con más baños son más caras.
```{r}
housesTrain %>%
  select(Bathroom, log_price) %>%
  ggpairs
```


```{r Bath2Grupos}
housesTrain %>% mutate(bath_cat = cut2(housesTrain$Bathroom, g=2)) %>% select(bath_cat) %>%
  table()

housesTrain %>% mutate(bath_cat = cut2(housesTrain$Bathroom, g=2))  %>%
  ggplot(aes(x=log_price, colour=bath_cat)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>%
  mutate(bath_cat = cut2(housesTrain$Bathroom, g=2)) %>% 
  ggplot(aes(y=log_price, fill=bath_cat)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

__Bedroom2__

Hay 20 dormitorios en una casa que, buscando en Maps, es muy pequeña. Se elimina ese registro al ser solo uno. Dividiendo en 3 grupos parecidos, se observa influencia en el precio.

```{r}
housesTrain %>%
  select(Bedroom2, log_price) %>%
  ggpairs
```

```{r}
housesTrain %>%
  filter(Bedroom2!=20) %>%
  mutate(bed_cat = cut2(Bedroom2, g=3)) %>% select(bed_cat) %>%
  table()

housesTrain %>%
  filter(Bedroom2!=20) %>%
  mutate(bed_cat = cut2(Bedroom2, g=3))  %>%
  ggplot(aes(x=log_price, colour=bed_cat)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>%
  filter(Bedroom2!=20) %>%
  mutate(bed_cat = cut2(Bedroom2, g=3)) %>% 
  ggplot(aes(y=log_price, fill=bed_cat)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```


__Car__

Los valores de *Car* se concentran en 1 o 2 por lo que, al igual que las anteriores, es candidata a discretización.

Por las gráficas obtenidas tras la categorización, parece que el número de plazas podría ayudar a diferenciar el precio.

```{r CarPrice}
housesTrain %>%
  select('Car', 'Price') %>%
  na.omit() %>%
  ggpairs
```

```{r Car2Grupos}
housesTrain %>%
  mutate(car_cat = cut(Car, breaks = c(0,2,9), labels = c("Hasta 1", "Más de 2"), include.lowest = TRUE, right = FALSE)) %>%
  select(car_cat) %>%
  table()

housesTrain %>% 
  na.omit() %>%
  mutate(car_cat = cut(Car, breaks = c(0,2,9), labels = c("Hasta 1", "Más de 2"), include.lowest = TRUE, right = FALSE)) %>%
  ggplot(aes(x=log_price, colour=car_cat)) +
  geom_density() + scale_fill_manual(values=palette34)

housesTrain %>% 
  na.omit() %>%
  mutate(car_cat = cut(Car, breaks = c(0,2,9), labels = c("Hasta 1", "Más de 2"), include.lowest = TRUE, right = FALSE)) %>%
  ggplot(aes(y=log_price, fill=car_cat)) +
  geom_boxplot() + scale_fill_manual(values=palette34)
```

__Distance__

Se comparan las transformaciones de ambas variables, observando que a priori la correlación lineal es muy débil, aunque negativa. Esto indica que a menor distancia al centro, mayor podría ser el precio.

```{r DistancePrice}
housesTrain %>%
  select(sqrt_distance, log_price) %>%
  ggpairs
```

__Latitud y longitud__

No hay demasiados indicios de que relación lineal entre las coordenadas y el precio de la vivienda. El signo de las correlaciones parece indicar que el precio de la vivienda aumenta en la parte sur (menor latitud) y en la parte este (mayor longitud) de Melbourne. 

```{r}
housesTrain %>%
  select('Lattitude', 'Longtitude', 'log_price') %>%
  ggpairs
```

__Sell date__

Lo primero que hacemos con esta variable será extraer el año, ya que viene en formato dd/MM/yyyy, y probablemente el año nos proporcione más información acerca del precio de venta de la casa.


```{r CrearSellYear}
housesTrain$sellYear <- separate(housesTrain, Date, c('Day','Month', 'Year'), sep = '/')$Year
#housesTrain$sellYear <- factor(housesTrain$year, levels=c("2016", "2017"))
```
```{r}
housesTrain$sellYear <- separate(housesTrain, Date, c('day','month', 'year'), sep = '/')$year
housesTrain$sellYear <-factor(housesTrain$sellYear, levels=c("2016", "2017"))
```

Observamos que solo hay 2 años de venta, y que no tiene demasiada relación con el precio.
```{r}
describe(housesTrain$sellYear)
```

```{r SellYearPrice, message=FALSE}
housesTrain %>% 
  ggplot(aes(x=log_price, colour=sellYear)) +
  geom_density()

housesTrain %>%
  select(sellYear, log_price) %>%
  ggpairs
```


¿Y con otras variables?
Parece que en 2017 se vendieron casa más alejadas del centro.
```{r message=FALSE}
housesTrain %>% select(sqrt_distance, Lattitude, Longtitude, sellYear) %>% ggpairs(columns=1:3, ggplot2::aes(colour=sellYear, alpha=0.1))
```

Probamos a analizar el mes por si nos da información adicional. Vemos que no hay ventas de casas en Enero para ningún año.
Si la tratamos como una variable categórica, no se observan diferencias significativas en el precio de la vivienda para ningún mes.

```{r message=FALSE}
housesTrain$sellMonth <- separate(housesTrain, Date, c('day','month', 'year'), sep = '/')$month
housesTrain$sellMonth <- factor(housesTrain$sellMonth)
```

```{r message=FALSE}
housesTrain %>% ggplot(aes(y=Price, color=sellMonth)) + geom_boxplot() + scale_fill_manual(values=palette34)
```

__PropertyCount__

Esta variable nos informa de todas las propiedades que hay en cada barrio. Se puede prueba lo siguiente:

- Categorizar esta variable para diferenciar entre barrios muy poblados y menos poblados, y ver si tiene relación con el precio.
- Combinar esta información con el número de casas vendidas por barrio, obtener la tasa de venta en cada barrio, y ver si hay barrios más populares que otros en cuanto a ventas.

Diviendo en 3 grupos parecidos, la primera opción no parece arrojar ninguna luz sobre el precio de la vivienda.
```{r}
housesTrain %>% mutate(property_count_cat = cut2(Propertycount, g=10)) %>% select(property_count_cat) %>% table()
housesTrain %>% mutate(property_count_cat = cut2(Propertycount, g=10)) %>% ggplot(aes(x=log_price, color=property_count_cat)) + geom_density()
```

Con la tasa de casas vendidas por barrio tampoco se diferencia demasiado. En todo caso, habría que probar con más categorías, pero se acabaría alcanzando el número distinto de barrios que hay.

```{r propertyCount}
suburbs_count <- housesTrain %>% group_by(Suburb) %>% tally() %>% arrange(desc(n))
#housesTrain <- housesTrain %>% inner_join(suburbs_count, by='Suburb') %>% mutate(sell_rate_suburb = round(n/Propertycount*100,2))
housesTrain %>% mutate(property_count_cat = cut2(sell_rate_suburb, g=2)) %>% select(property_count_cat) %>% table() 
housesTrain %>% mutate(property_count_cat = cut2(sell_rate_suburb, g=2)) %>%  ggplot(aes(x=log_price, color=property_count_cat)) + geom_density()
```


<br>

# Análisis exploratorio de datos faltantes

<br>

Hemos visto anteriormente que las variables *YearBuilt*, *BuildingArea* y *Car* tenían valores NAs, y que la variable *Landsize* tenía valores sospechosos (0).
Examinamos primero que % de los registros no tienen estos valores rellenos.

Observamos que cerca del 50% de los registros tienen la variable BuildingArea sin informar, mientras que el 40% no tienen la variable YearBuilt informada. Vemos además que en el 37% de los casos, BuildingArea y YearBuilt faltan simultáneamente. El % de registros con la variable Car vacía es muy pequeño. Además, tenemos 15% de 0 en la variable Landsize, lo que podríamos considerar como valores faltantes.

```{r HistFaltantes}
housesTrain$Landsize[which(housesTrain$Landsize == 0)] <- NA
housesTrain$CouncilArea[which(housesTrain$CouncilArea == '')] <- 'Unavailable'
housesTrain$CouncilArea[which(housesTrain$CouncilArea == 'Unavailable')] <- NA
housesTrain$YearBuilt[which(housesTrain$YearBuilt<1800)] <- NA
cols_na <- c('BuildingArea', 'Landsize', 'YearBuilt', 'Car', 'CouncilArea')
aggr_plot <- aggr(housesTrain[,cols_na], col=c('#666666','#E6AB02'), numbers=TRUE, sortVars=TRUE,
                  labels=names(housesTrain[,cols_na]), cex.axis=0.6, gap=3, 
                  ylab=c("Histogram of missing data","Patrón de co-ocurrencia"))

```

A modo de resumen respecto a los datos faltantes podemos decir que:

- Para la variable *BuildingArea*, el 47% de los valores (5145 de 10865) son NaNs, y el 0.11% son 0. En total, tendríamos 47.11% de valores informados.

- Para la variable *YearBuilt*, el 40% de los valores son NaNs.

- Para la variable *Landsize*, no hay NaNs, pero el 14.3% de los valores son 0.

- La variable *Car* tiene 49 valores faltantes (0.45%).

- Para la variable *CouncilArea*, el 10% son desconocidos (1098)

<br>

__Relación entre BuildingArea y Landsize informados__

La variable *BuildingArea* podría ser de gran interés a la hora de predecir el precio de venta de una casa. Sin embargo, la gran cantidad de datos faltantes lleva a pensar que una técnica de imputación puede no ser recomendable. Como también se tiene la variable *Landsize*, se procede a comprobar si existe alguna relación entre los valores informados de *BuildingArea* y esta variable, ya que la podríamos utilizar por su bajo número de datos faltantes.

Como era de esperar se observa una correlación positiva entre ambas variables. El valor de la correlación es de 0.5 lo que también cabe dentro de lo esperable. Sin embargo, es difícil apoyarse en las gráficas debido a sus distribuciones por lo que se que se procede a utilizar el logarítmo de ambas variables.

```{r BuildingLand}
housesTrain %>% select(BuildingArea, Landsize) %>% na.omit() %>% ggpairs()
```
```{r BuildingLanLog}
housesTrain %>% select(BuildingArea, Landsize) %>%
  na.omit() %>%
  filter(Landsize > 0 & BuildingArea > 0) %>%
  mutate(log_landsize = log10(Landsize), log_buildingArea = log10(BuildingArea)) %>%
  select(log_landsize, log_buildingArea) %>% ggpairs()
```

La relación entre los logaritmos tiene una correlación menor que lo visto anteriormente. Pese a ello, y la gran cantidad de valores faltantes que tienen, decidimos eliminar la variable y quedarnos con *Landsize*.

__Landsize__

Aparte de los valores a 0, esta variable presenta valores muy dispersos. Se puede probar una transformación logarítmica para verla mejor (hay valores muy extremos). A la hora de realizar su imputación vamos a ver cómo se relaciona *Landsize* con *Longtitud* , *Lattitude* y *Distance* ya que, pensando en el ámbito en el que estamos trabajando, creemos que la ubicación de la vivienda estará relacionado con el area del perímetro previo a la vivienda.

```{r LandFaltantes}
housesTrain %>% select(Landsize) %>%
  na.omit() %>%
  mutate(log_landsize = log10(Landsize)) %>%
  ggplot(aes(x=log_landsize)) +
  geom_density()
```

Vamos a ver como se distribuyen los valores informados y faltantes en función de la localización (latitud y longitud).

```{r message=FALSE}
housesTrain %>% filter(is.na(Landsize)) %>% select(sqrt_distance, Lattitude, Longtitude) %>% ggpairs() + labs(title='Landsize no informado')

housesTrain %>% select(sqrt_distance, Lattitude, Longtitude, Landsize) %>%
  na.omit() %>%
  mutate(log_landsize = log10(Landsize)) %>%
  select(sqrt_distance, Lattitude, Longtitude, log_landsize) %>%
  ggpairs + labs(title='Landsize informado')
```

Parece que los valores faltantes de *Landsize* tienen muchos de sus valores en valores bajos de *Lattitude*.

```{r}
housesTrain %>%
  mutate(log_landsize = log10(Landsize)) %>%
  select(Lattitude, log_landsize) %>% marginplot()
```

Para la longitud, los valores faltantes de *Landsize* se concentran alrededor de la mediana de la longitud.

```{r}
housesTrain %>%
  mutate(log_landsize = log10(Landsize)) %>%
  select(Longtitude, log_landsize) %>% marginplot()
```


Si además se examina en función de la distancia, se puede aprecia que los valores faltantes de *Landsize* se dan para valores bajos de *Distance*, es decir, para casas más cercanas al centro de la ciudad.

```{r}
housesTrain %>%
  mutate(log_landsize = log10(Landsize)) %>%
  select(Distance, log_landsize) %>% marginplot()
```
Si probamos a imputar con kNN los valores de *Landsize* en función de las coordenadas y la distancia, vemos que los nuevos valores no se alejan demasiado de las distribuciones iniciales.

```{r LandsizeImputacion}
imputationsLandsize <- housesTrain %>% select(Lattitude, Longtitude, Distance, Landsize) %>% VIM::kNN(variable='Landsize')
```

Se puede ver mejor si transformamos los valores de *Landsize* una vez imputados:
```{r}
imputationsLogLandsize <- imputationsLandsize %>% mutate(log_landsize=log10(Landsize), log_landsize_imp = Landsize_imp)
```
```{r}
imputationsLogLandsize %>% select(Lattitude, log_landsize, log_landsize_imp) %>% marginplot(., delimiter = '_imp')
imputationsLogLandsize %>% select(Longtitude, log_landsize, log_landsize_imp) %>% marginplot(., delimiter = '_imp')
imputationsLogLandsize %>% select(Distance, log_landsize, log_landsize_imp) %>% marginplot(., delimiter = '_imp')
```

Comparamos la distribución de *Landsize* con valores informados con la generada a través de imputación.

```{r}
p1 <- housesTrain %>% select(Landsize) %>%
  na.omit() %>%
  mutate(log_landsize = log10(Landsize)) %>%
  ggplot(aes(y=log_landsize)) +
  geom_boxplot()

p2 <- imputationsLogLandsize %>% select(log_landsize) %>%
  ggplot(aes(y=log_landsize)) +
  geom_boxplot()

grid.arrange(p1, p2, nrow=1)
```

Además, se comparan la nueva variable *log_landsize* con valores imputados con el precio, como se ha hecho en el análisis multivariante. No parece que haya una gran correlación entre ambas variables.

```{r}
housesTrain$log_landsize <- imputationsLandsize %>% mutate(log_landsize=log10(Landsize)) %>% select(log_landsize) %>% unlist()
housesTrain %>% select(log_landsize, log_price) %>% ggpairs()
```


__Car__

Para la variable *Car*, solo hay 49 valores faltantes (0.45%). Al ser una variable numérica con valores discretos (solo toma valores enteros desde 0 hasta 8), no parece buena idea utilizar métodos de imputación por regresiones en base a otras variables.

Podemos por un lado tratar de imputarlas basándonos en otras variables y usando kNN. Usaremos las variables *Rooms* y *Distance*:

```{r ImputCarconRooms}
housesTrain %>% select(Rooms, Car)  %>% marginplot()
housesTrain %>% select(Distance, Car)%>% marginplot()
imputationsCar <- housesTrain %>% select(Rooms, Distance, Car) %>% VIM::kNN(variable='Car')
imputationsCar %>% select(Distance, Car, Car_imp) %>% marginplot(., delimiter = '_imp')
```

Otras dos opciones:

- Borrar esos 49 registros (a riesgo de poder perder algo de información).
- Transformar la variable a categórica e intentar imputar el factor.


Se podría intentar categorizar la variable (ya vimos que se puede dividir en 2 categorías con aproximadamente el mismo numero de muestras) y utilizar la función *polyreg* del paquete MICE que permite imputar valores categóricos mediante una regresión politómica. Este método se basa en construir un modelo multinomial para las respuestas categóricas, realizar las predicciones para los valores faltantes y después añadir ruido a estas predicciones. (NOTA: esta aproximación no ha funcionado bien, se deja el planteamiento)

```{r}
housesTrain$car_cat = housesTrain %>%
  mutate(car_cat = addNA(cut(Car, breaks = c(0,2,9), labels = c("0 o 1 plaza", "2 o más"), include.lowest = TRUE, right = FALSE))) %>%
  select(car_cat) 

housesTrain$car_cat %>% table
```


Una vez imputado, previsiblemente no habrá cambiado la distribución de la variable al ser un % tan pequeño de datos faltantes.

__CouncilArea__

Para las viviendas donde *CouncilArea* no está registrado, lo imputaremos con un KNN utilizando la latitud y longitud. Con estas variables la imputación debería ser correcta, ya que las coordenadas informan directamente del área en el que está la casa.

```{r imputacionCouncilArea}
imputationCouncilArea <- housesTrain %>% select(CouncilArea, Lattitude, Longtitude) %>% VIM::kNN(variable='CouncilArea')

imputationCouncilArea$Price <- housesTrain$Price

housesTrain %>% select(Price, CouncilArea)  %>% marginplot
imputationCouncilArea %>% select(Price, CouncilArea, CouncilArea_imp)  %>% marginplot(., delimiter = '_imp')

ggplot(data=imputationCouncilArea, aes(x=CouncilArea, fill = CouncilArea)) +       geom_bar(stat="count",position=position_dodge()) +
  scale_fill_manual(values=palette34) +
  labs(x= 'CouncilArea_imp', y = 'Frecuencias', fill=NULL) + 
  theme(axis.text.x=element_blank())


imputationCouncilArea %>% select(c("Price", "CouncilArea")) %>%
  ggplot(aes(y=Price, fill=CouncilArea)) +
  geom_boxplot()
```

__YearBuilt__

Retomando el análisis univariante de esta variable hay que recordar que presenta una distribución con varios picos que sugiere su discretización. De esta manera, se pretende transformarla en una variable categórica que represente intervalos temporales a la que añadiremos categoría Unknown para los valores faltantes. También recordamos que para una de las observaciones el valor de *YearBuilt* era de 1196 y, dado que es anterior al año de fundación de Melbourne, lo tomaríamos como dato perdido.

En la siguiente gráfica se muestra la relación con *Price* y su logaritmo eliminando la observación de 1196.

```{r}
housesTrain <- housesTrain
housesTrain %>% select(YearBuilt, log_price) %>% na.omit() %>% ggpairs
```

Se categoriza en cuatro grupos de manera que los grupos tienen un número de observaciones más o menos similar. Al verla en relación a *Price*, se observa que los datos faltantes quedan entre la categoría de casas más antiguas (hasta 1951) y las más modernas.

```{r}
year_categories <- housesTrain %>% 
  mutate(year_cat = addNA(cut2(YearBuilt, g=2)))

year_categories %>%
  select(year_cat) %>% table()

year_categories %>%
  ggplot(aes(x=log_price, colour=year_cat)) +
  geom_density()

year_categories %>%
  ggplot(aes(y=log_price, fill=year_cat)) +
  geom_boxplot()
```
Si se prueba a imputar respecto a las coordenadas (REVISAR)
```{r imputarYearBuilt}
imputationsYearBuilt <- housesTrain %>% select(Lattitude, Longtitude, YearBuilt) %>% VIM::kNN(variable='YearBuilt')

year_categoriesImp <- imputationsYearBuilt %>% mutate(year_cat2 = cut2(YearBuilt, g=2))

year_categoriesImp %>%
  select(year_cat2) %>% table()

year_categoriesImp$log_price <- housesTrain$log_price

year_categoriesImp %>%
          ggplot(aes(x=log_price, colour=year_cat2)) +
          geom_density()

```


<br>

# Transformaciones y procesado de variables

<br>

A modo de resumen, se contemplan las siguientes transformaciones de las variables cuantitativas disponibles:
* Transformación logarítmica de *Landsize* y *Price* (variable target, cambiaría interpretabilidad, pero puede mejorar predicción)
* Transformación de raíz cuadrada de *Distance*
* Categorización de *Rooms*, *YearBuilt* y *Car*
* Reescalado de variables cuantitativas

```{r cuantitative_transformations}
# Filtros e imputaciones (YearBuilt categórica)
#housesTrainFinal <- housesTrain
housesTrain$YearBuilt[which(housesTrain$YearBuilt<1850)] <- NA
housesTrain$Landsize[which(housesTrain$Landsize == 0)] <- NA
imputationsLandsizeFinal <- housesTrain %>% select(Lattitude, Longtitude, Distance, Landsize) %>% VIM::kNN(variable='Landsize')
imputationsCarFinal <- housesTrain %>% select(Rooms, Distance, Car) %>% VIM::kNN(variable='Car') 

#Necesario para la función de construcción del dataset
housesTrain$LandsizeImp <- imputationsLandsizeFinal %>% select(Landsize) %>% unlist()
housesTrain$CarImp <- imputationsCarFinal %>% select(Car) %>% unlist()
imputationsLandsizeforTest <- housesTrain %>% select(Suburb, Address, Rooms, Type, Price, Method, SellerG, Date, Distance, Postcode, Bedroom2, Bathroom, Car, LandsizeImp, BuildingArea, YearBuilt, CouncilArea, Lattitude, Longtitude, Regionname, Propertycount) 
names(imputationsLandsizeforTest)[names(imputationsLandsizeforTest) == "LandsizeImp"] <- "Landsize"
imputationsCarforTest <- housesTrain %>% select(Suburb, Address, Rooms, Type, Price, Method, SellerG, Date, Distance, Postcode, Bedroom2, Bathroom, CarImp, Landsize, BuildingArea, YearBuilt, CouncilArea, Lattitude, Longtitude, Regionname, Propertycount) 
names(imputationsCarforTest)[names(imputationsCarforTest) == "CarImp"] <- "Car"

# Transformaciones de variables cuantitativas

sqrt_distance <- housesTrain %>% mutate(sqrt_distance = sqrt(Distance)) %>% select(sqrt_distance)
housesTrain$sqrt_distance <- unlist(sqrt_distance)
log_landsize <- imputationsLandsizeFinal %>% mutate(log_landsize=log10(Landsize)) %>% select(log_landsize)
housesTrain$log_landsize <- unlist(log_landsize)
log_price <- housesTrain %>% mutate(log_price = log10(Price)) %>% select(log_price)
housesTrain$log_price <- unlist(log_price)

# Discretización de variables cuantitativas
housesTrain$rooms_cat <- cut(housesTrain$Rooms,  breaks = c(1,3,4,10), labels = c("Pequeñas", "Medianas", "Grandes"), include.lowest = TRUE, right = FALSE)
housesTrain$year_built_cat <- factor(cut2(housesTrain$YearBuilt, g=2), labels = c("Antigua", "Moderna"))
levels(housesTrain$year_built_cat) <- c(levels(housesTrain$year_built_cat), 'Desconocido')
housesTrain <- housesTrain %>% mutate_at(vars(year_built_cat), ~replace(., is.na(.), 'Desconocido'))
# Hasta 1 baño
housesTrain$bath_cat <- factor(cut2(housesTrain$Bathroom, g=2), labels=c("Pocos_baños", "Muchos_baños"))
#De 0 a 3 dormitorios
housesTrain$bed_cat <- factor(cut2(housesTrain$Bedroom2, g=2), labels=c("Pocos_dormitorios", "Muchos_dormitorios"))
#0-1 y 2 o más
housesTrain$car_cat <-cut(imputationsCarFinal$Car, breaks = c(0,2,9), labels = c("Pocas_plazas", "Muchas_plazas"), include.lowest = TRUE, right = FALSE)

#El join solo se debe ejecutar una vez
suburbs_count <- housesTrain %>% group_by(Suburb) %>% tally() %>% arrange(desc(n))
housesTrain <- housesTrain %>% inner_join(suburbs_count, by='Suburb') %>% mutate(sell_rate_suburb = round(n/Propertycount*100,2))
# Hasta 1.47% de tasa de venta por barrio
housesTrain$sell_rate_cat <- factor(cut2(housesTrain$sell_rate_suburb, g=2), labels=c("Menos_populares", "Más_populares"))

# Estandarización de variables
num_vars <- c('sqrt_distance', 'log_landsize', 'Lattitude', 'Longtitude')
cat_vars <- c('rooms_cat', 'year_built_cat', 'car_cat', 'Regionname', 'Type', 'Method', 'bath_cat', 'bed_cat', 'sell_rate_cat')
housesTrainNum <- housesTrain %>% select(num_vars)
normParam <- preProcess(housesTrainNum)
housesTrainNumNorm <- predict(normParam, housesTrainNum)

housesTrainFinal <- data.frame(housesTrainNumNorm[,num_vars], housesTrain[,cat_vars], housesTrain[, c('Price', 'log_price')])

housesTrainFinal$Regionname = factor(housesTrainFinal$Regionname, levels=c('Southern Metropolitan', 'Northern Metropolitan', 'Western Metropolitan', 'Eastern Metropolitan', 'South-Eastern Metropolitan', 'Eastern Victoria', 'Northern Victoria', 'Western Victoria'))
#Utilizo la librería plyr
housesTrainFinal$Regionname  = mapvalues(housesTrainFinal$Regionname, from = c('Southern Metropolitan', 'Northern Metropolitan', 'Western Metropolitan', 'Eastern Metropolitan','South-Eastern Metropolitan', 'Eastern Victoria', 'Northern Victoria', 'Western Victoria'), to = c('Southern_Metropolitan', 'Northern_Metropolitan', 'Western_Metropolitan', 'Eastern_Metropolitan','South_Eastern_Metropolitan', 'Eastern_Victoria', 'Northern_Victoria', 'Western_Victoria'))

housesTrainFinal$Method = factor(housesTrainFinal$Method, levels=c('S', 'SP', 'PI', 'VB', 'SA'))
housesTrainFinal$Type = factor(housesTrainFinal$Type, levels=c('h', 'u', 't'))
```

Comprobamos los niveles de los factores de las variables categóricas. Siempre que sea posible, el primer nivel debería asociarse a la categoría con más muestras, ya que previsiblemente será la que menor desviación estándar tenga. Además, si los factores siguen un orden lógico, debería respetarse en los niveles.

Teniendo en cuenta esto, reordenamos las variables *Regionname*, *Method* y *Type*.

```{r}
housesTrainFinal$rooms_cat %>% table()
housesTrainFinal$year_built_cat %>% table()
housesTrainFinal$car_cat %>% table()
housesTrainFinal$Regionname %>% table()
housesTrainFinal$Method %>% table()
housesTrainFinal$Type %>% table()
```

```{r echo=FALSE}
levels(housesTrainFinal$rooms_cat)
levels(housesTrainFinal$year_built_cat)
levels(housesTrainFinal$car_cat)
levels(housesTrainFinal$Regionname)
levels(housesTrainFinal$Method)
levels(housesTrainFinal$Type)
levels(housesTrainFinal$bath_cat)
levels(housesTrainFinal$bed_cat)
levels(housesTrainFinal$sell_rate_cat)
```

<br>
Con todo imputado y categorizado, podemos realizar más análisis multivariantes, combinando las nuevas variables.

En el siguiente gráfico se puede observar que las casas más pequeñas están más cercanas al centro, y que además tienen un menor *Landsize*. Con respecto al precio, se hace más evidente la relación con la distancia: si la casa es grande y está muy cercana al centro, es más cara.

```{r message=FALSE}
housesTrainFinal %>% select(sqrt_distance, log_landsize, log_price, rooms_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=rooms_cat, alpha=0.1))
housesTrainFinal %>% select(Lattitude, Longtitude, log_price, rooms_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=rooms_cat, alpha=0.1))
```

Con respecto al número de plazas de aparcamiento, lógicamente las casas situadas en el centro tienen menos plazas, y una parcela menor.
```{r}
housesTrainFinal %>% select(sqrt_distance, log_landsize, log_price, car_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=car_cat, alpha=0.1))
housesTrainFinal %>% select(Lattitude, Longtitude, log_price, car_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=car_cat, alpha=0.1))
```

```{r}
housesTrainFinal %>% select(sqrt_distance, log_landsize, log_price, year_built_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=year_built_cat, alpha=0.1))
housesTrainFinal %>% select(Lattitude, Longtitude, log_price, year_built_cat) %>% ggpairs(columns=1:3, ggplot2::aes(colour=year_built_cat, alpha=0.1))
```


```{r}
housesTrainFinal %>% select(sqrt_distance, log_landsize, log_price, Type) %>% ggpairs(columns=1:3, ggplot2::aes(colour=Type, alpha=0.1))
```
```{r}
housesTrainFinal %>% select(sqrt_distance, log_landsize, log_price, Method) %>% ggpairs(columns=1:3, ggplot2::aes(colour=Method, alpha=0.1))
```

```{r}
#write.csv(housesTrainFinal, file='train_set_new_variables.csv', row.names = FALSE)
```


# Aplicación de técnicas automáticas de selección de variables

<br>

Anteriormente hemos preseleccionado ciertas variables después de la limpieza y análisis inicial de nuestro conjunto de datos. En base al exploratorio realizado, hemos visto que no hay ninguna correlación lineal evidente entre las variables cuantitativas y la variable objetivo. Por tanto, vamos a aplicar diversos métodos de selección de variables para encontrar la mejor combinación posible.Actualmente disponemos de un total de 10 variables explicativas:

```{r cargadatos, echo=F}
train <- read.csv('train_set.csv')
train %>% select(-c(log_price,Price)) %>% names()
```

Inicialmente vamos a utilizar como target la variable *Price* en lugar de su transformación logarítmica:

```{r echo=F}
train <- train %>% select(-log_price)
# Necesito redefinirlos porque se me descolocaban los niveles al cargar el trainset
train$Regionname = factor(train$Regionname, levels=c('Southern_Metropolitan', 'Northern_Metropolitan', 'Western_Metropolitan', 'Eastern_Metropolitan', 'South_Eastern_Metropolitan', 'Eastern_Victoria', 'Northern_Victoria', 'Western_Victoria'))
train$Method = factor(train$Method, levels=c('S', 'SP', 'PI', 'VB', 'SA'))
train$Type = factor(train$Type, levels=c('h', 'u', 't'))
train$car_cat = factor(train$car_cat, levels = c('Hasta_1_plaza', '2_o_más_plazas'))
train$year_built_cat = factor(train$year_built_cat, levels = c('Antigua', 'Moderna', 'Desconocido'))
train$rooms_cat = factor(train$rooms_cat, levels = c('Pequeñas','Medianas','Grandes'))
```

<br>

### Best subsets

<br>

El primer método que hemos utilizado es el de best subset selección. El resumen de todos los modelos examinados se muestra a continuación:

```{r}
best_subsets_models <- regsubsets(Price~., data = train, nvmax = 22)
reg_sum <- summary(best_subsets_models)
reg_sum
```
Para evaluar los resultados obtenidos, se ha observado el coeficiente {R{2}} ajustado asociado a cada modelo:

```{r}
reg_sum$adjr2
which.max(reg_sum$adjr2)
```
A juzgar por los resultados, el mejor modelo es el que consta de 20 variables. 

Otra manera más gráfica de ver de cuántos predictores consta el mejor modelo posible sería a través del siguiente gráfico, donde además del R**2 ajustado se muestran otras estimaciones de la bondad de ajuste de los modelos:

```{r fig.align='center'}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(best_subsets_models, scale=metric)}
```

En los anteriores gráficos se puede observarque las variables que más aparecen en general son la distancia al centro (*sqrt_distance*), el número de habitaciones y el tipo de vivienda. También se puede observar que la mejoría del modelo es casi imperceptible a partir de un determinado número de variables.

```{r}
p <- ggplot(data = data.frame(n_predictores = 1:22,
                              R_ajustado = reg_sum$adjr2),
            aes(x = n_predictores, y = R_ajustado)) +
    geom_line() +
    geom_point()
p <- p + geom_point(aes(
                    x = n_predictores[which.max(reg_sum$adjr2)],
                    y = R_ajustado[which.max(reg_sum$adjr2)]),
                    colour = "red", size = 3)
p <- p +  scale_x_continuous(breaks = c(0:22)) + 
          theme_bw() +
          labs(title = 'R2_ajustado vs número de predictores (Best Subset)', 
               x =  'número predictores')
p
```


En la anterior gráfica se puede observar que, aunque el modelo con el mayor R^2 ajustado se consigue con 20 predictores (como ya habíamos visto), a partir de 10 predictores aproximadamente la mejoría es inapreciable. Como siempre va a ser mejor un modelo más explicable.

```{r}
reg_sum$adjr2[20]
reg_sum$adjr2[10]
```
Para el modelo de 10 predictores, los coeficientes son los siguientes:

```{r}
coef(object = best_subsets_models, id = 10)
```

<br>

### Forward Selection

<br>

Otra de las técnicas utilizadas es la de *Forward Selection*, en la que se parte de un modelo vacío y se van añadiendo variables. Los resultados obtenidos han sido los siguientes:


```{r}
best_forward_models <- regsubsets(Price~., data = train, nvmax = 22, method = 'forward')
reg_forward_sum <- summary(best_forward_models)
reg_forward_sum
```


```{r}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(best_forward_models, scale=metric)}
```

```{r}
p <- ggplot(data = data.frame(n_predictores = 1:22,
                              R_ajustado = reg_forward_sum$adjr2),
            aes(x = n_predictores, y = R_ajustado)) +
    geom_line() +
    geom_point()
p <- p + geom_point(aes(
                    x = n_predictores[which.max(reg_forward_sum$adjr2)],
                    y = R_ajustado[which.max(reg_forward_sum$adjr2)]),
                    colour = "red", size = 3)
p <- p +  scale_x_continuous(breaks = c(0:22)) + 
          theme_bw() +
          labs(title = 'R2_ajustado vs número de predictores (Forward Selection)', 
               x =  'número predictores')
p
```
De nuevo se cumple que el modelo que ofrece un mayor R^2 ajustado es el que utiliza 20 predictores. Ahora se necesitaría mínimo unos 12 predictores para alcanzar un valor similar.

```{r}
reg_forward_sum$adjr2[12]
coef(object = best_forward_models, id = 12)
```

<br>

### Backward Selection

<br>

El último método de selección automática de variables que se ha puesto en práctica es el de *Backward Selection*. En este caso se parte de un modelo con todos los predictores y se van eliminando uno a uno (en cada paso se elimina la variable más significativa para el modelo):

```{r}
best_backward_models <- regsubsets(Price~., data = train, nvmax = 22, method = 'backward')
reg_backward_sum <- summary(best_backward_models)
reg_backward_sum
```
```{r}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(best_backward_models, scale=metric)}
```
```{r}
p <- ggplot(data = data.frame(n_predictores = 1:22,
                              R_ajustado = reg_backward_sum$adjr2),
            aes(x = n_predictores, y = R_ajustado)) +
    geom_line() +
    geom_point()
p <- p + geom_point(aes(
                    x = n_predictores[which.max(reg_backward_sum$adjr2)],
                    y = R_ajustado[which.max(reg_backward_sum$adjr2)]),
                    colour = "red", size = 3)
p <- p +  scale_x_continuous(breaks = c(0:22)) + 
          theme_bw() +
          labs(title = 'R2_ajustado vs número de predictores (Forward Selection)', 
               x =  'número predictores')
p
```
```{r}
reg_forward_sum$adjr2[10]
coef(object = best_forward_models, id = 10)
```
Se puede concluir, que las 3 técnicas de selección automática de variables indican que el número óptimo de predictores es alrededor de 10. Además, según estas técnicas, las variables más significativas para el modelo serían:

* Distancia al centro.
* Longitud.
* Latitud.
* Número de habitaciones.
* Número de plazas de garaje.
* Tipo de vivienda.
* Región.

<br>

# Técnicas de regularización 

<br>

<br>

## Regularización Lasso

<br>

```{r}
 x <- model.matrix(Price~., data = train)[, -1]
y <- train$Price
```

En la siguiente gráfica se muestra como varía el valor de los coeficientes para diferentes valores de lambda, hasta que finalmente se hacen todos 0 (esto con Ridge no ocurriría).
```{r}
models_lasso <- glmnet(x = x, y = y, alpha = 1)
plot(models_lasso, xvar = "lambda", label = TRUE)
```
Para descubrir con qué valor de lambda se consigue el mejor modelo, y con cuántos predictores, el paquete 'glmnet' ofrece una función para averiguarlo. Para la evaluación de los modelos utiliza la técnica de validación cruzada (con 10 *folds* por defecto) y una función de pérdida que por defecto es el error cuadrático medio.

```{r}
set.seed(10)
cv_lasso <- cv.glmnet(x = x, y = y, alpha = 1)
plot(cv_lasso)
```

En la gráfica podemos apreciar dos líneas de puntos. La primera marca el lambda para el cual se consigue el modelo con un error cuadrático medio más bajo. La segunda, marca el lambda para el cual se consigue el modelo más sencillo cuyo error se encuentra a 1 desviación estándar del mínimo.  

En esta misma gráfica, podemos ver que se podría obtener un modelo con 14 predictores aproximadamente, cuyo error sería muy similar al del mejor modelo (que utiliza los 22).

```{r}
set.seed(10)
out_eleven <- glmnet(x,y,alpha=1,lambda = cv_lasso$lambda.1se)
lasso_coef_eleven <- predict(out_eleven,type="coefficients")[1:23,]
lasso_coef_eleven[lasso_coef_eleven!=0]
```

La información que obtenemos al aplicar regularización Lasso es similar a la que obteníamos con las técnicas aplicadas anteriormente.

## Regularización Ridge 

Otra técnica de regularización que se ha aplicado es la regresión Ridge. En este caso, el penalty aplicado a los coeficientes reducirá los coeficientes menos importantes, pero nunca los hará completamente 0, independientemente de valor que tome lambda. Esto se puede comprobar en la siguiente gráfica:

```{r}
models_ridge <- glmnet(x = x, y = y, alpha = 0)
plot(models_ridge, xvar = "lambda", label = TRUE)
```

De igual forma que con Lasso, en la siguiente gráfica se puede observar el error cuadrático medio calculado mediante validación cruzada para diferentes valores de lambda:

```{r}
set.seed(10)
cv_ridge <- cv.glmnet(x = x, y = y, alpha = 0)
plot(cv_ridge)
```

En este caso, se muestran los coeficientes que se obtienen del modelo cuyo error se encuentra a 1 desviación estándar del mínimo (y donde empieza a incrementarse el error cuadrático medio):

```{r}
out_ridge <- glmnet(x,y,alpha=0,lambda = cv_ridge$lambda.1se)
ridge_coef <- predict(out_ridge,type="coefficients")[1:23,]
sort(abs(ridge_coef), decreasing = TRUE) 
```
En este caso, la información más valiosa que nos aporta Ridge es que le otorga bastante importancia al tipo de vivienda.

```{r carga de datos}
train <- read.csv('train_set.csv')
train <- train %>% select(-log_price) #Voy a usar como target el Price sin transformar
head(train)
```

# Modelos

Una vez ya hemos analizado completamente nuestras variables, se ha procedido ha entrenar diversos modelos y evaluar sus resultados. A continuación se van mostrando los resultados obtenidos.

## Modelo 1

En este primer modelo se han utilizado las variables que más se repetían como relevantes para el modelo generalmente en todos los métodos de selección de variables empleados anteriormente. Son las siguientes:

* Distancia al centro.
* Número de habitaciones.
* Tipo de vivienda.
* Nombre de la región.
* Longitud.
* Latitud.

Los resultados del modelo han sido los siguientes:

```{r  echo=F}
model1 <- lm(Price~sqrt_distance + Lattitude + Longtitude + rooms_cat + Type + Regionname, data = train)
summary(model1)
```

## Modelo 2

Es el mismo que el primer modelo probado, pero añadiendo la variable year_cat, a pesar de que en vista a los resultados obtenidos en la parte de selección de variables no parece que sea muy influyente para los modelos. Con lo cual, las variables utilizadas por el modelo son las siguientes:

* Distancia al centro.
* Número de habitaciones.
* Tipo de vivienda.
* Nombre de la región.
* Longitud.
* Latitud.
* Año de construcción como categórica.

```{r}
model2 <- lm(Price~sqrt_distance + Lattitude + Longtitude + rooms_cat + Type + Regionname + year_built_cat, data = train)
summary(model2)
```
El resultado obtenido es prácticamente el mismo. De hecho, a pesar de que como era evidente el R^2 aumenta, el R^ajustado no varía respecto al modelo anterior. Además, se puede observar que los p-valores asociados a los coeficientes de las categorías de year_built son bastante altos. 

## Modelo 3

En este tercer caso, se ha introducido de nuevo la variable YearBuilt, pero en este caso como númerica (previa imputación), para poder ver como varía el funcionamiento del modelo. Por lo tanto, las variables utilizadas han sido las siguientes:


* Distancia al centro.
* Número de habitaciones.
* Tipo de vivienda.
* Nombre de la región.
* Longitud.
* Latitud.
* Año de construcción como numérica.


```{r echo=F, message=F}
#Carga de datos

train_year <- read_csv('train_yearbuilt.csv') %>% select(-log_price)
model3 <- lm(Price~sqrt_distance + Lattitude + Longtitude + YearBuilt + Type + rooms_cat + Regionname, data = train_year)
summary(model3)

```

Se puede ver que en este caso si que se mejora ligeramente la performance del modelo al añadir la variable YearBuilt como numérica.

```{r  echo=F}
#Puede ser útil
model_metrics <- augment(model3)
```

Las siguientes gráficas nos ayudan a estudiar los residuos resultantes tras aplicar el modelo:

```{r}
par(mfrow = c(2, 2))
plot(model3)
```

### Normalidad de los residuos

A continuación se muestra el diagrama Q-Q de los residuos para comprobar su normalidad:

```{r}
plot(model3, 2)
```
 
Se puede ver que para los valores más altos es cuando se empiezan a desviar de una normal. Los errores NO son normales.


### Homogeneidad de la varianza de los residuos 

```{r}
plot(model3,3)
```

En esta gráfica se ve claramente que la varianza de los residuos no es para nada constante. Esta varianza empieza a aumentar bastante para los valores más altos. Por lo tanto no se cumple el principio de homodedasticity de los mismos.

### Valores influyentes

La siguiente gráfica muestra los puntos más influyentes para el modelo según la distancia de Cook:

```{r}
# Cook's distance
plot(model3, 4, id.n = 5) #Marco los 5 puntos más influyentes según la distancia de Cook
```

```{r}
# Imprimo los datos de esos 5 puntos más inlfuyentes
model_metrics %>% top_n(5, wt = .cooksd)
```

El punto más influyente para el modelo con diferencia es el siguiente:

```{r}
housesTrain %>% filter(Price == 9000000)
```

Se puede ver que está bastante alejado del centro para ser tan cara. También el número de habitaciones es bastante bajo y teniendo en cuenta que estas son dos de las variables más siginificativas para el modelo, nos podemos hacer una idea de por qué este punto influye tanto en el resultado.


## Selección de variables con log_price como target

En vista a que los resultados obtenidos utilizando Price sin transformar como variable objetivo no han sido especialmente buenos, se ha decido probar a utilizar su transformación logaritmica como target. Primeramente, hemos aplicado de nuevo un par de técnicas de selección de variables para asegurarnos de que las variables más relevantes siguen siendo las mismas a pesar de cambiar de target:


```{r message=F}
train_year2 <- read_csv('train_yearbuilt.csv') %>% select(-Price)
best_subsets_log <- regsubsets(log_price~., data = train_year2, nvmax = 22)
reg_sum_log <- summary(best_subsets_log)
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(best_subsets_log, scale=metric)}
```

```{r}
set.seed(10)
x <- model.matrix(log_price~., data = train_year2)[, -1]
y <- train_year2$log_price
cv_elastic <- cv.glmnet(x = x, y = y, alpha = 0.5)
plot(cv_elastic)
```

```{r}
coefss <- predict(glmnet(x,y,alpha=0.5,lambda = cv_elastic$lambda.1se),type="coefficients")[1:22,]
sort(abs(coefss), decreasing = T)
```

En vista a estos resultados, se ha determinado que las variables más relevantes son aproximadamente las mismas.


## Modelo 4

Con todo ello, se ha probado el modelo con las mismas variables con las que obteníamos anteriormente el mejor resultado, pero transformando el target. El resultado obtenido ha sido el siguiente: 


```{r message=F}
model4 <- lm(log_price~sqrt_distance + Lattitude + Longtitude + YearBuilt + Type + rooms_cat + Regionname, data = train_year2)
summary(model4)
```
Vemos que en este caso el porcentaje de varianza del precio que nuestro modelo es capaz de explicar es algo más alto. La mayoría de las variables tienen p-valores muy bajos. Las únicas que no serían estadísticamente siginificativas serían las dummies de algunas regiones.

En el siguiente gráfico se puede ver como la varianza de los residuos es más o menos uniforme para los valores predichos:

```{r echo=F}
plot(model4, 1)
```

En cuanto a la normalidad de los residuos, podemos ver que en esta ocasión sí que se ajustan bastante a una distribución normal:

```{r}
plot(model4, 2)
```

Al aplicar el test de normalidad de Lilliefors sobre los residuos del modelo, el p-valor devuelto es muy pequeño, lo que no nos permite rechazar la hipótesis de normalidad de estos:

```{r}
lillie.test(model4$residuals)
```

```{r}
plot(model4, 4, id.n = 5)
```

```{r}
model_metrics_2 <- augment(model4)
model_metrics_2 %>% top_n(5, wt = .cooksd)
```

# Score sobre test

Este último modelo parace ser el que consigue ajustarse de una mejor manera a los datos. Para poder evaluar realmente su correcto funcionamiento, es necesario comprobar su rendimiento sobre datos nuevos, es decir, el conjunto de test.


```{r}
test_data <- read.csv('test_yearbuilt.csv')
test_data <- test_data %>% select(-Price)
best_model <- model4
(preds <- data.frame(cbind(actual_values=test_data$log_price, predicted_values=predict(best_model,test_data))))[1:5,]
```

En la siguiente tabla se muestran los valores de diferentes medidas para el error:

```{r echo=F}
rmse <- RMSE(predict(best_model,test_data), test_data$log_price)
r2 <- R2(predict(best_model,test_data), test_data$log_price)
mae <- mae(predict(best_model,test_data), test_data$log_price)
model_metrics <- data.frame(error_metrics = c('R2','RMSE','MAE'), value = c(r2, rmse, mae))

model_metrics

```

Se puede observar por el valor que toma R2 (superior que en train), que nuestro modelo, a pesar de que no logra un gran acierto, no sufre ningún tipo de sobreajuste.



# Conclusiones y líneas futuras
- El análisis nos ha mostrado que no hay una relación lineal evidente entre las variables predictoras y el precio de la vivienda. Esta es una de las razones principales por las que se considera que el modelo da malos resultados.
- Muchas de las variables cuantitativas disponibles se han tenido que convertir a categóricas, lo que dificulta el modelado de una regresión lineal múltiple.
- La transformación de la variable de salida ha mejorado la performance del modelo, a costa de perder interpretabilidad.

Como líneas de trabajo futuras se propone:
- Intentar utilizar regularización Elastic net.
- Estudiar más interacciones entre variables, o generación de nuevas variables a partir de las actuales para ver si correlan en mayor medida con el target.


 

