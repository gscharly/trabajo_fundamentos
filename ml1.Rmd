---
title: "Machine Learning 1"
author: 
- name: Paula Santamaría Villaverde
- name: Manuel Jesús Pertejo Lope
- name: Carlos Gómez Sánchez
date: "21 de marzo de 2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      section_divs: true
    theme: "sandstone"
    highlight: "zenburn"
    code_folding: "hide"
---

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(cowplot)
library(scales)
library(tidyr)
library(InformationValue)
library(rpart)
library(rpart.plot)
library(e1071)
library(pROC)
library(gridExtra)
library(glmnetUtils)
library(kmed)
library(Rtsne)
library(doFuture)
library(randomForest)
library(grid)
library(party)
library(ROCR)
#library(tidymodels)
source('utils/pred_type_distribution.R')
source('utils/calculate_roc.R')
source('utils/plot_roc.R')
source('utils/metrics_function.R')
set.seed(10)
```

```{r paleta34}
#Paleta creada a partir de Brewer Dark2
palette34 = c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D", "#666666","#1DA632","#9E1B84","#1B849E","#7CD902","#A61E22","#1EA6A2","#8AE729", "#AEB370","#E7E529","#70B397","#29E786","#292BE7","#E7292B","#B3708D","#AFE602", "#02E6AB","#02AFE6","#92A61D","#771B9E","#1DA676","#1D4DA6","#02D95F","#02D9CB","#D9CB02","#B37570","#B39670")
```


# Aprendizaje no supervisado

```{r}
housesTrain <- read.csv('base_train.csv')
housesTest <- read.csv('base_test.csv')
```

En primer lugar, vamos a definir la base de datos con las variables cuantitativas y cualitativas ordinales a partir del dataset *housesTrain*.
Las variables cuantitativas las cogemos estandarizadas para eliminar el efecto de las distintas escalas de medida y las ordinales las estandarizamos al crearlas.

```{r}
housesTrainCluster <- housesTrain %>% dplyr::select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)

#las variables categóricas pasan a ser factores ordenados
housesTrainCluster['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() %>% scale()

housesTrainCluster['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric() %>% scale()

housesTrainCluster['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()

housesTrainCluster['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() %>% scale()

housesTrainCluster['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() %>% scale()
```

## Estudio de la tendencia de clustering

Un método de clustering permite encontrar agrupaciones en un conjunto de datos, pero no siempre estas agrupaciones existen de manera natural. En nuestro caso, hemos tomado la variable precio para dividir el dataset y poder emplear métodos de clasificación sobre él. Sin embargo, vamos a estudiar la tendencia de clustering real que tienen nuestros datos.

Visual Assessment of cluster Tendency (VAT) es método que permite evaluar visualmente si los datos muestran indicios de algún tipo de agrupación.

```{r}
datos = housesTrainCluster
mat_dist <- dist(datos, method = "euclidean")

fviz_dist(dist.obj = mat_dist, show_labels = FALSE) +
          theme(legend.position = "bottom")

```
El estadístico Hopkins permite evaluar la tendencia de clustering de un conjunto de datos mediante el cálculo de la probabilidad de que dichos datos procedan de una distribución uniforme, es decir, estudia la distribución espacial aleatoria de las observaciones.

H=∑ni=1yi∑ni=1xi+∑ni=1yi

Valores de H en torno a 0.5 indican que ∑ni=1xi y ∑ni=1yi son muy cercanos el uno al otro, es decir, que los datos estudiados se distribuyen uniformemente y que por lo tanto no tiene sentido aplicar clustering. Cuanto más se aproxime a 0 el estadístico H, más evidencias se tienen a favor de que existen agrupaciones en los datos y de que, si se aplica clustering correctamente, los grupos resultantes serán reales. 

```{r}
# Estadístico H para el set de datos iris
hopkins(data = datos, n = nrow(datos) - 1)
```


## k-means

Ahora, vamos a buscar el número de clústers óptimo. La idea es que el número seŕa él óptimo cuando los individuos de un mismo grupo sean lo más homogéneos posible y los individuos pertenecientes a distintos grupos sean lo más heterogéneos posible. Esto es lo mismo a buscar una divisióndonde una suma de cuadrados entre (betweens) sea suficientemente grande y, por tanto, una suma de cuadrados dentro lo suficientemente pequeña (withins).

En las siguientes dos gráficas, se muestra el *betweens* y el *withins* para un número de clusters desde 2 hasta 15. Se obserba en ambas, que el punto donde se produce el cambio de tendencia es aproximadamente en el 3. Por esto se va a probar a realizar una clasificación en base a 3 grupos. 

```{r kmeansKoptimo1}
set.seed(123)
bss <- kmeans(housesTrainCluster,centers=1)$betweenss
 for (i in 2:15) bss[i] <- kmeans(housesTrainCluster,centers=i)$betweenss
plot(1:15, bss, type="b", xlab="Number of Clusters",ylab="Sum of squares between groups",pch=19, col=palette34[2])
```

```{r kmeansKoptimo2}
SSW <- vector(mode = "numeric", length = 15)
SSW[1] <- (n - 1) * sum(apply(X = housesTrainCluster, MARGIN = 2, FUN = 'var'))
for (i in 1:15) SSW[i] <- sum(kmeans(housesTrainCluster,centers=i,nstart=25)$withinss)
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col=palette34[2])
```

```{r}
clusters3=kmeans(housesTrainCluster,centers=3,nstart=25)
clusters3
```

Observando esl ratio de la suma de cuadrados entre-clusters y la suma de cuadrados totales, vemos que el porcentaje de varianza explicada por el modelo respecto al total de varianza observada es inferior al 50%. 


###PCA + visualización

```{r}
pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- clusters3$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34)
```

##Hierarchical

Se aplicará un cluster aglomerativo y se emplearán diferentes métodos de medida de la distancia entre clusters. 

```{r hierarchical}
mat_dist <- dist(housesTrainCluster, method = "euclidean")
hc1 <- hclust(d, method = "single" )
den = as.dendrogram(hc1)
plot(den)
```

A pesar de que el dendograma no ofrece una visualización clara debido al alto número de observaciones, sí se puede percibir como dos grandes clusteres siendo uno de ellos (el de la izquierda), aproximadamente, tres veces mayor que el otro.

Una vez creado el dendrograma, vamos a evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones empleando el coeficiente de correlación entre la altura de los nodos del dendrograma (distancia cophenetic) y la matriz de distancias original. Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos. Esta medida puede emplearse como criterio de ayuda para escoger entre los distintos métodos de linkage.

```{r}
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")
hc_euclidea_single  <- hclust(d = mat_dist, method = "single")
hc_euclidea_centroid  <- hclust(d = mat_dist, method = "centroid")
cor(x = mat_dist, cophenetic(hc_euclidea_complete))
cor(x = mat_dist, cophenetic(hc_euclidea_average))
cor(x = mat_dist, cophenetic(hc_euclidea_single))
cor(x = mat_dist, cophenetic(hc_euclidea_centroid))
```

A la vista de los resultados, el método que mejor refleja las distancias reales es el *average*. Este método supone un compromiso entre las dos características de los métodos en los que se emplea la distancia mínima (*single*) y en los que se emplea la máxima (*complete*). Un compromiso entre ser sensible a datos atípicos y manejar formas de clusters no elípticas.

## DBSCAN

Los métodos de clusterin empleados anteriormenete, son buenos encontrando agrupaciones con forma esférica y sensibles a outliers,k-means, pero fallan al tratar de identificar formas arbitrarias que es lo que tenemos en este caso.

DBSCAN evita este problema siguiendo la idea de que, para que una observación forme parte de un cluster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters están separados por regiones vacías o con pocas observaciones.


```{r dbscanEpsi}
datos = housesTrainCluster
dbscan::kNNdistplot(datos, k = 5)
```

La curva tiene el punto de inflexión en torno a 0.8, por lo que se escoge este valor como epsilon para DBSCAN.

```{r dbscan}
dbscan_cluster <- fpc::dbscan(data = datos, eps = 0.8, MinPts = 50)
head(dbscan_cluster$cluster)

# Visualización de los clusters

pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- dbscan_cluster$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34)


#fviz_cluster(object = dbscan_cluster, data = datos, stand = FALSE,
#             geom = "point", ellipse = FALSE, show.clust.cent = FALSE,
#             pallete = "palette34") +
#  theme_bw() +
#  theme(legend.position = "bottom")
```


# Métodos de reducción de dimensionalidad

```{r}
housesTrain <- read.csv('base_train.csv')
```

## PCA

PCA debe trabajar con variables numéricas que además están normalizadas. Por ello, elegimos las 4 variables numéricas con las que trabajamos en la parte de regresión lineal múltiple (raíz cuadrada de la distancia, logaritmo de la parcela, latitud y longitud). Con las dos primeras componentes se explicaría un 66,3% de la varianza.


```{r}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```


```{r}
train <- housesTrain %>% select(-Price)
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat, price_label_high)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

Se prueba a representar las dos componentes principales en dos dimensiones, intentando añadir una tercera dimensión en forma de color utilizando otras variables, con el objetivo de ver si hay diferencias en estas direcciones. Vemos por ejemplo que las casas caras parece que se concentran (ya se vio anteriormente que la localización influía en el precio), mientras que las casas baratas están más desperdigadas. 

Si pintamos la región, se pueden diferenciar fácilemente, ya que se han utilizado las coordenadas como entrada del PCA.


```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=price_label_high)) + geom_point() + labs(title='PCA: price label')
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=Regionname)) + geom_point() + labs(title='PCA: Regionname')
```


## MDS
Usaremos la distancia euclídea para realizar el MDS (debería ser muy parecido a PCA). Al tener bastantes datos, vamos a muestrear el train primero.

```{r}
set.seed(10)
train_sample <- housesTrain[sample(nrow(housesNum), 2500),]
train_num_sample <- train_sample %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
```

```{r}
# Distancia euclídea
d <- dist(train_num_sample) 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```

```{r}
df_fit <- data.frame(fit$points)
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='MDS: price_label_high')
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname')
```
Se prueba también la distancia de manhattan

```{r}
# Distancia euclídea
d <- dist(train_num_sample, method = 'manhattan') 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```

```{r}
df_fit <- data.frame(fit$points)
# No estoy seguro de hacer el join bien, se supone que es por indice
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='MDS: price_label_high')
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname')
```

## tSNE
Probamos tsne sobre nuestras variables numéricas escaladas, para que valores extremos no nos den problemas. El parámetro que se suele tocar es el perplexity, que es algo parecido al número de vecinos más cercanos cuando se comparan las distribuciones que utiliza tSNE.
Fuente: https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

De nuevo superponiendo distintas variables sobre el gráfico en dos dimensiones, podemos ver 

```{r}
houses_num_unique <- unique(housesNum)
tsne <- Rtsne(houses_num_unique, dims = 2, perplexity=50, verbose=TRUE, max_iter = 500)
```

```{r}
df_tsne <- data.frame(tsne$Y)
df_train_no_dup <- housesTrain[!duplicated(housesTrain[c("sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std")]),]
df_train_no_dup$X1 <- df_tsne$X1
df_train_no_dup$X2 <- df_tsne$X2
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='tSNE: price_label_high')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=sell_rate_cat)) + geom_point() + labs(title='tSNE: sell_rate_cat')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='tSNE: Regionname')
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Type)) + geom_point() + labs(title='tSNE: Type')

```


Mapa de Melbourne :) (lo podemos poner el algún sitio)
```{r}
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=price_label_high)) + geom_point()
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Regionname)) + geom_point()
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Type)) + geom_point()
```

# Aprendizaje supervisado


## Regresión logística

En este apartado, se va imlementar un modelo regresión logística sobre los datos de entrenamiento. 

```{r}
# Se creea la variable objetivo (75-25) y se eliminan los antiguos targets
train <- read.csv('base_train.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
train <- train %>%  select(-filter_cols)
train$price_label_high <- as.factor(ifelse(train$price_label_high==TRUE,1,0))


lr_model <- glm(price_label_high ~ ., family = binomial(link = 'logit'), data = train)
summary(lr_model)
```

El resultado óptimo para este modelo, en cuanto a optimización de la f1_score se refiere es la siguiente:

```{r}
beta <- 1
prob_values <- lr_model %>% predict(train[,1:13], type = "response")
pred_lr <- prediction(prob_values, train$price_label_high)
perf_lr <- performance(pred_lr, "prec", "rec")

f1_best_lr <- (1+beta^2)*perf_lr@y.values[[1]]*perf_lr@x.values[[1]]/(beta^2*perf_lr@y.values[[1]]+perf_lr@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_lr) # Mejor f1 score para este modelo
prec_lr_opt=perf_lr@y.values[[1]][optimo]
rec_lr_opt=perf_lr@x.values[[1]][optimo]
f1_measure_lr_opt <- (1+beta^2)*prec_lr_opt*rec_lr_opt/(beta^2*prec_lr_opt+rec_lr_opt)
threshold_optimo_lr <- perf_lr@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_lr_opt, precision = prec_lr_opt, recall = rec_lr_opt, threshold = threshold_optimo_lr))
```


```{r,message=F}
preds_train <- as.factor(ifelse(prob_values > threshold_optimo_lr ,1,0))
cm <- caret::confusionMatrix(preds_train, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(train$price_label_high,prob_values)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)
```

Como se puede observar, el valor no es muy alto, por lo que este resultado claramente puede mejorarse.

Si echamos un ojo a la columna de los p-valores obtenidos durante el entrenamiento del modelo, podemos ver que algunos de ellos nos indican que no se puede asegurar que los coeficientes asociados a ciertas variables sean diferentes de 0. Sin embargo, estos p-valores tan bajos van asociados a ciertos niveles de variables categóricas, por lo que si eliminásemos la variable estaríamos perdiendo información del resto de niveles, la cual sí que es relevante. 

Sin embargo, se podría aplicar alguna medida de regularización a través de la cual obtener más información sobre las posibles variables que no aportan demasiado al modelo y podrían ser eliminadas. A continuación, se va a utilizar una regresión logística Lasso y observar los coeficientes obtenidos para cada una de las variables:

```{r}
set.seed(123)
cv.lasso <- cv.glmnet(price_label_high~., data = train, alpha = 1, type.measure = "deviance", family = "binomial") # Con 10-fold cross-validation
#plot(cv.lasso)
coef(cv.lasso, cv.lasso$lambda.1se)
```

```{r}
lr_regularized <- glm(price_label_high ~. -sell_rate_cat - Method - bed_cat, family = binomial(link = 'logit'), data = train)
summary(lr_regularized)
```

```{r}
beta <- 1
prob_values <- lr_regularized %>% predict(train[,1:13], type = "response")
pred_lr <- prediction(prob_values, train$price_label_high)
perf_lr <- performance(pred_lr, "prec", "rec")

f1_best_lr <- (1+beta^2)*perf_lr@y.values[[1]]*perf_lr@x.values[[1]]/(beta^2*perf_lr@y.values[[1]]+perf_lr@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_lr) # Mejor f1 score para este modelo
prec_lr_opt=perf_lr@y.values[[1]][optimo]
rec_lr_opt=perf_lr@x.values[[1]][optimo]
f1_measure_lr_opt <- (1+beta^2)*prec_lr_opt*rec_lr_opt/(beta^2*prec_lr_opt+rec_lr_opt)
threshold_optimo_lr <- perf_lr@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_lr_opt, precision = prec_lr_opt, recall = rec_lr_opt, threshold = threshold_optimo_lr))
```

```{r, message=F}
preds_train <- as.factor(ifelse(prob_values > threshold_optimo_lr ,1,0))
cm <- caret::confusionMatrix(preds_train, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(train$price_label_high,prob_values)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)
```

Vemos que, al eliminar ciertas variables que simplemente estaban introduciendo información redundante al modelo, el rendimiento obtenido es prácticamente el mismo (incluso mejora muy mínimamente).
  
```{r}

probs_labels <- data.frame("probs" = prob_values, "labels" = train$price_label_high)

probs_labels %>% ggplot(aes(x=probs, color=labels)) +
  geom_density() + geom_vline(aes(xintercept=threshold_optimo_lr),
            color="blue", linetype="dashed")


```

A continuación, se procede a comprobar el funcionamiento de este modelo de regresión logística sobre los datos de test:

```{r}
test<- read.csv('base_test.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
test <- test %>%  select(-filter_cols)
test$price_label_high <- as.factor(ifelse(test$price_label_high==TRUE,1,0))


x_test <- test[,13]
prob_values_test <- lr_regularized %>% predict(test[,1:13], type = "response")
preds_test <- as.factor(ifelse(prob_values_test > threshold_optimo_lr ,1,0))

metrics_function_num(preds_test, test, 'price_label_high')
```


```{r, message=F}

cm <- caret::confusionMatrix(preds_test, test$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_lr_test <- roc(test$price_label_high,prob_values_test)
roc_plot <- ggplotROCCurve(roc_lr_test)


grid.arrange(roc_plot, cm_plot, ncol=2)
```


Se puede comprobar que el AUC se mantiene intacto. Si bien es cierto que la accuracy y la f1_score disminyen ligeramente, se puede afirmar que la regresión no sobreajusta para nada los datos.


## Árboles de decisión
Se van a probar árboles de decisión sobre nuestro conjunto de datos. En primer lugar, se utilizarán las variables que se consideraron importantes en el análisis exploratorio, para después probar distintas formas de tuning e intentar evitar el sobreaprendizaje del árbol.

### Utilizando las variables estudiadas
Se utilizarán directamente las variables estudiadas en el análisis exploratorio, dejando de lado transformaciones y estandarizaciones, además de variables categóricas con muchas categorías. Además, si recordamos, para la regresión lineal tuvimos que convertir varias features numéricas en categóricas (número de habitaciones, año de construcción...). Podemos probar a introducir las variables originales para ver qué cortes propone el árbol.


```{r load_variables_high}
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label_high")]
n = dim(housesTrainOrigVar)[1]
# Hay que factorizar el label para que se construya bien el árbol
housesTrainOrigVar$price_label_high <- as.factor(housesTrainOrigVar$price_label_high)
```


En los cortes se tiene en cuenta el número de habitaciones como corte principal (diferenciando entre casas pequeñas y grandes). El siguiente corte se realiza en base a la región (localización). Parece que la región por la que se discrimina es Southern Metropolitan. Si la casa es pequeña y está en cualquier región que no sea esa, se clasifica como barata la mayoría de las veces. 

Algo que también se puede ver es que hay varios nodos con pocas muestras (los que vienen de la distinción de tipo de casa es Southern Metropolitan). Vemos además que hay un bastantes falsos negativos (casas caras que se etiquetan como baratas).


```{r tree2_price_label}
housesTree2 = rpart(price_label_high ~ ., data = housesTrainOrigVar)
rpart.plot(housesTree2, type=4, fallen.leaves = FALSE, tweak =1.75)
rpart.rules(housesTree2, clip.facs = TRUE)
```

```{r metricsTree2}
preds <- predict(housesTree2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```

### Podando el árbol 
Intentamos eliminar esas últimas hojas que hemos comentado. Prácticamente no habría diferencia entre tener 7 hojas u 8. Podemos probar a ver si con 4 hojas el error aumenta mucho.

```{r}
printcp(housesTree2)
```
```{r}
plotcp(housesTree2)
```

Con 7 hojas, el error es prácticamente el mismo. Incluso mejora un poco el f1-score.

```{r pruneTree2}
pruneTREE2 = prune(housesTree2, cp = housesTree2$cptable[4,"CP"])
rpart.plot(pruneTREE2, type=4, fallen.leaves = FALSE, tweak =1.75)
```

```{r metricsPruneTree2}
preds <- predict(pruneTREE2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```

```{r}
preds_prob <- predict(pruneTREE2, housesTrainOrigVar, type = "prob")
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
cost_of_fp <-1
cost_of_fn <- 1
th <- 0.3
roc <- calculate_roc(predictions, 'price_label_high', cost_of_fp, cost_of_fn, n=100)
plot_roc(roc, th, cost_of_fp, cost_of_fn)
```

```{r}
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```


Bajando la complejidad hasta quedarnos solo con 4 hojas, observamos que el error aumenta en un 4% (hasta el 17%) y que el f1-score disminuye hasta 0.55, pero el árbol es mucho más fácil de interpretar. Las casas con menos de 4 habitaciones directamente son una hoja, con el 75% de los datos, y con una tasa de acierto aceptable (el 84% serían baratas). Las casas con más de 4 habitaciones en Southern Metropolitan (son solo el 9%) se clasifican como caras el 86% de las veces. La última división es en función de la distancia al centro, y ya falla más.

```{r pruneTree3}
pruneTREE3 = prune(housesTree2, cp = housesTree2$cptable[3,"CP"])
rpart.plot(pruneTREE3, type=4, fallen.leaves = FALSE, tweak =1)
```

```{r metricsPruneTree3}
preds <- predict(pruneTREE3, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```

```{r}
preds_prob <- predict(pruneTREE3, housesTrainOrigVar, type = "prob")
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```

### Eliminando variables
Hemos visto que en los árboles anteriores, las variables por las que se realizaban las divisiones eran *Rooms*, *Regionname*, *Distance*, *Type*. Previsiblemente, si solo introducimos estas variables, el resultado será el mismo, reduciendo operaciones al árbol.


```{r}
housesTrainImpVars <- housesTrain[c("Rooms", "Type", "Distance", "Regionname", "price_label_high")]
n = dim(housesTrainImpVars)[1]
# Hay que factorizar el label para que se construya bien el árbol
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
```

```{r}
housesTree4 = rpart(price_label_high ~ ., data = housesTrainImpVars)
pruneTREE4 = prune(housesTree4, cp = housesTree4$cptable[4,"CP"])
preds <- predict(pruneTREE4, type = "class")
table(pred = preds, obs = housesTrainImpVars$price_label_high)
metrics_function(preds, housesTrainImpVars, 'price_label_high', bool=TRUE)
```


## Test
Observamos que no se produce sobreajuste, ya que las métricas en test son muy parecidas a train (utilizando el árbol entrenado con 4 variables)

```{r}
housesTest <- read.csv('base_test.csv')
```

```{r}
preds <- predict(pruneTREE4, housesTest, type = "class")
table(pred = preds, obs = housesTest$price_label_high)
metrics_function(preds, housesTest, 'price_label_high', bool = TRUE)
```
```{r}
preds_prob <- predict(pruneTREE4, housesTest, type = "prob")
predictions <- data.frame(price_label_high=housesTest$price_label_high, pred=preds_prob[,2])
cost_of_fp <-1
cost_of_fn <- 1
th <- 0.3
roc <- calculate_roc(predictions, 'price_label_high', cost_of_fp, cost_of_fn, n=100)
plot_roc(roc, th, cost_of_fp, cost_of_fn)
```



```{r}
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```

## Random Forest
En este apartado se van a utilizar random forests para el problema de clasificación. Al no estar demasiado expuesto a los outliers, y aceptar variables categóricas como entrada, la primera prueba que se realizará será con la mayoría de variables del conjunto de datos, y con unos parámetros iniciales que después se modificarán en una etapa de selección de hiperparámetros.

```{r}
housesTrain <- read.csv('base_train.csv')
```

```{r}
set.seed(10)
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label_high")]
housesTrainOrigVar$price_label_high <- as.factor(housesTrainOrigVar$price_label_high)
rf2 <- randomForest(price_label_high ~ ., data=housesTrainOrigVar, ntree=100)
```


```{r}
preds <- predict(rf2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```


```{r message=FALSE}
roc <- roc(housesTrain$price_label_high, rf2$votes[,2])
ggplotROCCurve(roc)
```

```{r}
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=rf2$votes[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) 
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) 
```

Tomando como referencia las métricas, y que las variables importantes son bastante lógicas, este modelo arroja resultados muy válidos. Además, se podría utilizar como una primera parte en una etapa de ensamblado de modelos: se podría utilizar un Random Forest muy sobre ajustado (aumentando el número máximo de nodos terminales por ejemplo) para seleccionar las variables más importantes, que después se utilizarían como variables en otro modelo.

### Selección de variables

Si observamos la importancia de las variables del anterior (medida como la caída media en el índice gini que produce cada variable), vemos que las más importantes son aquellas relacionadas con la situación geográfica de la vivienda (cómo ya se había visto anteriormente): coordenadas, distancia al centro y región están en las 6 primeras posiciones, además de variables que informan sobre el tamaño de la vivienda (número de habitaciones y tamaño de la parcela). 


```{r}
# Extracts variable importance (Mean Decrease in Gini Index)
# Sorts by variable importance and relevels factors to match ordering
var_importance <- data_frame(variable=setdiff(colnames(housesTrainOrigVar), "price_label_high"),
                             importance=as.vector(importance(rf2)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p + scale_fill_discrete(name="Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```

Podemos probar si seleccionado las variables más imporantes y volviendo a entrenar un modelo con los mismos parámetros, obtenemos resultados parecidos.

Si nos quedamos con las 6 primeras, el accuracy desciende ligeramente hasta 0.89, pero el f1 score baja hasta 0.77. Si se incluye la variable *Type* adicionalmente, se consiguen resultados muy parecidos a los del primer modelo, reduciendo la complejidad.

```{r}
set.seed(10)
housesTrainImpVars <- housesTrain[c("Rooms", "Distance", "LandsizeImp",
                                    "Lattitude", "Longtitude", "Regionname", "Type",
                                     "price_label_high")]
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
rf3 <- randomForest(price_label_high ~ ., data=housesTrainImpVars, ntree=100)
```


```{r}
preds <- predict(rf3, type = "class")
table(pred = preds, obs = housesTrainImpVars$price_label_high)
metrics_function(preds, housesTrainImpVars, 'price_label_high', bool=TRUE)
```

```{r}
var_importance <- data_frame(variable=setdiff(colnames(housesTrainImpVars), "price_label_high"),
                             importance=as.vector(importance(rf3)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p + scale_fill_discrete(name="Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```


## Utilizando cross validation
Como primera prueba, y teniendo en cuenta que las variables importantes el modelo son bastante lógicas, los resultados de este modelo son bastante buenos. Ahora se va a intentar tunear el número de árboles y la profundidad máxima de los mismos con grid search y CV, para intentar obtener unos mejores resultados.


- Número de árboles
- Número de variables que se muestrean aleatoriamente al crear los árboles
- Mínimo número de puntos para que un nodo se divida.

Realizamos 10 Fold cross validation con 5 iteraciones.

Se utilizan las variables que se han comentado en el apartado anterior.


```{r}
set.seed(10)
rf_model <- rand_forest(trees=tune(), mtry=tune(), min_n=tune()) %>% set_mode('classification') %>% set_engine('randomForest')
houses_rec <- recipe(price_label_high ~ ., data=housesTrainImpVars)
#rf_model %>% parameters() %>% update(trees = trees(c(25,1000)), mtry=mtry(c(1,5)), min_n=min_n(c(2,20)))
# Create workflow
rf_workflow <- workflow() %>% add_model(rf_model) %>% add_recipe(houses_rec)
rf_workflow

rf_param <- rf_workflow %>% parameters() %>% update(trees = trees(c(50,500)), mtry=mtry(c(1,5)), min_n=min_n(c(2,20)))

# Create grid
rf_grid <- grid_regular(rf_param, levels=3)
rf_grid
#rf_fit <- fit(rf_model, price_label_high ~ ., housesTrainOrigVar)
# Valores por defecto
#trees()
#mtry()
#min_n()
```

```{r message=FALSE}
# Parallelize
all_cores <- parallel::detectCores(logical = FALSE) - 2
registerDoFuture()
cl <- makeCluster(all_cores)
plan(future::cluster, workers = cl)
```

```{r}
houses_folds <- vfold_cv(housesTrainOrigVar, v = 10, repeats = 3)
houses_folds
rf_search <- tune_grid(rf_workflow, grid = rf_grid, resamples=houses_folds, param_info = rf_param)
```

```{r}
#Execute after finishing training
stopCluster(cl)
```

```{r}
autoplot(rf_search) +
    labs(title = "Results of Grid Search for Two Tuning Parameters of a Random Forest")
```

```{r}
show_best(rf_search, metric='roc_auc')
```

```{r}
# Prepare for final prediction
rf_param_final <- select_best(rf_search, "roc_auc")
rf_wflow_final <- finalize_workflow(rf_workflow, rf_param_final)
rf_wflow_final_fit <- fit(rf_wflow_final, data = housesTrainOrigVar)
```


## Test

```{r}
housesTest <- read.csv('base_test.csv')
housesTestOrigVar <- housesTest[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label_high")]
housesTestOrigVar$price_label_high <- as.factor(housesTestOrigVar$price_label_high)
#levels(housesTestOrigVar$price_label_high) <- c("barata", "cara")

housesTestOrigVar$.pred <- predict(rf_wflow_final_fit, 
                          new_data = housesTestOrigVar)$.pred_class
housesTestOrigVar$.pred_TRUE <- predict(rf_wflow_final_fit, 
                          new_data = housesTestOrigVar, type='prob')$.pred_TRUE
metrics(housesTestOrigVar, truth = price_label_high, .pred, .pred_TRUE)
```

```{r}
table(pred = housesTestOrigVar$.pred, obs = housesTestOrigVar$price_label_high)
metrics_function(housesTestOrigVar$.pred, housesTestOrigVar, 'price_label_high', bool=TRUE)
```


```{r}
predictions <- data.frame(price_label_high=housesTest$price_label_high, pred=housesTestOrigVar$.pred_TRUE)
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) 
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) 
```





## kNN
El siguiente algoritmo que se va a probar es kNN. Para ello, necesitamos que los datos estén de nuevo estandarizados, ya que es un método basado en distancias, y las variables necesitan estar en el mismo rango de valores.

PARA HACER: intentar probar otras distancias

```{r}
housesTrain <- read.csv('base_train.csv')
housesTest <- read.csv('base_test.csv')
# Para que funcione confusionMatrix de caret, el target debe ser un factor
housesTrain$price_label <- as.factor(housesTrain$price_label)
housesTrain$price_label_high <- as.factor(housesTrain$price_label_high)
housesTest$price_label <- as.factor(housesTest$price_label)
housesTest$price_label_high <- as.factor(housesTest$price_label_high)
```

```{r knn_functions}
perform_knn <- function(train, test, label, k, cols) {
  predictions <- knn(train[,cols], test[,cols], k=k, cl=train[,label])
  test_metrics <- metrics_function(predictions, test, label, bool=TRUE)
  return(test_metrics)
  
}
```


```{r knn1}
num_std_cols <- c('sqrt_distance_std', 'log_landsize_std', 'lattitude_std', 'longtitude_std')
metrics <- perform_knn(housesTrain, housesTest, 'price_label_high', 1, num_std_cols)
metrics
```

## k óptimo
ESTO HABRIA QUE HACERLO SOBRE VALIDACIÓN NO?
Para price_label:

```{r}
plot_acc_f1_k <- function(train, label, cols){
  long = 15
  accuracy = rep(0,long)
  f1score = rep(0,long)
  recall = rep(0,long)
  precision = rep(0,long)
  for (i in 1:long)
  {
    prediccion_knn_cv =knn.cv(train[,cols], 
                              k=i, cl=train[,label])
    accuracy[i] = sum(prediccion_knn_cv == train[,label]) /nrow(train)
    recall[i] = sum(prediccion_knn_cv == train[,label] & train[,label] == TRUE) / sum(train[,label] == TRUE)
    precision[i] = sum(prediccion_knn_cv == train[,label] & prediccion_knn_cv == TRUE) / sum(prediccion_knn_cv == TRUE)
    f1score[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])
  }
  resultados_knn = as.data.frame(cbind(accuracy,f1score,precision,recall))
  resultados_knn = resultados_knn %>% mutate(index=as.factor(seq(1:long)))
  
  max(resultados_knn$f1score)
  which.max(resultados_knn$f1score)
  
  
  p1 <- ggplot(data=resultados_knn,aes(x=index,y=accuracy)) + 
    geom_col(colour="cyan4",fill="cyan3")+
    ggtitle("Accuracy")
  
  
  p2 <- ggplot(data=resultados_knn,aes(x=index,y=f1score)) + 
    geom_col(colour="orange4",fill="orange3") +
    ggtitle("F1_score values")
  
  plot_grid(p1, p2, rel_heights = c(1/2, 1/2))
  }

plot_acc_f1_k(housesTrain, 'price_label', num_std_cols)
```
Para k=5 obtenemos un accuracy y un f1 score entre 0.75 y 0.8. Se observa que a medida que k va aumentando, las métricas no incrementan.

HABRIA QUE SACAR F1
```{r knn5}
# En train
prediccion_knn5_train =knn.cv(housesTrain[,num_std_cols], 
                              k=5, cl=housesTrain$price_label)
metrics_function(prediccion_knn5_train, housesTrain, 'price_label')

#En test
prediccion_knn5_test=knn(housesTrain[,num_std_cols], housesTest[,num_std_cols],
                         k=5, cl=housesTrain$price_label)
metrics_function(prediccion_knn5_test, housesTest, 'price_label')
```


Para price_label_high, vemos de nuevo que el f1-score es menor. Los mejores valores de k podrían ser 5 o 7.

```{r}
plot_acc_f1_k(housesTrain, 'price_label_high', num_std_cols)
```
```{r knn7high}
# En train
prediccion_knn7_train =knn.cv(housesTrain[,num_std_cols], 
                              k=7, cl=housesTrain$price_label_high)
metrics_function(prediccion_knn7_train, housesTrain, 'price_label_high')

#En test
prediccion_knn7_test=knn(housesTrain[,num_std_cols], housesTest[,num_std_cols],
                         k=7, cl=housesTrain$price_label_high)

metrics_function(prediccion_knn7_test, housesTest, 'price_label_high')
```



# Naive Bayes
Como simplificación de redes bayesianas, vamos a entrenar un modelo utilizando Naive Bayes.

```{r}
housesTrain <- read.csv('base_train.csv')
```

## Primer modelo

Como primera aproximación, utilizaremos las mismas variables utilizadas en el modelo de regresión lineal múltiple (aunque el escalado debería ser indiferente). Para variables numéricas asume una distribución gausiana para calcular la verosimilitud (likelihood). Lo que saca el mod es la media en la primera columna y la varianza en la segunda para las distribuciones condicionadas.

```{r}
mod <- naiveBayes(price_label_high ~ sqrt_distance_std + log_landsize_std + lattitude_std + longtitude_std + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data=housesTrain)
mod
```

Métricas en train. El f1 es flojillo, ya que ni el recall ni el precision son demasiado buenos. 

```{r}
preds <- predict(mod, housesTrain, type = "class")
table(pred = preds, obs = housesTrain$price_label_high)
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```
Cambiando el th:
```{r}
preds_prob <- predict(mod, housesTrain, type = "raw")
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
th = 0.5
predictions <- predictions %>% mutate(predict_class = ifelse(pred >=th, TRUE, FALSE))
table(pred = predictions$predict_class, obs = predictions$price_label_high)
metrics_function(predictions$predict_class, housesTrain, 'price_label_high', bool=TRUE)
```


```{r}
cost_of_fp <-1
cost_of_fn <- 1
th <- 0.3
roc <- calculate_roc(predictions, 'price_label_high', cost_of_fp, cost_of_fn, n=500)
plot_roc(roc, th, cost_of_fp, cost_of_fn)
```


```{r}
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```

```{r}
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) 
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) 
```

## Otros modelos probados

Probando algún modelo con menos variables. A modo de resumen:

- Utilizando solo las categóricas, los resultados empeoran, con un accuracy de 0.8079153, un f1 de 0.6207523 y una auc roc de 0.8552.
- Utilizando únicamente las numéricas, el resultado lógicamente empeora, ya que Naive Bayes asume una distribución normal de las entradas numérocas, y trabaja mejor con categóricas al calcular las probabilidades de una forma más precisa conociendo las distribuciones. En este caso, tenemos un accuracy de 0.7727566, un f1 de 0.4759075 y una auc roc de 0.7913


```{r}
mod <- naiveBayes(price_label_high ~ sqrt_distance_std + log_landsize_std + lattitude_std + longtitude_std, data=housesTrain)
mod
```

```{r}
preds <- predict(mod, housesTrain, type = "class")
table(pred = preds, obs = housesTrain$price_label_high)
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```


```{r}
preds_prob <- predict(mod, housesTrain, type = "raw")
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
th = 0.5
predictions <- predictions %>% mutate(predict_class = ifelse(pred >=th, TRUE, FALSE))
table(pred = predictions$predict_class, obs = predictions$price_label_high)
metrics_function(predictions$predict_class, housesTrain, 'price_label_high', bool=TRUE)
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```

## Test: mejor modelo

En test:

```{r}
housesTest <- read.csv('base_test.csv')
```

```{r}
mod <- naiveBayes(price_label_high ~ sqrt_distance_std + log_landsize_std + lattitude_std + longtitude_std + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data=housesTrain)
mod
```

```{r}
preds <- predict(mod, housesTest, type = "class")
table(pred = preds, obs = housesTest$price_label_high)
metrics_function(preds, housesTest, 'price_label_high', bool = TRUE)
```

```{r}
preds_prob <- predict(mod, housesTrain, type = "raw")
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=preds_prob[,2])
th = 0.5
predictions <- predictions %>% mutate(predict_class = ifelse(pred >=th, TRUE, FALSE))
```

```{r}
cost_of_fp <-1
cost_of_fn <- 1
th <- 0.5
roc <- calculate_roc(predictions, 'price_label_high', cost_of_fp, cost_of_fn, n=500)
plot_roc(roc, th, cost_of_fp, cost_of_fn)
```

```{r}
auc_roc <- auc(predictions$price_label_high, predictions$pred)
auc_roc
```



NOTA: ESTO MEJOR CUANDO ELIJAMOS EL MEJOR MODELO
Mediante estos gráficos, podemos ver de una forma gráfica las muestras que quedarían clasificadas en función del umbral que se elija.

```{r}
th = 0.5
preds_prob <- predict(rf2, type = "prob")
predictions <- data.frame(price_label_high=housesTrainOrigVar$price_label_high, pred=preds_prob[,2])
predictions$price_label_high <- ifelse(predictions$price_label_high==TRUE,1,0)
df_preds <- pred_type_distribution(predictions, th, 'price_label_high')
```


```{r}
 ggplot(data=df_preds, aes(x=price_label_high, y=pred)) + 
    geom_violin(fill=rgb(1,1,1,alpha=0.6), color=NA) + 
    geom_jitter(aes(color=pred_type), alpha=0.6) +
    geom_hline(yintercept=th, color="red", alpha=0.6) +
    scale_color_discrete(name = "type") +
    labs(title=sprintf("Threshold at %.2f", th))
```


