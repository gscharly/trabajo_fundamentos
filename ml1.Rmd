---
title: "Machine Learning 1"
author: 
- name: Paula Santamaría Villaverde
- name: Manuel Jesús Pertejo Lope
- name: Carlos Gómez Sánchez
date: "21 de marzo de 2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      section_divs: true
    theme: "sandstone"
    highlight: "zenburn"
    code_folding: "hide"
---

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(cowplot)
library(scales)
library(tidyr)
library(InformationValue)
library(rpart)
library(rpart.plot)
library(e1071)
library(pROC)
library(gridExtra)
library(glmnetUtils)
library(kmed)
library(Rtsne)
library(doFuture)
library(randomForest)
library(grid)
library(party)
library(ROCR)
library(tidymodels)
library(workflows)
library(knitr)
library(kableExtra)
library(gridExtra)
library(Information)
library(dendextend)
library(ape)
library(PRROC)
library(Hmisc)
library(plyr)
library(reshape2)

source('utils/pred_type_distribution.R')
source('utils/calculate_roc.R')
source('utils/plot_roc.R')
source('utils/metrics_function.R')
source('utils/plot_utils.R')
set.seed(10)
```

```{r paleta34}
#Paleta creada a partir de Brewer Dark2
palette34 = c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D", "#666666","#1DA632","#9E1B84","#1B849E","#7CD902","#A61E22","#1EA6A2","#8AE729", "#AEB370","#E7E529","#70B397","#29E786","#292BE7","#E7292B","#B3708D","#AFE602", "#02E6AB","#02AFE6","#92A61D","#771B9E","#1DA676","#1D4DA6","#02D95F","#02D9CB","#D9CB02","#B37570","#B39670")
```

```{r}
housesTrain <- read.csv('base_train.csv')
housesTest <- read.csv('base_test.csv')
housesVal <- read.csv('base_val.csv')
```

# Análisis exploratorio en base al nuevo objetivo
El dataset original estaba enfocado a la predicción del precio de viviendas en Melbourne. Para convertirlo en un problema de clasificación, vamos a utilizar la variable *Price* para diferenciar entre casas baratas y caras. Tenemos varias opciones:


- Utilizar la mediana de *Price* para tener un conjunto de datos balanceado, pero donde habrá muchas muestras cercanas con las que tendremos problemas a la hora de clasificar.
- Establecer el corte en un valor de *Price* mayor, donde tendremos un problema de desbalanceo de datos.

Se ha optado por la segunda opción, para introducir una mayor complejidad al problema de clasificación.

Con esta nueva variable objetivo, se procede a repetir parte del análisis multivariante realizado en el anterior informe, de cara a estudiar la relación con la nueva variable objetivo, y empezar a preseleccionar variables útiles para los modelos.

Además, se realizan test estadísticos para comprobar la independencia de las variables con el objetivo (que ahora es una variable objetivo). 

Contraste chi cuadrado: para comprobar la independencia entre dos variables categóricas.
H0: las variables son independientes
H1: las variables no son independientes

One way anova test: permite ver si hay diferencias entre la media de una variable numérica en los distintos grupos de una categórica-
H0: no hay diferencia en la media de los grupos
H1: al menos la media de un grupo es diferente

## Variables categóricas

__Regionname__

Representamos el número de casas baratas/cara en cada región, tanto en valor absoluto como porcentual. Se puede observar que el barrio con mayor número de casas caras es Southern Metropolitan, mientras que Eastern Metropolitan tiene mayor porcentaje de casas caras.

```{r}
bar_plot_target(housesTrain, "Regionname", "price_label_high")
```


Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Regionname, housesTrain$price_label_high)
```

__Type__

Recordemos los distintos tipos de casas que hay:

* h: houses, cottage, villa, semi, terrace
* t: townhouse
* u: unit

Se puede observar que al mayoría de casas caras se engloban en el tipo h.

```{r}
bar_plot_target(housesTrain, "Type", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Type, housesTrain$price_label_high)
```


__Method__

Los métodos de venta son los siguientes:

* S - property sold
* SP - property sold prior
* PI - property passed in
* VB - vendor bid
* SA - sold after auction

Porcentualmente, parece que hay más casas caras vendidas con el método VB.

```{r}
bar_plot_target(housesTrain, "Method", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Method, housesTrain$price_label_high)
```

__CouncilArea__

Parece que áreas como Bayside o Boroondara tienen casas más caras. Sin embargo, al haber tantas categorías, no es una variable para incluir en los modelos.

```{r fig.width=15, fig.height=4}
housesTrain %>% ggplot() + geom_bar(aes(x=CouncilArea, fill=price_label_high)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_fill_manual(values=palette34)
housesTrain %>% ggplot() + geom_bar(aes(x=CouncilArea, fill=price_label_high), position='fill') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_fill_manual(values=palette34)
```

Teniendo en cuenta el ámbito de aplicación, pensamos que aquellas viviendas que estén en la bahía serán más caras. Utilizando la variable *CouncilArea*, se han seleccionado aquellas zonas que rodean la bahía. 

Parece que hay mas casas caras porcentualmente en aquellas zonas con posibles vistas al mar.


```{r}
water_councils <- c('Wyndham', 'Hobsons Bay', 'Port Phillip', 'Bayside', 'Kingston', 'Frankston', 'Stonnington')
housesTrain$may_have_water <- factor(ifelse(housesTrain$CouncilArea %in% water_councils, TRUE, FALSE))

bar_plot_target(housesTrain, "may_have_water", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$may_have_water, housesTrain$price_label_high)
```

__rooms_cat__

Como es lógico, aquellas casas con más habitaciones son más caras.

```{r}
bar_plot_target(housesTrain, "rooms_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$rooms_cat, housesTrain$price_label_high)
```


__bath_cat__
Exactamente lo mismo que con el número de habitaciones.

```{r}
bar_plot_target(housesTrain, "bath_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$bath_cat, housesTrain$price_label_high)
```

__bed_cat__
De nuevo, si tiene más dormitorios, la casa suele ser cara.

```{r}
bar_plot_target(housesTrain, "bed_cat", "price_label_high")
```


Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$bed_cat, housesTrain$price_label_high)
```


__car_cat__
Mismas (y lógicas) conclusiones.

```{r}
bar_plot_target(housesTrain, "car_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$car_cat, housesTrain$price_label_high)
```

__Sell date__

Lo primero que hacemos con esta variable será extraer el año, ya que viene en formato dd/MM/yyyy, y probablemente el año nos proporcione más información acerca del precio de venta de la casa.


```{r CrearSellYear}
housesTrain$sellYear <- separate(housesTrain, Date, c('day','month', 'year'), sep = '/')$year
housesTrain$sellYear <-factor(housesTrain$sellYear, levels=c("2016", "2017"))
```

Observamos que solo hay 2 años de venta, y que no tiene demasiada relación con el precio.

```{r SellYearPrice, message=FALSE}
bar_plot_target(housesTrain, "sellYear", "price_label_high")
```

En este caso, no se puede rechazar la hipótesis nula.
```{r}
chisq.test(housesTrain$sellYear, housesTrain$price_label_high)
```

__sell_rate_cat__
Recordemos que esta variable indica si el barrio en el que está una casa tiene muchas casas vendidas. Parece que hay una ligera diferencia porcentualmente entre barrios populares y no populares en cuanto a casas caras.

```{r, message=FALSE}
bar_plot_target(housesTrain, "sell_rate_cat", "price_label_high")
```

Se puede rechazar la hipótesis de independencia.
```{r}
chisq.test(housesTrain$sell_rate_cat, housesTrain$price_label_high)
```


__year_built_cat__
El problema con esta variable venía con que había muchas casas sin fecha de construcción informada. Sí que se puede intuir que hay más casas caras porcentualmente en la categoría de casas antiguas.

```{r, message=FALSE}
bar_plot_target(housesTrain, "year_built_cat", "price_label_high")
```

```{r}
chisq.test(housesTrain$year_built_cat, housesTrain$price_label_high)
```


## Variables cuantitativas


__Correlaciones__


```{r}
housesNum <- housesTrain %>% mutate(log_landsize = log10(LandsizeImp), sqrt_distance = sqrt(Distance), log_room_land = log10(Rooms/LandsizeImp*1000))
numeric_cols <- c("sqrt_distance", "Lattitude", "Longtitude", "log_landsize", "log_room_land")
cormat <- round(cor(na.omit(housesNum[,numeric_cols])), 2)

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  # Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "#1EA6A2", high = "#A61E22", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

ggheatmap + geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```


__Distance__
Se observa en los boxplots que las casas caras están ligeramente más cercanas al centro de la ciudad.

```{r}
p<-housesTrain %>% mutate(sqrt_distance = sqrt(Distance)) %>%
  select(sqrt_distance, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Se rechaza la hipótesis nula: hay diferencias por categoría.
```{r}
summary(aov(price_label_high~Distance, data=housesTrain))
```

__Latitud y longitud__

Las distribuciones parecen indicar que el precio de la vivienda aumenta en la parte sur (menor latitud) y en la parte este (mayor longitud) de Melbourne. 

```{r}
p<-housesTrain %>%
  select('Lattitude', 'Longtitude', 'price_label_high') %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Para ambas variables se rechaza la hipótesis nula.
```{r}
summary(aov(price_label_high~Lattitude, data=housesTrain))
summary(aov(price_label_high~Longtitude, data=housesTrain))
```


__Landsize__

Las casas caras parece que tienen un tamaño de parcela ligeramente mayor.

```{r}
p<-housesTrain %>% mutate(log_landsize = log10(LandsizeImp)) %>% 
  select(log_landsize, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Se puede rechazar la hipótesis nula de que la media es igual en ambas categorías.
```{r}
summary(aov(price_label_high~log_landsize_std, data=housesTrain))
```


# Selección de variables

El aálisis exploratorio realizado nos permite saber para qué variables la distribución de casas baratas y caras es diferente, es decir, nos indica qué variables pueden ayudar más a la clasificación. 
Para seguir con la selección de variables vamos a untilizar el Information Value (IV). Esta medida nos indica cómo de buena es una variable a la hora de distinguir entre las categorías binarias de la variable respuesta o dependiente.

```{r}
a <- housesTrain %>% select("CouncilArea", "sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std", "rooms_cat", "year_built_cat", "car_cat", "Regionname", "Type", "Method", "bath_cat", "bed_cat", "sell_rate_cat", "price_label_high")


a$price_dep <- a %>% pull(price_label_high) %>% as.numeric()
a <- a[-15]
table(a$price_dep) %>% kable() %>% kable_styling(position = 'center', row_label_position = 'c')

infoTables <- create_infotables(data = a, y = "price_dep",
                              bins = 10,
                              parallel = F)

infoTables$Summary %>% kable() %>% kable_styling(position = 'center', row_label_position = 'c')
plotFrame <- infoTables$Summary[order(-infoTables$Summary$IV),]
plotFrame$Variable <- factor(plotFrame$Variable, levels = plotFrame$Variable[order(-plotFrame$IV)])

ggplot(plotFrame, aes(x = Variable, y = IV)) +
geom_bar(width = .35, stat = "identity", color = palette34[1], fill = "white") +
ggtitle("Information Value") +
theme_minimal() +
theme(plot.title = element_text(size = 12, hjust = 0.5)) +
theme(axis.text.x = element_text(angle = 90))
```

A la vista de los resultados, se puede ver que aquellas variables relacionadas con el tamaño de la vivienda (Type, rooms_cat) y con su ubicación (CouncilArea, lattitude, longtitude, Regionname) tienen mucho peso a la hora de clasificar la variable target.


# Aprendizaje no supervisado: clustering

En primer lugar, vamos a definir la base de datos con las variables cuantitativas y cualitativas ordinales a partir del dataset *housesTrain*.

Las variables cuantitativas las cogemos estandarizadas para eliminar el efecto de las distintas escalas de medida y las ordinales las estandarizamos al crearlas.

Estas son todas las variables que pueden emplearse para los métodos de clustring. Sin embargo, no se empezará probando con todas ellas sino que se tomarán de menos a más utilizando diferentes combinaciones. 

```{r}
housesTrainCluster <- housesTrain %>% dplyr::select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)

#las variables categóricas pasan a ser factores ordenados
housesTrainCluster['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() %>% scale()

housesTrainCluster['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric() %>% scale()

housesTrainCluster['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()

housesTrainCluster['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() %>% scale()

housesTrainCluster['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() %>% scale()
```


## k-means

Ahora, vamos a buscar el número de clústers óptimo. La idea es que el número seŕa él óptimo cuando los individuos de un mismo grupo sean lo más homogéneos posible y los individuos pertenecientes a distintos grupos sean lo más heterogéneos posible. Esto es lo mismo a buscar una división donde una suma de cuadrados entre (betweens) sea suficientemente grande y, por tanto, una suma de cuadrados dentro lo suficientemente pequeña (withins).

En las siguientes dos gráficas, se muestra el *betweens* y el *withins* para un número de clusters desde 1 hasta 15. Aquel punto en el que se produce el codo y le sigue una cierta estabilización es el número de clusters que se toma como óptimo.

Se observa en ambas, que el punto donde se produce el cambio de tendencia es aproximadamente en el 3. Por esto se va a probar a realizar una clasificación en base a 3 grupos. 

De todas las combinaciones probadas, atendiendo a la forma del gráfico y a la información de selección de variables, se van a utilizar La forma que tome esta gráfica nos ayuda a seleccionar las variables. Si la combinación de variables empleada genera una gráfica sin un codo claro y sin que haya una cierta estabilización, significaría que empleando esas variables no va a ser posible hacer clusters bien definidos.


```{r kmeansKoptimo1}
set.seed(123)
selec_var_kmeans <- housesTrainCluster %>% select(lattitude_std, bath_cat)
bss <- kmeans(selec_var_kmeans,centers=1)$betweenss
for (i in 1:15) bss[i] <- kmeans(selec_var_kmeans,centers=i)$betweenss
plot(1:15, bss, type="b", xlab="Number of Clusters",ylab="Sum of squares between groups",pch=19, col=palette34[2])
```

```{r kmeansKoptimo2}
n = nrow(housesTrain)
SSW <- vector(mode = "numeric", length = 15)
SSW[1] <- (n - 1) * sum(apply(X = selec_var_kmeans, MARGIN = 2, FUN = 'var'))
for (i in 1:15) SSW[i] <- sum(kmeans(selec_var_kmeans,centers=i,nstart=25)$withinss)
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col=palette34[2])
```

```{r}
clust_kmeans=kmeans(selec_var_kmeans,centers=5,nstart=25)
clust_kmeans
```

Observando el ratio de la suma de cuadrados entre-clusters y la suma de cuadrados totales, vemos que el porcentaje de varianza explicada por el modelo respecto al total de varianza observada es muy elevado(85.1%). Sin embargo, este indicador hay que tomarlo con cuidado puesto que se ve influído por el número de clusters empleado, de manera que aumenta conforme tomamos más grupos.


### PCA + visualización

```{r}
pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- clust_kmeans$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34)
```

Vamos a comprobar si los grupos generados a partir de la combinación de una variable del tamaño de la vivienda (bath_cat) y de su localización (lattitud_std) muestran diferencias respecto a la variable target (price_label_hight). En este caso, estos grupos podrían emplearse como variable en otros modelos.

```{r}
housesTrain <- housesTrain %>% mutate(kmeans_cluster = clust_kmeans$cluster)
bar_plot_target(housesTrain, "kmeans_cluster", "price_label_high")
chisq.test(housesTrain$kmeans_cluster, housesTrain$price_label_high)
```

```{r}
predict.kmeans <- function(object, newdata){
    centers <- object$centers
    n_centers <- nrow(centers)
    dist_mat <- as.matrix(dist(rbind(centers, newdata)))
    dist_mat <- dist_mat[-seq(n_centers), seq(n_centers)]
    max.col(-dist_mat)
}

#predict kmeans for validation and test datasets
housesTestKmeans <- housesTest %>% select(lattitude_std, bath_cat)
housesTestKmeans['bath_cat'] <-housesTest %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()
test_clust <- predict.kmeans(clust_kmeans, housesTestKmeans)
housesTest <- housesTest %>% mutate(kmeans_cluster = test_clust)

housesValKmeans <- housesVal %>% select(lattitude_std, bath_cat)
housesValKmeans['bath_cat'] <-housesVal %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()
val_clust <-predict.kmeans(clust_kmeans, housesValKmeans)
housesVal <- housesVal %>% mutate(kmeans_cluster = test_clust)
```


## Hierarchical

Se aplicará un cluster aglomerativo y se emplearán diferentes métodos de medida de la distancia entre clusters. 

```{r hierarchical}
mat_dist <- dist(housesTrainCluster, method = "euclidean")
hc1 <- hclust(mat_dist, method = "single" )
den = as.dendrogram(hc1, leaflab = "none")
plot(den)
```

A pesar de que el dendograma no ofrece una visualización clara debido al alto número de observaciones, sí se puede percibir como dos grandes clustes, siendo uno de ellos (el de la izquierda), aproximadamente, tres veces mayor que el otro y unas observaciones a la derecha del todo que se aglomeran en los últimos niveles.

Una vez creado el dendrograma, vamos a evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones empleando el coeficiente de correlación entre la altura de los nodos del dendrograma (distancia cophenetic) y la matriz de distancias original. Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos. Esta medida puede emplearse como criterio de ayuda para escoger entre los distintos métodos de linkage.

```{r}
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")
hc_euclidea_single  <- hclust(d = mat_dist, method = "single")
hc_euclidea_centroid  <- hclust(d = mat_dist, method = "centroid")
cor(x = mat_dist, cophenetic(hc_euclidea_complete))
cor(x = mat_dist, cophenetic(hc_euclidea_average))
cor(x = mat_dist, cophenetic(hc_euclidea_single))
cor(x = mat_dist, cophenetic(hc_euclidea_centroid))
```

A la vista de los resultados, el método que mejor refleja las distancias reales es el *average*. Este método supone un compromiso entre las dos características de los métodos en los que se emplea la distancia mínima (*single*) y en los que se emplea la máxima (*complete*). Un compromiso entre ser sensible a datos atípicos y manejar formas de clusters no elípticas.

Se prueba a graficar el dendograma empleando el método *average* y probando con cortes del árbol a diferentes alturas. Sin embargo, el alto número de observaciones impide tomar una decisión más o menos clara de por dónde plantear el corte. 

```{r, warning=FALSE}
hc2 <- hclust(mat_dist, method = "average" )
den = as.dendrogram(hc2)

avg_col_dend <- color_branches(den, h = 6, col = palette34)
plot(avg_col_dend, leaflab = "none", ylab = "Height")
abline(h = 6, col = 'black')
```


## DBSCAN

Los métodos de clustering empleados anteriormenete, son buenos encontrando agrupaciones con forma esférica y sensibles a outliers, pero fallan al tratar de identificar formas arbitrarias que es lo que tenemos en este caso.

DBSCAN evita este problema siguiendo la idea de que, para que una observación forme parte de un cluster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters están separados por regiones vacías o con pocas observaciones.


```{r dbscanEpsi}
datos = housesTrainCluster
dbscan::kNNdistplot(datos, k = 5)
```

La curva tiene el punto de inflexión en torno a 0.5, por lo que se escoge este valor como epsilon para DBSCAN.

```{r dbscan}
set.seed(10)
dbscan_cluster <- fpc::dbscan(data = datos, eps = 0.5, MinPts = 70)
head(dbscan_cluster$cluster)

# Visualización de los clusters

pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- dbscan_cluster$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34) + labs(title='PCA: DBSCAN clusters')

```


# Métodos de reducción de dimensionalidad

```{r}
housesTrain <- read.csv('base_train.csv')
```

## PCA

PCA debe trabajar con variables numéricas que además están normalizadas. Por ello, elegimos las 4 variables numéricas con las que trabajamos en la parte de regresión lineal múltiple (raíz cuadrada de la distancia, logaritmo de la parcela, latitud y longitud). Con las dos primeras componentes se explicaría un 66,3% de la varianza.


```{r}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```
```{r}
fviz_screeplot(housesPCA, ncp = 4, barfill = palette34[6], barcolor = palette34[6], linecolor = palette34[1], addlabels = TRUE, hjust = 0)
```



```{r}
train <- housesTrain %>% select(-Price)
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat, price_label_high)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

Se prueba a representar las dos componentes principales en dos dimensiones, intentando añadir una tercera dimensión en forma de color utilizando otras variables, con el objetivo de ver si hay diferencias en estas direcciones. Vemos por ejemplo que las casas caras parece que se concentran (ya se vio anteriormente que la localización influía en el precio), mientras que las casas baratas están más desperdigadas. 

Si pintamos la región, se pueden diferenciar fácilemente, ya que se han utilizado las coordenadas como entrada del PCA.


```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=price_label_high)) + geom_point() + labs(title='PCA: price label') + scale_color_manual(values = palette34)
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=Regionname)) + geom_point() + labs(title='PCA: Regionname') + scale_color_manual(values = palette34)
```


## MDS
Usaremos la distancia euclídea para realizar el MDS (debería ser muy parecido a PCA). Al tener bastantes datos, vamos a muestrear el train primero.

```{r}
housesTrainDim <- housesTrain

#las variables categóricas pasan a ser factores ordenados
housesTrainDim['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() %>% scale()

housesTrainDim['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric() %>% scale()

housesTrainDim['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()

housesTrainDim['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() %>% scale()

housesTrainDim['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() %>% scale()
```


```{r}
set.seed(10)
train_sample <- housesTrainDim[sample(nrow(housesNum), 2500),]
train_num_sample <- train_sample %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)
```

```{r}
# Distancia euclídea
d <- dist(train_num_sample) 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```



```{r}
df_fit <- data.frame(fit$points)
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point(alpha=0.5) + labs(title='MDS: price_label_high') + scale_color_manual(values = palette34)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname') + scale_color_manual(values = palette34)
```
Se prueba también la distancia de manhattan

```{r}
# Distancia euclídea
d <- dist(train_num_sample, method = 'manhattan') 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```

```{r}
df_fit <- data.frame(fit$points)
# No estoy seguro de hacer el join bien, se supone que es por indice
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='MDS: price_label_high') + scale_color_manual(values = palette34)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname') + scale_color_manual(values = palette34)
```

## tSNE
Probamos tsne sobre nuestras variables numéricas escaladas, para que valores extremos no nos den problemas. El parámetro que se suele tocar es el perplexity, que es algo parecido al número de vecinos más cercanos cuando se comparan las distribuciones que utiliza tSNE.
Fuente: https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

De nuevo superponiendo distintas variables sobre el gráfico en dos dimensiones, podemos ver 

```{r}
houses_num_unique <- unique(housesNum)
tsne <- Rtsne(houses_num_unique, dims = 2, perplexity=50, verbose=TRUE, max_iter = 500)
```

```{r}
df_tsne <- data.frame(tsne$Y)
df_train_no_dup <- housesTrain[!duplicated(housesTrain[c("sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std")]),]
df_train_no_dup$X1 <- df_tsne$X1
df_train_no_dup$X2 <- df_tsne$X2
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='tSNE: price_label_high') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=sell_rate_cat)) + geom_point() + labs(title='tSNE: sell_rate_cat') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='tSNE: Regionname') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Type)) + geom_point() + labs(title='tSNE: Type') + scale_color_manual(values = palette34)

```


Mapa de Melbourne :) (lo podemos poner el algún sitio)
```{r}
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=price_label_high)) + geom_point() + scale_color_manual(values = palette34)
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Regionname)) + geom_point() + scale_color_manual(values = palette34)
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Type)) + geom_point()+ scale_color_manual(values = palette34)
```

# Aprendizaje supervisado

# Regresión logística

Como primera aproximación, se ha probado el modelo de regresión logística con todas las variables disponibles en el dataset. El resultado obtenido es el siguiente:

```{r}
train <- read.csv('base_train.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
train <- train %>%  select(-filter_cols)

#train$may_have_water <- as.factor(train$may_have_water)
train$may_have_water <- factor(train$may_have_water, levels=c(F,T))
train$price_label_high <- as.factor(ifelse(train$price_label_high==TRUE,1,0))


lr_model <- glm(price_label_high ~ ., family = binomial(link = 'logit'), data = train)
summary(lr_model)
```


Se puede ver a partir de los coeficientes obtenidos, que las existen unas cuantas variables cuyos coeficientes no son siginificativamente diferentes de 0, por lo tanto lo más inteligente sería eliminarlas. Destacar que las más relevantes para el modelo son las numéricas relativas a la situación de las viviendas (latitud, longitud, distancia al centro...), así como las relativas al tamaño de la casa (número de baños, de plazas de garaje etc). También, y tal como habíamos visto en la parte de análisis exploratorio, la variable categórica tipo de vivienda es bastante relevante también para el modelo. 

De todas maneras, vamos a obtener las métricas de performance más relevantes de este modelo de cara a compararlo con uno más sencillo que construiremos a posteriori con menos variables:


```{r}
prob_lr <- lr_model %>% predict(train[,1:15], type = "response")
opt_f1_lr <- opt_f1_function_v2(prob_lr, train, "price_label_high")
c(optimal_thrshold = opt_f1_lr$threshold, precision = opt_f1_lr$precision, recall =  opt_f1_lr$recall, f1_score = opt_f1_lr$f1_opt)
```

Al igual que ocurre para el caso de regresión lineal, para el caso de regresión logística también se pueden aplicar las técnicas de regularización sobre los coeficientes de la misma. En concreto, se va a utilizar una regresión logística Lasso y observar los coeficientes obtenidos para cada una de las variables:

```{r}
set.seed(123)
cv.lasso <- cv.glmnet(price_label_high~., data = train, alpha = 1, type.measure = "deviance", family = "binomial") # Con 10-fold cross-validation
#plot(cv.lasso)
coef(cv.lasso, cv.lasso$lambda.1se)
```

Con la aplicación realizada mediante lasso, confirmamos un poco las sospechas que teníamos. Las variables numéricas relativas a la situación siguen siendo relevantes para lasso. Lo que ocurre es que en algunos casos el valor de los coeficientes asociados a ciertos niveles dentro de una misma variable categórica, son muy dispares. Esto se debe o bien a que hay un nivel muy predominante, o bien a que hay ciertos niveles que no son nada relevantes (y quizás podrían agruparse) pero otros sí. Un ejemplo claro de esto es el caso de la variable Regionname. Tal y como vimos en el análisis exploratorio, en la región Southern Metropolitan se encuentran una gran cantidad de casas "caras", mientras que en otras regiones no aparece ninguna y no van a ser relevantes para el modelo. 

En vista a estos resultados, se va a probar un modelo con muchas menos variables, y poder así comparar con el modelo más complejo:


```{r}
lr_regularized <- glm(price_label_high ~. -sell_rate_cat - Method - bed_cat - rooms_cat - car_cat , family = binomial(link = 'logit'), data = train)
summary(lr_regularized)
```

```{r}
prob_lr_regularized <- lr_regularized %>% predict(train[,1:15], type = "response")
opt_f1_lr_regularized <- opt_f1_function_v2(prob_lr_regularized, train, "price_label_high")
c(optimal_thrshold = opt_f1_lr_regularized$threshold, precision = opt_f1_lr_regularized$precision, recall =  opt_f1_lr_regularized$recall, f1_score = opt_f1_lr_regularized$f1_opt)
```

Vemos que no solo no ha disminuido la performance del modelo, sino que es incluso ligeramente superior en términos de f1 score. A continuación se muestra por un lado la curva PR (Precision-Recall) junto con el área bajo la misma, y por otro, una comparativa de Precision, Recall y F1 score para los diferentes thresholds:

```{r}
opt_f1_lr_regularized$p1
opt_f1_lr_regularized$p2
```

A continuación, se procede a medir la performance del modelo sobre los datos pertencientes al conjunto de validación:

```{r}
validation <- read.csv('base_val.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
validation <- validation %>%  select(-filter_cols)
validation$may_have_water <- factor(validation$may_have_water, levels=c(F,T))
validation$price_label_high <- as.factor(ifelse(validation$price_label_high==TRUE,1,0))



prob_values_test <- lr_regularized %>% predict(validation[,1:15], type = "response")
preds_validation_lr <- as.factor(ifelse(prob_values_test > opt_f1_lr_regularized$threshold ,1,0))

metrics_function_num(preds_validation_lr, validation, 'price_label_high')
```

Como podemos ver, al simpolificar el modelo se ha conseguido que no sobreajuste en absoluto a los datos de entrenamiento. Sin embargo los resultados obtenidos con este algoritmo no son excesivamente satisfactorios.



```{r}
# Para guardar el mejor modelo
saveRDS(lr_regularized, "./models/lr_best_model.rds")
```

```{r}
# Para cargarlo de nuevo
loaded_model <- readRDS("./models/lr_best_model.rds")
summary(loaded_model)
```



## Árboles de decisión
Se van a probar árboles de decisión sobre nuestro conjunto de datos. En primer lugar, se utilizarán las variables que se consideraron importantes en el análisis exploratorio, para después probar distintas formas de tuning e intentar evitar el sobreaprendizaje del árbol.

### Utilizando las variables estudiadas
Se utilizarán directamente las variables estudiadas en el análisis exploratorio, dejando de lado transformaciones y estandarizaciones, además de variables categóricas con muchas categorías. Además, si recordamos, para la regresión lineal tuvimos que convertir varias features numéricas en categóricas (número de habitaciones, año de construcción...). Podemos probar a introducir las variables originales para ver qué cortes propone el árbol.


En los cortes, se mira primero la región en la que está la casa, viendo si está o no en Southern Metropolitan (la zona con casas más caras). Después, utiliza variables como el número de habitaciones, el tipo de vivienda y la distancia para discriminar. Se ve que hay un grupo que contiene a cerca de la mitad de las muestras con un 9% de probabilidades de ser cara, solo teniendo en cuenta que no está en Southern Metropolitan y que tiene menos de 4 habitaciones.

Algo que también se puede ver es que hay varios nodos con pocas muestras (los que vienen de la distinción de tipo de casa es Southern Metropolitan). Vemos además que hay un bastantes falsos negativos (casas caras que se etiquetan como baratas).

```{r message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat"), "price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


En validación:

```{r message=FALSE}
validation_metrics <- val_metrics(model_metrics$dt, housesVal, c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat"), "price_label_high")
validation_metrics$metrics
validation_metrics$roc_plot
validation_metrics$opt_f1
```



Observamos que el árbol solo discrimina por las variables *Rooms*, *Distance*, *Type*, *Regionname*. Si probamos un árbol con solo estas variables, obtenemos un modelo muy parecido:

```{r}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Distance", "Regionname", "Type"),"price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Al ver que el árbol se fija en la localización, se prueba a entrenar un modelo solo con el número de habitaciones y las coordenadas, que presumiblemente dan información más detallada, además del tipo de casa. Se intenta evitar meter variables que pueden estar relaciones entre sí, ya que el árbol solo elegiría una de ellas para discriminar. Observamos que el f1 score mejora: por un lado, el recall ha aumentado (devolvemos más casas caras) pero el precision ha empeorado (más falsos positivos, el modelo dice que casas que no son caras lo son). Además, si comparamos ambas curvas ROC, vemos que este modelo tiene un AUC mejor (distingue mejor entre clases). 

Por otro lado, el árbol, aunque ofrece mejores métricas, resulta ser menos interpretable, ya que la mayoría de los cortes se basa en valores numéricos de las coordenadas.

```{r message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Lattitude", "Longtitude", "Type"), "price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
```


En ambos casos, se puede ver que hay muchos nodos con muy pocos casos (1%,2%...) que probablemente se puedan juntar en otro nodo sin perder poder predictivo. 

### Podando el árbol 
Intentamos eliminar esas últimas hojas que hemos comentado en el último modelo probado.

```{r}
printcp(model_metrics$dt)
```
```{r}
plotcp(model_metrics$dt)
```

Con 11 hojas, el error es prácticamente el mismo. Si bajamos al siguiente nivel de complejidad (5 hojas) las métricas del modelo empeoran mucho.


```{r pruneTree2, warning=FALSE}
pruneTREE <- rpart::prune(model_metrics$dt, cp = model_metrics$dt$cptable[5, "CP"])
rpart.plot(pruneTREE, type=4, fallen.leaves = FALSE, tweak =1.75)
preds <- predict(pruneTREE, type = "class")
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Bajando la complejidad hasta quedarnos solo con 5 hojas, observamos que el accuracy es parecido, pero que el f1-score disminuye hasta 0.56 (con una precisión bastante aceptable). Por otro lado, el árbol es mucho más fácil de interpretar. Las casas con menos de 4 habitaciones directamente son una hoja con las casas baratas (83% de las casas de este nodo son baratas), con el 75% de los datos. Las casas con más de 4 habitaciones en se distribuyen en función de las coordenadas.

```{r warning=FALSE}
pruneTREE <- rpart::prune(model_metrics$dt, cp = model_metrics$dt$cptable[4, "CP"])
rpart.plot(pruneTREE, type=4, fallen.leaves = FALSE, tweak =1.75)
preds <- predict(pruneTREE, type = "class")
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```

### Cross validation
Utilizaremos las variables que mejor resultados nos han dado en una etapa de validación cruzada para obtener el mejor parámetro de complejidad.

```{r}
# Hay que factorizar el label para que se construya bien el árbol
housesTrainCV <- housesTrain %>% select(Rooms, Lattitude, Longtitude, Type)
housesTrainCV$price_label_high <- as.factor(housesTrain$price_label_high)
housesTrainCV$price_label_high <- as.factor(ifelse(housesTrainCV$price_label_high==TRUE,"caras","baratas"))

fit_control <- caret::trainControl(method = "cv", number=10, summaryFunction = twoClassSummary, classProbs = TRUE)
cv_model <- train(price_label_high ~ ., data=housesTrainCV, method='rpart', trControl=fit_control, tuneLength=10, metric='ROC')
```

```{r}
cv_model
```

```{r}
plot(cv_model)
plot(cv_model$results$ROCSD)
```

Fijándonos en la gráfica, quizás querramos sacrificar un poco de roc a costa de que nuestro árbol sea menos complejo. Viendo el resultado, es un árbol con muchos nodos, que quizás esté sobre ajustando.

```{r}
final_dt <- cv_model$finalModel
rpart.plot(final_dt)
```

```{r}
preds <- predict(final_dt, type = "class")
preds<- as.factor(ifelse(preds=="caras",TRUE,FALSE))
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```


```{r}
prob_values <- predict(final_dt, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
```

```{r}
opt_f1_obj <- opt_f1_function(prob_values, housesTrain, 'price_label_high')
opt_f1_obj$p1
opt_f1_obj$p2
```

```{r}
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=prob_values[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
```


## Random Forest

### Selección de variables
En este apartado se van a utilizar random forests para el problema de clasificación. Al no estar demasiado expuesto a los outliers, y aceptar variables categóricas como entrada, la primera prueba que se realizará será con la mayoría de variables del conjunto de datos, y con unos parámetros iniciales que después se modificarán en una etapa de selección de hiperparámetros. Se busca en esta primera prueba un modelo con muchos árboles, de tal forma que se pueda ver la importancia de las variables, y pueda servir de etapa previa de selección de las mismas. Aquí, hay que tener varias cosas en cuenta:

- Si se introducen variables muy correladas, solo una tendrá mucha importancia, y el efecto de la otra quedará "enmascarado". Esto puede afectar a la interpretación de los datos.
- Los random forest tienden a estar sesgados hacia variables categóricas con muchas categorías.

```{r}
housesTrain <- read.csv('base_train.csv')
```

```{r}
set.seed(10)
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label_high")]
housesTrainOrigVar$price_label_high <- as.factor(housesTrainOrigVar$price_label_high)
rf2 <- randomForest(price_label_high ~ ., data=housesTrainOrigVar, ntree=500)
```

Observamos que ya de primeras se obtienen buenas métricas.

```{r}
preds <- predict(rf2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```


```{r message=FALSE}
roc <- roc(housesTrain$price_label_high, rf2$votes[,2])
ggplotROCCurve(roc)
```





Si observamos la importancia de las variables del anterior (medida como la caída media en el índice gini que produce cada variable), vemos que las más importantes son aquellas relacionadas con la situación geográfica de la vivienda (cómo ya se había visto anteriormente): coordenadas, distancia al centro y región están en las 6 primeras posiciones, además de variables que informan sobre el tamaño de la vivienda (número de habitaciones y tamaño de la parcela). 


```{r}
# Extracts variable importance (Mean Decrease in Gini Index)
# Sorts by variable importance and relevels factors to match ordering
var_importance <- data_frame(variable=setdiff(colnames(housesTrainOrigVar), "price_label_high"),
                             importance=as.vector(importance(rf2)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p+ scale_fill_manual(values = palette34, name = "Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```

Podemos probar si seleccionado las variables más importantes y volviendo a entrenar un modelo con los mismos parámetros, obtenemos resultados parecidos.

Si nos quedamos con las 6 primeras, el accuracy desciende ligeramente hasta 0.89, pero el f1 score baja hasta 0.77. Si se incluye la variable *Type* adicionalmente, se consiguen resultados muy parecidos a los del primer modelo, reduciendo la complejidad. Si además quitamos la variable *Regionname*, por tener bastantes categorías, conseguimos un f1-score de 0.8.

```{r}
set.seed(10)
housesTrainImpVars <- housesTrain[c("Rooms", "Distance", "LandsizeImp",
                                    "Lattitude", "Longtitude", "Type",
                                     "price_label_high")]
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
rf3 <- randomForest(price_label_high ~ ., data=housesTrainImpVars, ntree=100)
```


```{r}
preds <- predict(rf3, type = "class")
table(pred = preds, obs = housesTrainImpVars$price_label_high)
metrics_function(preds, housesTrainImpVars, 'price_label_high', bool=TRUE)
```

```{r}
var_importance <- data_frame(variable=setdiff(colnames(housesTrainImpVars), "price_label_high"),
                             importance=as.vector(importance(rf3)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p + scale_fill_manual(values = palette34, name="Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```


## Utilizando cross validation
Como primera prueba, y teniendo en cuenta que las variables importantes el modelo son bastante lógicas, los resultados de este modelo son bastante buenos. Ahora se va a intentar tunear el número de árboles y la profundidad máxima de los mismos con grid search y CV, para intentar obtener unos mejores resultados, además de evaluar el modelo con conjuntos de validación. Los parámetros a tunear son:

- Número de árboles
- Número de variables que se muestrean aleatoriamente al crear los árboles
- Mínimo número de puntos para que un nodo se divida.

Realizamos 10 Fold cross validation con 5 iteraciones.

Se utilizan las variables que se han comentado en el apartado anterior.


```{r}
set.seed(10)
rf_model <- rand_forest(trees=tune(), mtry=tune(), min_n=tune()) %>% set_mode('classification') %>% set_engine('randomForest')
houses_rec <- recipe(price_label_high ~ ., data=housesTrainImpVars)
# Create workflow
rf_workflow <- workflow() %>% add_model(rf_model) %>% add_recipe(houses_rec)
rf_param <- rf_workflow %>% parameters() %>% update(trees = trees(c(50,500)), mtry=mtry(c(1,5)), min_n=min_n(c(2,20)))
# Create grid
rf_grid <- grid_regular(rf_param, levels=3)
```

```{r message=FALSE}
# Parallelize
all_cores <- parallel::detectCores(logical = FALSE) - 2
registerDoFuture()
cl <- makeCluster(all_cores)
plan(future::cluster, workers = cl)
```

```{r}
houses_folds <- vfold_cv(housesTrainOrigVar, v = 10, repeats = 3)
houses_folds
rf_search <- tune_grid(rf_workflow, grid = rf_grid, resamples=houses_folds, param_info = rf_param)
```

```{r}
#Execute after finishing training
stopCluster(cl)
```

```{r}
autoplot(rf_search) +
    labs(title = "Results of Grid Search for Two Tuning Parameters of a Random Forest")
```

```{r}
show_best(rf_search, metric='roc_auc')
```

```{r}
# Prepare for final prediction
rf_param_final <- select_best(rf_search, "roc_auc")
rf_wflow_final <- finalize_workflow(rf_workflow, rf_param_final)
rf_wflow_final_fit <- fit(rf_wflow_final, data = housesTrainOrigVar)
```

```{r}
housesTrainOrigVar$.pred <- predict(rf_wflow_final_fit, 
                          new_data = housesTrainOrigVar)$.pred_class
housesTrainOrigVar$.pred_TRUE <- predict(rf_wflow_final_fit, 
                          new_data = housesTrainOrigVar, type='prob')$.pred_TRUE
metrics(housesTrainOrigVar, truth = price_label_high, .pred, .pred_TRUE)
```

```{r}
table(pred = housesTrainOrigVar$.pred, obs = housesTrainOrigVar$price_label_high)
metrics_function(housesTrainOrigVar$.pred, housesTrainOrigVar, 'price_label_high', bool=TRUE)
```

```{r}
opt_f1_obj <- opt_f1_function(housesTrainOrigVar$.pred_TRUE, housesTrainOrigVar, 'price_label_high', 0.3)
opt_f1_obj$p1
opt_f1_obj$p2
```


```{r}
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=housesTrainOrigVar$.pred_TRUE)
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
```



## kNN
El siguiente algoritmo que se va a probar es kNN. Para ello, necesitamos que los datos estén de nuevo estandarizados, ya que es un método basado en distancias, y las variables necesitan estar en el mismo rango de valores. Es importante que no se utilicen muchas variables en este método, ya que al estar basado en distancias, puede sufrir de la "maldición de la dimensionalidad".


```{r}
housesTrain <- read.csv('base_train.csv')
# Para que funcione confusionMatrix de caret, el target debe ser un factor
housesTrain$price_label_high <- as.factor(housesTrain$price_label_high)
```

En primer lugar, se probarán las variables numéricas con las que se trabajó en el anterior informe: distancia al centro, coordenadas y tamaño de la parcela. Se utiliza un k=3 para esta primera prueba. Para evaluar los modelos, se utiliza la función knn.cv, que utiliza leave one out cross validation. Para cada fila del dataset de entrenamiento, se utilizan los K vecinos más cercanos para decidir la clase.

Se obtiene un recall bastante bajo, lo que hace que el f1 score sea mediocre. Para intentar mejorar el modelo, se intentará elegir el valor óptimo de k.

```{r knn1}
num_std_cols <- c('sqrt_distance_std', 'log_landsize_std', 'lattitude_std', 'longtitude_std')
knn_metrics <- perform_knn_cv(housesTrain, 'price_label_high', 3, num_std_cols)
knn_metrics
```


### k óptimo
Se muestra en la gráfica los distintos valores de accuracy y F1 que se obtendrían cambiando los valores de k. Se observa que el f1 score es bastante bajo, dejando de mejorar a partir de k=5 o k=7. El accuracy no pasa de 0.8 a partir de k=3.

```{r}
plot_acc_f1_k(housesTrain, 'price_label_high', num_std_cols)
```

Si probamos con k=7, los resultados mejoran ligeramente, pero el recall sigue siendo bastante flojo (hay muchos FP).

```{r knn7high}
knn_metrics <- perform_knn_cv(housesTrain, 'price_label_high', 7, num_std_cols)
knn_metrics
```

### Usando otras variables
Hasta ahora, se han utilizado las 4 variables numéricas que teníamos identificadas. Se puede probar a crear otras variables adicionales como combinaciones de ordinales y numéricas. Por ejemplo, habitaciones por unidad de parcela.

```{r}
housesTrain <- housesTrain %>% mutate(log_room_land = log10(Rooms/LandsizeImp*1000))
housesTrain$log_room_land_std <- housesTrain %>% select(log_room_land) %>% scale()
housesTrain %>% ggplot(aes(y=log_room_land_std)) + geom_boxplot()
housesTrain %>% ggplot(aes(x=log_room_land_std)) + geom_histogram()

p<-housesTrain %>%
  select(log_room_land, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

```{r}
summary(aov(price_label_high~log_room_land, data=housesTrain))
```



```{r}
num_std_cols <- c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'log_room_land')
plot_acc_f1_k(housesTrain, 'price_label_high', num_std_cols)
```

Con esta nueva variable, podemos elegir un k=3, consiguiendo un f1 de 0.75, y además eliminar una variable (**sqrt_distance_std**).

```{r}
knn_metrics <- perform_knn_cv(housesTrain, 'price_label_high', 3, num_std_cols)
knn_metrics
```

El resto de variables son categóricas, pero algunas además son ordinales, por lo que tendría sentido aplicar distancias utilizando también esas variables.


```{r}
housesTrainKnn <- housesTrain %>% dplyr::select(sqrt_distance_std, log_room_land, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)


#las variables categóricas pasan a ser factores ordenados
housesTrainKnn['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric()

housesTrainKnn['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()

housesTrainKnn['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric()

housesTrainKnn['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric()

housesTrainKnn['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric()

# Norm
normParam <- preProcess(housesTrainKnn)
housesTrainKnnNorm <- predict(normParam, housesTrainKnn)

housesTrainKnnNorm$price_label_high <- as.factor(housesTrain$price_label_high)
```

Observamos que con estas variables conseguimos incrementar tanto el accuracy como el f1 score, y que además a partir de k=3 se mantendrian constantes.

```{r}
std_cols <- c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'rooms_cat', 'car_cat', 'sell_rate_cat')
plot_acc_f1_k(housesTrainKnnNorm, 'price_label_high', std_cols)
```

Introduciendo todas las variables, el modelo no consigue mejorar. La combinación que ofrece mejores resultados es el uso del tamaño de la parcela, coordenadas, habitaciones, plazas de aparcamiento y popularidad del barrio.

```{r}
knn_metrics <- perform_knn_cv(housesTrainKnnNorm, 'price_label_high', 3, std_cols)
knn_metrics
```


##SVM

A continuación se va aplicar el algoritmo de SVM (con diferentes hiperparámetros) sobre nuestros datos. Este algoritmo se fundamenta en la idea de encontrar un hiperplano óptimo que separe perfectamente los puntos. Además, para la resolución de este problema se basa en el teorema de Cover, el cual afirma que para cualquier dataset, la separación lineal de las clases se hace más latente a medida que las dimensiones aumentan. 

Para mapear los datos de entrada a un espacios de mayores dimensiones, se hace uso de un tipo de funciones llamadas funciones de kernel. Los kernels pueden ser de 3 tipos:

- Lineales.
- Polinómicos.
- Gaussianos (basados en funciones de base radial).

Lo adecuado que va a ser utilizar un tipo de kernel sobre otro va a estar totalmente determinado por el tipo de datos. En espacios con baja dimensionalidad (como mucho hasta 3 dimensiones y ni siquiera), resulta interesante darle un primer vistazo a cómo se reparten las clases en el espacio, para principalemente observar si son linearmente separables o no. 

En nuestro conjunto de datos no contamos con esa posibilidad, ya que aparecen hasta 13 variables. Lo único que se puede hacer, es aplicar de nuevo alguna técnica de dimensionalidad (PCA), por si se pudiese intuir algo en dos dimensiones:

```{r, echo=F}
train <- read.csv('base_train.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
train <- train %>%  select(-filter_cols)

# Estandarización de variables categóricas ordinales
cat_with_order = c("rooms_cat", "car_cat", "bath_cat", "bed_cat", "sell_rate_cat")

train['rooms_cat']<- train %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() 
train['car_cat'] <- train %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()
train['bath_cat'] <- train %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() 
train['bed_cat'] <- train %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() 
train['sell_rate_cat'] <- train %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() 

ordinal_variables <- train %>% select(cat_with_order) 
normParam <- preProcess(ordinal_variables)
ordinal_variables_norm <- predict(normParam, ordinal_variables)

for(cat in cat_with_order){
  train[cat] <- ordinal_variables_norm[cat]
}

train <- train %>% select(-c("year_built_cat", "Regionname", "Type", "Method"))
train$price_label_high<-as.factor(ifelse(train$price_label_high==TRUE,1,0))
    
```


```{r}
housesNum <- train %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum, center = TRUE, scale = TRUE)
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca$price_label <- train$price_label_high
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=price_label)) + geom_point() + scale_color_manual(values = palette34)
```

Con esta represntación podemos ver que claramente nuestros datos no son linearmente separables si los reducimos a dos dimensiones. Aun así, no se va a descartar la realización de pruebas utilizando un kernel lineal, ya que como se ha dicho nuestros datos no están en dos, sino en 9 dimensiones. 

## Kernel Lineal 

Inicialmente se va a probar utilizando un kernel lineal para la SVM. En el caso de la utilización de un kernel lineal, aparece un hiperparámetro a utilizar, denominado parámetro de coste (C). Se van a probar diferentes valores de este parámetro:


```{r pressure, echo=FALSE}
# Tarda aproximadamente 7 minutos
svm_linear <- e1071::tune("svm", price_label_high ~ ., data = train,
               kernel = 'linear',
               ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))


summary(svm_linear)

```

Como se puede el menor error de clasificación utilizando este kernel lineal se obtiene para C=100. Sin embargo a partir de 0.1 apenas varia de forma significativa este error. Además, las SVM tienen un cierto caracter aleatorio, que hace que en otra ejecución el mejor modelo podría conseguirse con un C distinto. Aún así, vamos a seleccionar como este modelo como el mejor con kernel linear y obtener algunas métricas adicionales de rendimiento:

```{r, meassage=f}
best_linear <- svm(price_label_high~., data = train, kernel = "linear", cost = svm_linear$best.parameters$cost, probability = T)
x_train <- train[,1:9]
pred_train <- predict(best_linear, x_train, probability = T)
prob_values <- attr(pred_train, "probabilities")[,1] # La probabilidad de pertenecer a la clase "cara" (1)

beta <- 1 #Beta value for F1 score
pred_svm <- prediction(prob_values, train$price_label_high)
perf_svm <- performance(pred_svm, "prec", "rec")
f1_best_linear <- (1+beta^2)*perf_svm@y.values[[1]]*perf_svm@x.values[[1]]/(beta^2*perf_svm@y.values[[1]]+perf_svm@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_linear) # Mejor f1 score para este modelo
prec_svm_opt=perf_svm@y.values[[1]][optimo]
rec_svm_opt=perf_svm@x.values[[1]][optimo]
f1_measure_svm_opt <- (1+beta^2)*prec_svm_opt*rec_svm_opt/(beta^2*prec_svm_opt+rec_svm_opt)
threshold_optimo_svm <- perf_svm@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_svm_opt, precision = prec_svm_opt, recall = rec_svm_opt, threshold = threshold_optimo_svm))
```


```{r,message=F}

preds <- as.factor(ifelse(prob_values > threshold_optimo_svm ,1,0))
cm <- caret::confusionMatrix(preds, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_linear <- roc(train$price_label_high,prob_values)
roc_plot <- ggplotROCCurve(roc_linear)


grid.arrange(roc_plot, cm_plot, ncol=2)


```


Se ha alcanzado una f1_score de 0.70 aproximadamente sobre el conjunto de entrenamiento utilizando la svm con kernel lineal. A pesar de obtenerse una f1_score tan baja, el área bajo la curva es de 0.89, lo cual nos quiere decir que en general el modelo está realizando un bastante buen trabajo separando las dos clases. Sin embargo, se puede intuir que utilizando un tipo de kernel no lineal se podrían mejorar estos resultados. Para comprobarlo, se procede a probar el funcionamiento de una SVM haciendo uso de un kernel gaussiano. En este caso, además del parámetro de coste que ya conocíamos, aparece otro hiperparámetro que será necesario optimizar (gamma). Esta vez, para buscar los mejores hiperparámetros posibles para el modelo, se va a hacer una búsqueda bayesiana. Los resultados obtenidos se muestan a continuación:

```{r}
svm_mod <- svm_rbf(mode = "classification", cost = tune(), rbf_sigma = tune()) %>% set_engine("kernlab")
svm_rec <- recipe(price_label_high ~ ., data=train)
svm_wflow <- workflow() %>% add_model(svm_mod) %>% add_recipe(svm_rec)
svm_param <- svm_wflow %>% parameters() #%>% update(cost = cost(range = c()), rbf_sigma = rbf_sigma(range = c()))
svm_param
```

```{r message=FALSE}
# Parallelize
all_cores <- parallel::detectCores(logical = FALSE) - 2
registerDoFuture()
cl <- makeCluster(all_cores)
plan(future::cluster, workers = cl)
```

```{r}
#tiempò aproximado de ejecución: 15 minutos
set.seed(1291)

svm_folds <- vfold_cv(train, v = 10, repeats = 1)

svm_search <- tune_bayes(svm_wflow,
                         resamples = svm_folds,
                         param_info = svm_param,
                         initial = 5,
                         iter = 30,
                         metrics = metric_set(roc_auc),
                         control = control_bayes(no_improve = 15, verbose = FALSE))


show_best(svm_search, metric = "roc_auc")

```

En le anterior tabla se muestran los 5 mejores resultados obtenidos (en cuanto a área bajo la curva). Para el cáculo de métricas y búsqueda del mejor threshold, no se va a utilizar el mejor modelo devuelto, sino que se va a hacer uso de la "one-standard error rule" propuesta por Breinman y seleccionar el modelo cuya área bajo la curva se encuentra a una desviación estándar de distancia del modelo con mejor auc. Con esto, estaremos intentando reducir en la medida de lo posbile el sobreajuste a los datos de entrenamiento: A continuación se muestran los resultados obtenidos tras el proceso de búsqueda del thrshold óptimo para el modelo:

```{r}
svm_param_best <- select_by_one_std_err(svm_search, rbf_sigma, cost, metric = "roc_auc")
svm_best <- finalize_workflow(svm_wflow, svm_param_best)
svm_best_fit <- fit(svm_best, data = train)
svm_best_fit
```

Una vez seleccionado el modelo, se pretende buscar el threshold óptimo. Se entiende por threshold óptimo, el que hace que el modelo alcanze la mayor f1-score. Los resultados han sido los siguientes:

```{r}
beta <- 1 #Beta value for F1 score
train$probs <- predict(svm_best_fit, new_data = train, type = "prob")$.pred_1
pred_svm <- prediction(train$probs, train$price_label_high)
perf_svm <- performance(pred_svm, "prec", "rec")
f1_best_linear <- (1+beta^2)*perf_svm@y.values[[1]]*perf_svm@x.values[[1]]/(beta^2*perf_svm@y.values[[1]]+perf_svm@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_linear) # Mejor f1 score para este modelo
prec_svm_opt=perf_svm@y.values[[1]][optimo]
rec_svm_opt=perf_svm@x.values[[1]][optimo]
f1_measure_svm_opt <- (1+beta^2)*prec_svm_opt*rec_svm_opt/(beta^2*prec_svm_opt+rec_svm_opt)
threshold_optimo_svm <- perf_svm@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_svm_opt, precision = prec_svm_opt, recall = rec_svm_opt, threshold = threshold_optimo_svm))

```

La mejor f1 score obtenida con este Kernel es 0.82, bastante superior al caso del kernel lineal (como era de esperar). En el siguiente gráfico se muestran la curva ROC y la matriz de confusión del modelo sobre los datos de entrenamiento:


```{r, message=F}

train$preds <- as.factor(ifelse(train$probs > threshold_optimo_svm ,1,0))

cm <- caret::confusionMatrix(train$preds, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(train$price_label_high,train$probs)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)

```

Como se puede apreciar, los resultados de este modelo son bastante buenos. El valor de 0.96 para el área bajo la curva demuestra que el modelo es capaz de separar correctamente ambas clases. En el siguiente gráfico se puede ver la distribución de las probabilidades y obtenidas mediante el modelo, junto al threshold óptimo calculado. También se muestra en forma de histograma: 


```{r}

train %>% ggplot(aes(x=probs, color=price_label_high)) +
  geom_density() + geom_vline(aes(xintercept=threshold_optimo_svm),
            color="blue", linetype="dashed")

train %>% ggplot(aes(x=probs, fill=price_label_high)) + 
  geom_histogram(alpha=0.5) + geom_vline(aes(xintercept=threshold_optimo_svm),
            color="blue", linetype="dashed")

```


Se puede ver que el modelo es muy bueno captando los 0. Parece que tiene más problemas en el caso de los 1, aunque si nos fijamos en el histograma, se puede ver que el problema no es tan grave como parecía en la función de densidad de probabilidad.

Cabe destacar que eL algoritmo de SVM, en un principio solo proporciona las etiquetas predichas. Sin embargo, de cara a la optimización de ciertas métricas puede ser útil obtener probabilidades. Para obtenerlas, se aplica una técnica llamada calibrado de Platt, que a grandes rasgos consiste en aplicar una regresión logística sobre las etiquetas calculadas por la SVM. Por lo tanto, al no estar diseñadas las SVM para proporcionar outputs probabilísticos, tampoco hay que darle mayor importancia a esta gráfica.

A continuación, se procede a validar el modelo utilizando el conjunto de test:

```{r, echo = F}

test <- read.csv('base_test.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
test<- test %>%  select(-filter_cols)

# Estandarización de variables categóricas ordinales
cat_with_order = c("rooms_cat", "car_cat", "bath_cat", "bed_cat", "sell_rate_cat")

test['rooms_cat']<- test %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() 
test['car_cat'] <- test %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()
test['bath_cat'] <- test %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() 
test['bed_cat'] <- test %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() 
test['sell_rate_cat'] <- test %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() 

ordinal_variables <- test %>% select(cat_with_order) 
ordinal_variables_norm <- predict(normParam, ordinal_variables) # Estandarizamos con el objeto estandarizador de train

for(cat in cat_with_order){
  test[cat] <- ordinal_variables_norm[cat]
}

test <- test %>% select(-c("year_built_cat", "Regionname", "Type", "Method"))


test$price_label_high <- as.factor(ifelse(test$price_label_high==TRUE,1,0))

```

```{r}
test$probs <- predict(svm_best_fit, new_data = test, type = "prob")$.pred_1
test$preds <- as.factor(ifelse(test$probs > threshold_optimo_svm ,1,0))


metrics_function_num(test$preds, test, 'price_label_high')
```


```{r}

cm <- caret::confusionMatrix(test$preds, test$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(test$price_label_high,test$probs)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)
```

Como era de esperar, las métricas disminuyen, sin embargo  no demasiado. La performance del modelo puede decirse que es bastante similar sobre los datos de test, lo cual quiere decir que apenas existe sobreajuste con este modelo.


## Naive Bayes
Como simplificación de redes bayesianas, vamos a entrenar un modelo utilizando Naive Bayes.

```{r}
housesTrain <- read.csv('base_train.csv')
```

### Primer modelo

Como primera aproximación, utilizaremos las mismas variables utilizadas en el modelo de regresión lineal múltiple (aunque el escalado debería ser indiferente). Para variables numéricas asume una distribución gausiana para calcular la verosimilitud (likelihood). Lo que saca el mod es la media en la primera columna y la varianza en la segunda para las distribuciones condicionadas.

```{r}
mod <- naiveBayes(price_label_high ~ sqrt_distance_std + log_landsize_std + lattitude_std + longtitude_std + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data=housesTrain)
```

Métricas en train. El f1 es flojillo, ya que ni el recall ni el precision son demasiado buenos. Sin embargo, el AUC ROC es bastante bueno (0.9)

```{r}
preds <- predict(mod, housesTrain, type = "class")
table(pred = preds, obs = housesTrain$price_label_high)
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```

```{r}
prob_values <- predict(mod, housesTrain, type='raw')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
```

Probamos a reducir el número de variables, utilizando por ejemplo las variables más importantes seleccionadas por el random forest. El modelo mejora ligeramente.

```{r}
mod <- naiveBayes(price_label_high ~ Distance + Regionname + Type + rooms_cat, data=housesTrain)
```

```{r}
preds <- predict(mod, housesTrain, type = "class")
table(pred = preds, obs = housesTrain$price_label_high)
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
```

```{r}
prob_values <- predict(mod, housesTrain, type='raw')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
```

Probando algún modelo con menos variables. A modo de resumen:

- Utilizando solo las categóricas, los resultados empeoran, con un accuracy de 0.8079153, un f1 de 0.6207523 y una auc roc de 0.8552.
- Utilizando únicamente las numéricas, el resultado lógicamente empeora, ya que Naive Bayes asume una distribución normal de las entradas numéricas, y trabaja mejor con categóricas al calcular las probabilidades de una forma más precisa conociendo las distribuciones. En este caso, tenemos un accuracy de 0.7727566, un f1 de 0.4759075 y una auc roc de 0.7913

### Validación
Utilizaremos ahora cross validation para comprobar si el modelo sobre ajusta. Observamos que con 10 fold cross validation repetido 3 veces, obtenemos una ROC de 0.85 con una desviación de 0.01. Usando todas las variables se obtiene una AUC ROC de 0.867 con una desviación de 0.012, pero elegimos el primer modelo por ser más sencillo.

```{r}
housesTrainCV <- housesTrain %>% select(Distance, Regionname, Type, rooms_cat, price_label_high)
housesTrainCV$price_label_high <- as.factor(ifelse(housesTrainCV$price_label_high==TRUE,"caras","baratas"))
fit_control <- caret::trainControl(method = "repeatedcv", number=10, repeats=3, summaryFunction = twoClassSummary, classProbs = TRUE)
grid <- expand.grid(usekernel=c(FALSE), laplace=c(0), adjust=c(0))
cv_model <- train(price_label_high ~ ., data=housesTrainCV, method='naive_bayes', trControl=fit_control, metric='ROC', tuneGrid=grid)
cv_model$results
```


```{r}
housesTrainCV <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat, price_label_high)
housesTrainCV$price_label_high <- as.factor(ifelse(housesTrainCV$price_label_high==TRUE,"caras","baratas"))
fit_control <- caret::trainControl(method = "repeatedcv", number=10, repeats=3, summaryFunction = twoClassSummary, classProbs = TRUE)
grid <- expand.grid(usekernel=c(FALSE), laplace=c(0), adjust=c(0))
cv_model <- train(price_label_high ~ ., data=housesTrainCV, method='naive_bayes', trControl=fit_control, metric='ROC', tuneGrid=grid)
cv_model$results
```

Finalmente, buscamos el corte óptimo con el mejor modelo.


```{r}
mod <- naiveBayes(price_label_high ~ Distance + Regionname + Type + rooms_cat, data=housesTrain)
prob_values <- predict(mod, housesTrain, type='raw')
opt_f1_obj <- opt_f1_function(prob_values, housesTrain, 'price_label_high')
opt_f1_obj$p1
opt_f1_obj$p2

predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=prob_values[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_obj$threshold, linetype='dashed', color=palette34[4])
```



NOTA: ESTO MEJOR CUANDO ELIJAMOS EL MEJOR MODELO
Mediante estos gráficos, podemos ver de una forma gráfica las muestras que quedarían clasificadas en función del umbral que se elija.

```{r}
th = 0.5
preds_prob <- predict(rf2, type = "prob")
predictions <- data.frame(price_label_high=housesTrainOrigVar$price_label_high, pred=preds_prob[,2])
predictions$price_label_high <- ifelse(predictions$price_label_high==TRUE,1,0)
df_preds <- pred_type_distribution(predictions, th, 'price_label_high')
```


```{r}
 ggplot(data=df_preds, aes(x=price_label_high, y=pred)) + 
    geom_violin(fill=rgb(1,1,1,alpha=0.6), color=NA) + 
    geom_jitter(aes(color=pred_type), alpha=0.6) +
    geom_hline(yintercept=th, color="red", alpha=0.6) +
    scale_color_discrete(name = "type") +
    labs(title=sprintf("Threshold at %.2f", th))
```

# Evaluación

```{r}
dfMetricas <- data.frame(
  Modelos = c('Regresión logística', 'Árbol de decisión', 'Random forest', 'KNN'),
  F1 = c('0.7401130', '0.7005650', 'x', '0.7330383'))

dfMetricas %>% kable() %>% kable_styling(bootstrap_options = c('hover'), position = 'center')

#stripe_index utilizar dentro de kable_styling para marcar el modelo con mejor f1
```



