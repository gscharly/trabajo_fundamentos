---
title: "ml1"
author: "Carlos Gomez Sanchez"
date: "25 de enero de 2020"
output: html_document
---

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(cluster)
library(factoextra)

set.seed(10)
```


```{r}
housesTrain <- read.csv('train_set_new_variables.csv', encoding = 'UTF-8')
```


# PCA
Con las 3 primeras componentes, se captura cerca del 85% de la varianza de los datos.
```{r}
housesNum <- housesTrain %>% select(sqrt_distance, log_landsize, Lattitude, Longtitude)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```


Probamos a lanzar un modelo de regresión lineal múltiple con las variables numéricas después de aplicar PCA.
```{r}
train <- housesTrain %>% select(-Price)

train$Regionname = factor(train$Regionname, levels=c('Southern_Metropolitan', 'Northern_Metropolitan', 'Western_Metropolitan', 'Eastern_Metropolitan', 'South_Eastern_Metropolitan', 'Eastern_Victoria', 'Northern_Victoria', 'Western_Victoria'))
train$Method = factor(train$Method, levels=c('S', 'SP', 'PI', 'VB', 'SA'))
train$Type = factor(train$Type, levels=c('h', 'u', 't'))
train$car_cat = factor(train$car_cat, levels = c("Pocas_plazas", "Muchas_plazas"))
train$year_built_cat = factor(train$year_built_cat, levels = c('Antigua', 'Moderna', 'Desconocido'))
train$rooms_cat = factor(train$rooms_cat, levels = c('Pequeñas','Medianas','Grandes'))
train$bed_cat = factor(train$bed_cat, levels=c("Pocos_dormitorios", "Muchos_dormitorios"))
train$bath_cat = factor(train$bath_cat, levels=c("Pocos_baños", "Muchos_baños"))
train$sell_rate_cat = factor(train$sell_rate_cat, levels=c("Menos_populares", "Más_populares"))
```

```{r}
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

Hemos conseguido que las variables de entrada sean independientes, pero estas nuevas variables no están correladas linealmente con la salida, por lo que el modelo no debería ser demasiado bueno.
```{r}
train_pca_cat %>% select(PC1, PC2, PC3, log_price) %>%
    na.omit() %>%
    ggpairs(columns=1:4)
```


```{r}
lm_model <- lm(log_price~PC1 + PC2 + PC3 + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data = train_pca_cat)
summary(lm_model)
```

# Conversión a problema de clasificación
Para aplicar los siguientes algoritmos, necesitamos convertir nuestro problema a un problema de clasificación. Para ello, vamos a construir una variable *price_label* que dividirá las casas en dos grupos: baratas y caras. El problema ahora será clasificar las viviendas en una de estas dos categorías (se podría extender a más categorías para un problema multiclase).


```{r}
housesTrain %>% ggplot(aes(x=Price)) + geom_histogram()
```



```{r}
housesTrain <-housesTrain %>% mutate(price_label = Price > median(Price))
housesTrain %>% ggplot(aes(x=Price, color=price_label)) + geom_density()
```

```{r}
housesTrain %>% count(price_label)
```

# kNN

```{r}
test <- read.csv('test_set_new_variables.csv', encoding = 'UTF-8')
test <- test %>% mutate(price_label = Price > median(Price))
```

```{r knn1}
# Se usa distancia euclidea para este kNN
prediccion_knn1 =knn(housesTrain[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")], test[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")], k=1, cl=housesTrain$price_label)

table(prediccion_knn1,test$price_label)

# Medidas de precisión

accuracy = sum(prediccion_knn1 == test$price_label) /nrow(test)
error = 1-accuracy

# Acierto sobre el total de las casas CARAS, sensitivity o recall
sensitivity = sum(prediccion_knn1 == test$price_label & test$price_label == TRUE) / sum(test$price_label == TRUE)
recall = sensitivity

# Acierto sobre el total de las casas BARATAS
specificity =  sum(prediccion_knn1 == test$price_label & test$price_label == FALSE) / sum(test$price_label == FALSE)

# Acierto cuando el predicho es CARO
precision = sum(prediccion_knn1 == test$price_label & prediccion_knn1 == TRUE) / sum(prediccion_knn1 == TRUE)

# Acierto cuando el predicho es BARATO
npv = sum(prediccion_knn1 == test$price_label & prediccion_knn1 == FALSE) / sum(prediccion_knn1 == FALSE)

# F1_score
f1score = 2*precision*recall /(precision+recall)
c(accuracy = accuracy, error = error, sensitivity = sensitivity, specificity = specificity, precision = precision, npv = npv, f1=f1score)
```

```{r confusion_knn1}
confusionMatrix(table(prediccion_knn1,test$price_label), positive="TRUE")
```

## k óptimo

```{r}
long = 15
accuracy = rep(0,long)
f1score = rep(0,long)
recall = rep(0,long)
precision = rep(0,long)
for (i in 1:long)
{
  prediccion_knn_cv =knn.cv(housesTrain[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")], 
                            k=i, cl=housesTrain$price_label)
  accuracy[i] = sum(prediccion_knn_cv == housesTrain$price_label) /nrow(housesTrain)
  recall[i] = sum(prediccion_knn_cv == housesTrain$price_label & housesTrain$price_label == TRUE) / sum(housesTrain$price_label == TRUE)
  precision[i] = sum(prediccion_knn_cv == housesTrain$price_label & prediccion_knn_cv == TRUE) / sum(prediccion_knn_cv == TRUE)
  f1score[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])
}
resultados_knn = as.data.frame(cbind(accuracy,f1score,precision,recall))
resultados_knn = resultados_knn %>% mutate(index=as.factor(seq(1:long)))

max(resultados_knn$f1score)
which.max(resultados_knn$f1score)


ggplot(data=resultados_knn,aes(x=index,y=accuracy)) + 
  geom_col(colour="cyan4",fill="cyan3")+
  ggtitle("Accuracy")


ggplot(data=resultados_knn,aes(x=index,y=f1score)) + 
  geom_col(colour="orange4",fill="orange3") +
  ggtitle("F1_score values")
```
Para k=5 obtenemos un accuracy y un f1 score cercanos a 0.8, Se observa que a medida que k va aumentando, las métricas no incrementan.

## Test con k óptimo
```{r knn5}
# En train
prediccion_knn5_train =knn.cv(housesTrain[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")], 
                              k=5, cl=housesTrain$price_label)
confusionMatrix(table(prediccion_knn5_train,housesTrain$price_label), positive="TRUE")

#En test
prediccion_knn5_test=knn(housesTrain[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")], test[,c("sqrt_distance","log_landsize", "Lattitude", "Longtitude")],
                         k=5, cl=housesTrain$price_label)
confusionMatrix(table(prediccion_knn5_test,test$price_label), positive="TRUE")
```


# k-means

```{r}
housesTrainKmeans <- housesTrain %>% dplyr::select(-Regionname, -Type, -Method, -year_built_cat)

#las variables categóricas pasan a ser factores ordenados
housesTrainKmeans['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric()

housesTrainKmeans['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()

housesTrainKmeans['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric()

housesTrainKmeans['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric()

housesTrainKmeans['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric()



```

```{r kmeans}
#nume <- housesTrainKmeans %>% dplyr::select(sqrt_distance, log_landsize, Lattitude, Longtitude, Price, log_price)

fac = as.matrix(housesTrainKmeans)

#Guardamos el número filas
n <- nrow(fac)

#Probamos un cluster de 2 grupos
clusters2=kmeans(fac,centers=2,nstart=25)

#Para ver la variabilidad dentro de los grupos correspondiente a esos dos clusters
clusters2$withinss

#Buscamos el número de clusters óptimo 

#Inicializamos el vector
SSW <- vector(mode = "numeric", length = 15)
#Variabilidad de todos los datos, es decir, todos los datos como un único cluster
SSW[1] <- (n - 1) * sum(apply(X = fac, MARGIN = 2, FUN = 'var'))
#Variabilidad de cada modelo, desde 2 clusters hasta 15 clusters
for (i in 2:15) SSW[i] <- sum(kmeans(fac,centers=i,nstart=25)$withinss)
#Dibujamos un gráfico con el resultado
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within
groups",pch=19, col="steelblue4")

#Otra manera de encontrar el número de clusters óptimo: silhouette coefficient 
#This coefficiente ranges from  −1  to  1 , so that:
  #Near +1 indicate that the sample is far away from the neighboring clusters.
  #A value of 0 indicates that the sample is on or very close to the decision boundary     between two neighboring clusters and
  #Negative values indicate that those samples might have been assigned to the wrong       cluster.

fviz_nbclust(fac, kmeans, method='silhouette')

avg_sil <- function(k) {
  km.res <- kmeans(fac, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(fac))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- sapply(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```

Según las tres gráficas obtenidas, el k óptimo es 2. (dudo??)





