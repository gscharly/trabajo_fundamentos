---
title: "Machine Learning 1"
author: 
- name: Paula Santamaría Villaverde
- name: Manuel Jesús Pertejo Lope
- name: Carlos Gómez Sánchez
date: "21 de marzo de 2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      section_divs: true
    theme: "sandstone"
    highlight: "zenburn"
    code_folding: "hide"
---

```{r setup, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(error = TRUE)
```

```{r imagenMelbourne, echo=FALSE}
knitr::include_graphics("melbourneModif.png")
```

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(cowplot)
library(scales)
library(tidyr)
library(InformationValue)
library(rpart)
library(rpart.plot)
library(e1071)
library(pROC)
library(gridExtra)
library(glmnetUtils)
library(kmed)
library(Rtsne)
library(doFuture)
library(randomForest)
library(grid)
library(party)
library(ROCR)
library(tidymodels)
library(workflows)
library(knitr)
library(kableExtra)
library(gridExtra)
library(Information)
library(dendextend)
library(ape)
library(PRROC)
library(Hmisc)
library(plyr)
library(reshape2)
library(ggmap)
library(stringr)
library(fuzzyjoin)
library(woeBinning)

source('utils/pred_type_distribution.R')
source('utils/calculate_roc.R')
source('utils/plot_roc.R')
source('utils/metrics_function.R')
source('utils/plot_utils.R')
source('utils/try_functions.R')
set.seed(10)
```

```{r paleta34}
#Paleta creada a partir de Brewer Dark2
palette34 = c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D", "#666666","#1DA632","#9E1B84","#1B849E","#7CD902","#A61E22","#1EA6A2","#8AE729", "#AEB370","#E7E529","#70B397","#29E786","#292BE7","#E7292B","#B3708D","#AFE602", "#02E6AB","#02AFE6","#92A61D","#771B9E","#1DA676","#1D4DA6","#02D95F","#02D9CB","#D9CB02","#B37570","#B39670")
```

```{r}
#houses <- read.csv('./datasets/melb_data.csv')
housesTrain <- read.csv('base_train.csv')
housesTest <- read.csv('base_test.csv')
housesVal <- read.csv('base_val.csv')
```
```{r}
housesTrain$kmeans_cluster <- factor(housesTrain$kmeans_cluster, levels = c(1,2,3,4,5))
housesVal$kmeans_cluster <- factor(housesVal$kmeans_cluster, levels = c(1,2,3,4,5))
housesTest$kmeans_cluster <- factor(housesTest$kmeans_cluster, levels = c(1,2,3,4,5))
```
 
# Definición de objetivos

<br>

*Objetivo general*

- Clasificar las viviendas de la ciudad de Melbourne

*Objetivos específicos*

- Realizar un análisis explotratorio multivariante de todas las variables con respecto a la nueva variable target (*price_label*)
- Seleccionar las variables adecuadas al modelo.
- Ajustar los distintos modelos de clasificación vistos en la asignatura.
- Comparación de modelos en base a una métrica
- Elección del mejor punto de corte para nuestro problema
- Evaluación del modelo final

Enlace a Github: https://github.com/gscharly/trabajo_fundamentos


# Precio de la vivienda en Melbourne

A modo de introducción, se muestran una serie de mapas de Melbourne.

En el primero, podemos ver, de un solo vistazo, cómo se distribuyen las viviendas vendidas según su región. Vemos que hay zonas de Melbourne, como la zona oeste de la bahía donde no hay viviendas registradas en nuestro data set. Se puede observar que la mayoría de casas vendidas se concentran en el centro de la ciudad de Melbourne, pero hay casas en regiones que se encuentran bastante alejadas (Victoria). Se ve que estas regiones son amplias, y que posiblemente el precio no solo varíe de unas a otras, si no también dentro de la misma región. Además, la ciudad destaca por la bahía de Port Phillip, habiendo casas vendidas que rodean esta bahía sin estar cerca del centro. Esto puede hacer pensar que el precio no tiene por qué estar directamente relacionado con la distancia al centro (variable disponible en el dataset), si no que también podría estar influido por la posición respecto a la bahía.


```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10}
melbourne_map <- get_map(location = "melbourne", zoom = 10)
houses_map <- houses[,c("Regionname", "Lattitude", "Longtitude")]
houses_map$region_colours <- revalue(houses_map$Regionname, c("Southern Metropolitan" = "#E6AB02", "Western Metropolitan" = "#A6761D", "Northern Metropolitan" = "#7570B3",
                                                      "Eastern Metropolitan" = "#1B9E77", "South-Eastern Metropolitan" = "#66A61E", "Eastern Victoria" = "#D95F02",
                                                      "Northern Victoria" = "#E7298A", "Western Victoria" = "#666666"))
  
ggmap(melbourne_map) + geom_point(aes(x = houses_map$Longtitude, y = houses_map$Lattitude), colour=houses_map$region_colours , data = houses, alpha=0.5, size = 0.75) + labs(title='Casas vendidas por región')
```


Si pintamos el precio medio por cada región, se puede observar que hay regiones más caras que otras en media. Sin embargo, dentro de una misma región puede haber bastante variabilidad de precio. Si se representa el precio medio por barrio, se observa que en la región más cara efectivamente se encuentran las casas más caras, pero también otras que son más baratas. Parece que se confirma que los barrios pegados a la bahía tienen un precio más alto, aunque hay viviendas caras en el interior también.


```{r mapsPrice, warning=FALSE}
housesMeanPriceRegion <- houses %>% dplyr::group_by(Regionname) %>% dplyr::summarise(mean_price_region = mean(Price), sd_price_region = sd(Price))
housesMeanPriceSuburb <- houses %>% dplyr::group_by(Suburb) %>% dplyr::summarise(mean_price_suburb = mean(Price), sd_price_suburb = sd(Price))

housesPrice <- houses %>% inner_join(housesMeanPriceSuburb, by='Suburb') %>% inner_join(housesMeanPriceRegion, by='Regionname')

ggmap(melbourne_map) + geom_point(aes(x = housesPrice$Longtitude, y = housesPrice$Lattitude,  colour=housesPrice$mean_price_region), data = housesPrice, alpha=0.5, size = 0.5) + scale_colour_gradient(low='Blue', high='red') + labs(title='Precio medio en función de la región')

ggmap(melbourne_map) + geom_point(aes(x = housesPrice$Longtitude, y = housesPrice$Lattitude,  colour=housesPrice$mean_price_suburb), data = housesPrice, alpha=0.5, size = 0.5) + scale_colour_gradient(low='Blue', high='red') + labs(title='Precio medio en función del barrio')
```

A pesar de que el barrio parece indicativo y distintivo de precios más baratos y más caros, dentro de los barrios "caros" sigue habiendo variedad de precios. Esto se puede ver si se representa la desviación típica del precio por barrio:


```{r mapsSdPrice, warning=FALSE}
ggmap(melbourne_map) + geom_point(aes(x = housesPrice$Longtitude, y = housesPrice$Lattitude,  colour=housesPrice$sd_price_suburb), data = housesPrice, alpha=0.5, size = 0.5) + scale_colour_gradient(low='Blue', high='red') + labs(title='Desviación típica del precio en función del barrio')
```


Con estos mapas, se pretende introducir el hecho de que el precio de venta varía con la zona en la que está situada la vivienda y que puede ayudar a distinguir entre viviendas caras y baratas, aunque serán necesarias otras variables que expliquen el precio en aquellas zonas en las que la variabilidad del mismo sea elevada.


# Análisis exploratorio en base al nuevo objetivo
El dataset original estaba enfocado a la predicción del precio de viviendas en Melbourne. Para convertirlo en un problema de clasificación, vamos a utilizar la variable *Price* para diferenciar entre casas baratas y caras. Tenemos varias opciones:


- Utilizar la mediana de *Price* para tener un conjunto de datos balanceado, pero donde habrá muchas muestras cercanas con las que tendremos problemas a la hora de clasificar.
- Establecer el corte en un valor de *Price* mayor, donde tendremos un problema de desbalanceo de datos.

Se ha optado por la segunda opción, para introducir una mayor complejidad al problema de clasificación.

Se presenta de nuevo el mapa de Melbourne teniendo en cuenta esta nueva separación. Vemos que la zona de casas caras sigue siendo parecida, pero se concentran en puntos más específicos. 

```{r warning=FALSE}
housesPriceSuburb <- housesTrain %>% dplyr::group_by(Suburb) %>% dplyr::summarise(sum_price_label = sum(price_label_high))

housesPriceLabel <- housesTrain %>% inner_join(housesPriceSuburb, by='Suburb')

ggmap(melbourne_map) + geom_point(aes(x = housesPriceLabel$Longtitude, y = housesPriceLabel$Lattitude,  colour=housesPriceLabel$sum_price_label), data = housesPriceLabel, alpha=0.5, size = 0.5) + scale_colour_gradient(low='Blue', high='red') + labs(title='Número de 1s (casas caras) en función de la región')
```

Con esta nueva variable objetivo, se procede a repetir parte del análisis multivariante realizado en el anterior informe, de cara a estudiar la relación con la nueva variable objetivo, y empezar a preseleccionar variables útiles para los modelos.

Además, se realizan test estadísticos para comprobar la independencia de las variables con el objetivo (que ahora es una variable objetivo). 

Contraste chi cuadrado: para comprobar la independencia entre dos variables categóricas.
H0: las variables son independientes
H1: las variables no son independientes

One way anova test: permite ver si hay diferencias entre la media de una variable numérica en los distintos grupos de una categórica-
H0: no hay diferencia en la media de los grupos
H1: al menos la media de un grupo es diferente

## Variables categóricas

__Regionname__

Representamos el número de casas baratas/cara en cada región, tanto en valor absoluto como porcentual. Se puede observar que el barrio con mayor número de casas caras es Southern Metropolitan, mientras que Eastern Metropolitan tiene mayor porcentaje de casas caras.

```{r}
bar_plot_target(housesTrain, "Regionname", "price_label_high")
```


Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Regionname, housesTrain$price_label_high)
```

__Type__

Recordemos los distintos tipos de casas que hay:

* h: houses, cottage, villa, semi, terrace
* t: townhouse
* u: unit

Se puede observar que al mayoría de casas caras se engloban en el tipo h.

```{r}
bar_plot_target(housesTrain, "Type", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Type, housesTrain$price_label_high)
```


__Method__

Los métodos de venta son los siguientes:

* S - property sold
* SP - property sold prior
* PI - property passed in
* VB - vendor bid
* SA - sold after auction

Porcentualmente, parece que hay más casas caras vendidas con el método VB.

```{r}
bar_plot_target(housesTrain, "Method", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$Method, housesTrain$price_label_high)
```

__CouncilArea__

Parece que áreas como Bayside o Boroondara tienen casas más caras. Sin embargo, al haber tantas categorías, no es una variable para incluir en los modelos.

```{r fig.width=15, fig.height=4}
housesTrain %>% ggplot() + geom_bar(aes(x=CouncilArea, fill=price_label_high)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_fill_manual(values=palette34)
housesTrain %>% ggplot() + geom_bar(aes(x=CouncilArea, fill=price_label_high), position='fill') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    scale_fill_manual(values=palette34)
```

Teniendo en cuenta el ámbito de aplicación, pensamos que aquellas viviendas que estén en la bahía serán más caras. Utilizando la variable *CouncilArea*, se han seleccionado aquellas zonas que rodean la bahía. 

Parece que hay mas casas caras porcentualmente en aquellas zonas con posibles vistas al mar.


```{r}
water_councils <- c('Wyndham', 'Hobsons Bay', 'Port Phillip', 'Bayside', 'Kingston', 'Frankston', 'Stonnington')
housesTrain$may_have_water <- factor(ifelse(housesTrain$CouncilArea %in% water_councils, TRUE, FALSE))

bar_plot_target(housesTrain, "may_have_water", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$may_have_water, housesTrain$price_label_high)
```

__rooms_cat__

Como es lógico, aquellas casas con más habitaciones son más caras.

```{r}
bar_plot_target(housesTrain, "rooms_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$rooms_cat, housesTrain$price_label_high)
```


__bath_cat__
Exactamente lo mismo que con el número de habitaciones.

```{r}
bar_plot_target(housesTrain, "bath_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$bath_cat, housesTrain$price_label_high)
```

__bed_cat__
De nuevo, si tiene más dormitorios, la casa suele ser cara.

```{r}
bar_plot_target(housesTrain, "bed_cat", "price_label_high")
```


Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$bed_cat, housesTrain$price_label_high)
```


__car_cat__
Mismas (y lógicas) conclusiones.

```{r}
bar_plot_target(housesTrain, "car_cat", "price_label_high")
```

Se rechaza la hipótesis de independencia:
```{r}
chisq.test(housesTrain$car_cat, housesTrain$price_label_high)
```

__Sell date__

Lo primero que hacemos con esta variable será extraer el año, ya que viene en formato dd/MM/yyyy, y probablemente el año nos proporcione más información acerca del precio de venta de la casa.


```{r CrearSellYear}
housesTrain$sellYear <- separate(housesTrain, Date, c('day','month', 'year'), sep = '/')$year
housesTrain$sellYear <-factor(housesTrain$sellYear, levels=c("2016", "2017"))
```

Observamos que solo hay 2 años de venta, y que no tiene demasiada relación con el precio.

```{r SellYearPrice, message=FALSE}
bar_plot_target(housesTrain, "sellYear", "price_label_high")
```

En este caso, no se puede rechazar la hipótesis nula.
```{r}
chisq.test(housesTrain$sellYear, housesTrain$price_label_high)
```

__sell_rate_cat__
Recordemos que esta variable indica si el barrio en el que está una casa tiene muchas casas vendidas. Parece que hay una ligera diferencia porcentualmente entre barrios populares y no populares en cuanto a casas caras.

```{r, message=FALSE}
bar_plot_target(housesTrain, "sell_rate_cat", "price_label_high")
```

Se puede rechazar la hipótesis de independencia.
```{r}
chisq.test(housesTrain$sell_rate_cat, housesTrain$price_label_high)
```


__year_built_cat__
El problema con esta variable venía con que había muchas casas sin fecha de construcción informada. Sí que se puede intuir que hay más casas caras porcentualmente en la categoría de casas antiguas.

```{r, message=FALSE}
bar_plot_target(housesTrain, "year_built_cat", "price_label_high")
```

```{r}
chisq.test(housesTrain$year_built_cat, housesTrain$price_label_high)
```


## Variables cuantitativas


__Correlaciones__


```{r}
housesNum <- housesTrain %>% mutate(log_landsize = log10(LandsizeImp), sqrt_distance = sqrt(Distance), log_room_land = log10(Rooms/LandsizeImp*1000))
numeric_cols <- c("sqrt_distance", "Lattitude", "Longtitude", "log_landsize", "log_room_land")
cormat <- round(cor(na.omit(housesNum[,numeric_cols])), 2)

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  # Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "#1EA6A2", high = "#A61E22", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

ggheatmap + geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```


__Distance__
Se observa en los boxplots que las casas caras están ligeramente más cercanas al centro de la ciudad.

```{r}
p<-housesTrain %>% mutate(sqrt_distance = sqrt(Distance)) %>%
  select(sqrt_distance, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Se rechaza la hipótesis nula: hay diferencias por categoría.
```{r}
summary(aov(price_label_high~Distance, data=housesTrain))
```

__Latitud y longitud__

Las distribuciones parecen indicar que el precio de la vivienda aumenta en la parte sur (menor latitud) y en la parte este (mayor longitud) de Melbourne. 

```{r}
p<-housesTrain %>%
  select('Lattitude', 'Longtitude', 'price_label_high') %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Para ambas variables se rechaza la hipótesis nula.
```{r}
summary(aov(price_label_high~Lattitude, data=housesTrain))
summary(aov(price_label_high~Longtitude, data=housesTrain))
```


__Landsize__

Las casas caras parece que tienen un tamaño de parcela ligeramente mayor. Además la variabilidad respecto al tamaño es bastante menor en las casas caras.

```{r}
p<-housesTrain %>% mutate(log_landsize = log10(LandsizeImp)) %>% 
  select(log_landsize, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

Se puede rechazar la hipótesis nula de que la media es igual en ambas categorías.
```{r}
summary(aov(price_label_high~log_landsize_std, data=housesTrain))
```


# Selección de variables

El aálisis exploratorio realizado nos permite saber para qué variables la distribución de casas baratas y caras es diferente, es decir, nos indica qué variables pueden ayudar más a la clasificación. 
Para seguir con la selección de variables vamos a untilizar el Information Value (IV). Esta medida nos indica cómo de buena es una variable a la hora de distinguir entre las categorías binarias de la variable respuesta o dependiente.

```{r}
a <- housesTrain %>% select("CouncilArea", "sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std", "rooms_cat", "year_built_cat", "car_cat", "Regionname", "Type", "Method", "bath_cat", "bed_cat", "sell_rate_cat", "price_label_high")


a$price_dep <- a %>% pull(price_label_high) %>% as.numeric()
a <- a[-15]
table(a$price_dep) %>% kable() %>% kable_styling(position = 'center', row_label_position = 'c')

infoTables <- create_infotables(data = a, y = "price_dep",
                              bins = 10,
                              parallel = F)

infoTables$Summary %>% kable() %>% kable_styling(position = 'center', row_label_position = 'c')
plotFrame <- infoTables$Summary[order(-infoTables$Summary$IV),]
plotFrame$Variable <- factor(plotFrame$Variable, levels = plotFrame$Variable[order(-plotFrame$IV)])

ggplot(plotFrame, aes(x = Variable, y = IV)) +
geom_bar(width = .35, stat = "identity", color = palette34[1], fill = "white") +
ggtitle("Information Value") +
theme_minimal() +
theme(plot.title = element_text(size = 12, hjust = 0.5)) +
theme(axis.text.x = element_text(angle = 90))
```

A la vista de los resultados, se puede ver que aquellas variables relacionadas con el tamaño de la vivienda (Type, rooms_cat) y con su ubicación (CouncilArea, lattitude, longtitude, Regionname) tienen mucho peso a la hora de clasificar la variable target.


# Aprendizaje no supervisado: clustering

En primer lugar, vamos a definir la base de datos con las variables cuantitativas y cualitativas ordinales a partir del dataset *housesTrain*.

Las variables cuantitativas las cogemos estandarizadas para eliminar el efecto de las distintas escalas de medida y las ordinales las estandarizamos al crearlas.

Estas son todas las variables que pueden emplearse para los métodos de clustring. Sin embargo, no se empezará probando con todas ellas sino que se tomarán de menos a más utilizando diferentes combinaciones. 

```{r}
housesTrainCluster <- housesTrain %>% dplyr::select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)

#las variables categóricas pasan a ser factores ordenados
housesTrainCluster['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() %>% scale()

housesTrainCluster['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric() %>% scale()

housesTrainCluster['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()

housesTrainCluster['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() %>% scale()

housesTrainCluster['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() %>% scale()
```


## k-means

Ahora, vamos a buscar el número de clústers óptimo. La idea es que el número seŕa él óptimo cuando los individuos de un mismo grupo sean lo más homogéneos posible y los individuos pertenecientes a distintos grupos sean lo más heterogéneos posible. Esto es lo mismo a buscar una división donde una suma de cuadrados entre (betweens) sea suficientemente grande y, por tanto, una suma de cuadrados dentro lo suficientemente pequeña (withins).

En las siguientes dos gráficas, se muestra el *betweens* y el *withins* para un número de clusters desde 1 hasta 15. Aquel punto en el que se produce el codo y le sigue una cierta estabilización es el número de clusters que se toma como óptimo.


La forma que tome esta gráfica nos ayuda a seleccionar las variables. Si la combinación de variables empleada genera una gráfica sin un codo claro y sin que haya una cierta estabilización, significaría que empleando esas variables no va a ser posible hacer clusters bien definidos.

De todas las combinaciones probadas, atendiendo a la forma del gráfico y a la información de selección de variables, se van a utilizar la latitud y el número de baños para formar los grupos. 


```{r kmeansKoptimo2}
n = nrow(housesTrain)
SSW <- vector(mode = "numeric", length = 15)
SSW[1] <- (n - 1) * sum(apply(X = selec_var_kmeans, MARGIN = 2, FUN = 'var'))
for (i in 1:15) SSW[i] <- sum(kmeans(selec_var_kmeans,centers=i,nstart=25)$withinss)
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col=palette34[2])
```

```{r}
clust_kmeans=kmeans(selec_var_kmeans,centers=5,nstart=25)
clust_kmeans
```

Observando el ratio de la suma de cuadrados entre-clusters y la suma de cuadrados totales, vemos que el porcentaje de varianza explicada por el modelo respecto al total de varianza observada es muy elevado(85.1%). Sin embargo, este indicador hay que tomarlo con cuidado puesto que se ve influído por el número de clusters empleado, de manera que aumenta conforme tomamos más grupos.


### PCA + visualización

```{r}
pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- clust_kmeans$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34)
```

Vamos a comprobar si los grupos generados a partir de la combinación de una variable del tamaño de la vivienda (bath_cat) y de su localización (lattitud_std) muestran diferencias respecto a la variable target (price_label_hight). En este caso, estos grupos podrían emplearse como variable en otros modelos.

```{r}
housesTrain <- housesTrain %>% mutate(kmeans_cluster = clust_kmeans$cluster)
bar_plot_target(housesTrain, "kmeans_cluster", "price_label_high")
chisq.test(housesTrain$kmeans_cluster, housesTrain$price_label_high)
```


## Kmeans WoE

```{r}
housesTrainWoe <- a
binning <- woe.binning(housesTrainWoe, 'price_dep', housesTrainWoe)
housesTrainWoe <- woe.binning.deploy(housesTrainWoe, binning,
                                               add.woe.or.dum.var='woe')

housesTrainWoe <- housesTrainWoe %>% select("woe.Type.binned", "woe.rooms_cat.binned",         "woe.CouncilArea.binned", "woe.Regionname.binned", "woe.lattitude_std.binned", "woe.bath_cat.binned", "woe.bed_cat.binned", "woe.longtitude_std.binned", "woe.car_cat.binned",          
"woe.sqrt_distance_std.binned", "woe.log_landsize_std.binned", "woe.year_built_cat.binned", "woe.Method.binned", "woe.sell_rate_cat.binned")

select_var_woe = housesTrainWoe %>% select("woe.Type.binned", "woe.lattitude_std.binned")

n = nrow(housesTrainWoe)
SSW <- vector(mode = "numeric", length = 6)
SSW[1] <- (n - 1) * sum(apply(X = select_var_woe, MARGIN = 2, FUN = 'var'))
for (i in 1:6) SSW[i] <- sum(kmeans(select_var_woe,centers=i,nstart=25)$withinss)
plot(1:6, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col=palette34[2])

clust_kmeans_woe=kmeans(select_var_woe,centers=3,nstart=25)
clust_kmeans_woe


pca_var <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- clust_kmeans_woe$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34)

housesTrain <- housesTrain %>% mutate(kmeans_cluster_woe = clust_kmeans_woe$cluster)
bar_plot_target(housesTrain, "kmeans_cluster_woe", "price_label_high")
chisq.test(housesTrain$kmeans_cluster, housesTrain$price_label_high)

```

## Hierarchical

Se aplicará un cluster aglomerativo y se emplearán diferentes métodos de medida de la distancia entre clusters. 

```{r hierarchical}
mat_dist <- dist(selec_var_kmeans, method = "euclidean")
hc1 <- hclust(mat_dist, method = "single" )
den = as.dendrogram(hc1, leaflab = "none")
plot(den)
```

A pesar de que el dendograma no ofrece una visualización clara debido al alto número de observaciones, sí se puede percibir como dos grandes clustes, siendo uno de ellos (el de la izquierda), aproximadamente, tres veces mayor que el otro y unas observaciones a la derecha del todo que se aglomeran en los últimos niveles.

Una vez creado el dendrograma, vamos a evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones empleando el coeficiente de correlación entre la altura de los nodos del dendrograma (distancia cophenetic) y la matriz de distancias original. Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos. Esta medida puede emplearse como criterio de ayuda para escoger entre los distintos métodos de linkage.

```{r}
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")
hc_euclidea_single  <- hclust(d = mat_dist, method = "single")
hc_euclidea_centroid  <- hclust(d = mat_dist, method = "centroid")
cor(x = mat_dist, cophenetic(hc_euclidea_complete))
cor(x = mat_dist, cophenetic(hc_euclidea_average))
cor(x = mat_dist, cophenetic(hc_euclidea_single))
cor(x = mat_dist, cophenetic(hc_euclidea_centroid))
```

A la vista de los resultados, el método que mejor refleja las distancias reales es el *average*. Este método supone un compromiso entre las dos características de los métodos en los que se emplea la distancia mínima (*single*) y en los que se emplea la máxima (*complete*). Un compromiso entre ser sensible a datos atípicos y manejar formas de clusters no elípticas.

Se prueba a graficar el dendograma empleando el método *average* y probando con cortes del árbol a diferentes alturas. Sin embargo, el alto número de observaciones impide tomar una decisión más o menos clara de por dónde plantear el corte. 

```{r, warning=FALSE}
hc2 <- hclust(mat_dist, method = "average" )
den = as.dendrogram(hc2)

avg_col_dend <- color_branches(den, h = 2, col = palette34)
plot(avg_col_dend, leaflab = "none", ylab = "Height")
abline(h = 2, col = 'black')
```


## DBSCAN

Los métodos de clustering empleados anteriormenete, son buenos encontrando agrupaciones con forma esférica y sensibles a outliers, pero fallan al tratar de identificar formas arbitrarias que es lo que tenemos en este caso.

DBSCAN evita este problema siguiendo la idea de que, para que una observación forme parte de un cluster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters están separados por regiones vacías o con pocas observaciones.


```{r dbscanEpsi}
datos = selec_var_kmeans
dbscan::kNNdistplot(datos, k = 8)
```

La curva tiene el punto de inflexión en torno a 0.5, por lo que se escoge este valor como epsilon para DBSCAN.

```{r dbscan}
set.seed(10)
dbscan_cluster <- fpc::dbscan(data = datos, eps = 0.1, MinPts = 50)
head(dbscan_cluster$cluster)

# Visualización de los clusters

pca_var <- housesTrainCluster %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(pca_var, center = TRUE)
summary(housesPCA)
cluster_group <- dbscan_cluster$cluster %>%  as.factor()
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=cluster_group)) + geom_point() +     scale_color_manual(values = palette34) + labs(title='PCA: DBSCAN clusters')

```


# Métodos de reducción de dimensionalidad

```{r}
housesTrain <- read.csv('base_train.csv')
```

## PCA

PCA debe trabajar con variables numéricas que además están normalizadas. Por ello, elegimos las 4 variables numéricas con las que trabajamos en la parte de regresión lineal múltiple (raíz cuadrada de la distancia, logaritmo de la parcela, latitud y longitud). Con las dos primeras componentes se explicaría un 66,3% de la varianza.


```{r}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```
```{r}
fviz_screeplot(housesPCA, ncp = 4, barfill = palette34[6], barcolor = palette34[6], linecolor = palette34[1], addlabels = TRUE, hjust = 0)
```



```{r}
train <- housesTrain %>% select(-Price)
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat, price_label_high)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

Se prueba a representar las dos componentes principales en dos dimensiones, intentando añadir una tercera dimensión en forma de color utilizando otras variables, con el objetivo de ver si hay diferencias en estas direcciones. Vemos por ejemplo que las casas caras parece que se concentran (ya se vio anteriormente que la localización influía en el precio), mientras que las casas baratas están más desperdigadas. 

Si pintamos la región, se pueden diferenciar fácilemente, ya que se han utilizado las coordenadas como entrada del PCA.


```{r}
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=price_label_high)) + geom_point(alpha=0.5) + labs(title='PCA: price label') + scale_color_manual(values = palette34)
train_pca_cat %>% ggplot(aes(x=PC1, y=PC2, color=Regionname)) + geom_point() + labs(title='PCA: Regionname') + scale_color_manual(values = palette34)
```


## MDS
Usaremos la distancia euclídea para realizar el MDS (debería ser muy parecido a PCA). Al tener bastantes datos, vamos a muestrear el train primero.

```{r}
housesTrainDim <- housesTrain

#las variables categóricas pasan a ser factores ordenados
housesTrainDim['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() %>% scale()

housesTrainDim['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric() %>% scale()

housesTrainDim['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() %>% scale()

housesTrainDim['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric() %>% scale()

housesTrainDim['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric() %>% scale()
```


```{r}
set.seed(10)
train_sample <- housesTrainDim[sample(nrow(housesNum), 2500),]
train_num_sample <- train_sample %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)
```

```{r}
# Distancia euclídea
d <- dist(train_num_sample) 
# MDS
fit <- cmdscale(d,eig=TRUE, k=2)
```



```{r}
df_fit <- data.frame(fit$points)
df_mds_cat <- merge(df_fit, train_sample, by.x=0, by.y=0)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point(alpha=0.5) + labs(title='MDS: price_label_high') + scale_color_manual(values = palette34)
df_mds_cat %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='MDS: Regionname') + scale_color_manual(values = palette34)
```

## tSNE
Probamos tsne sobre nuestras variables numéricas escaladas, para que valores extremos no nos den problemas. El parámetro que se suele tocar es el perplexity, que es algo parecido al número de vecinos más cercanos cuando se comparan las distribuciones que utiliza tSNE.
Fuente: https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/

De nuevo superponiendo distintas variables sobre el gráfico en dos dimensiones, podemos ver 

```{r}
houses_num_unique <- unique(housesNum)
tsne <- Rtsne(houses_num_unique, dims = 2, perplexity=50, verbose=TRUE, max_iter = 500)
```

```{r}
df_tsne <- data.frame(tsne$Y)
df_train_no_dup <- housesTrain[!duplicated(housesTrain[c("sqrt_distance_std", "log_landsize_std", "lattitude_std", "longtitude_std")]),]
df_train_no_dup$X1 <- df_tsne$X1
df_train_no_dup$X2 <- df_tsne$X2
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=price_label_high)) + geom_point() + labs(title='tSNE: price_label_high') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=sell_rate_cat)) + geom_point() + labs(title='tSNE: sell_rate_cat') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Regionname)) + geom_point() + labs(title='tSNE: Regionname') + scale_color_manual(values = palette34)
df_train_no_dup %>% ggplot(aes(x=X1, y=X2, color=Type)) + geom_point() + labs(title='tSNE: Type') + scale_color_manual(values = palette34)

```


# Aprendizaje supervisado

Como ya se ha comentado anteriormente, el problema a resolver consiste en clasificar las casas de la ciudad de Melbourne en dos categorías en función de su precio. La clase positiva es la que vamos a identificar con las casas con precios altos (“caras”) y la clase negativa con las casas con precios más bajos (“baratas”).

Para aportar cierto realismo al problema, vamos a ponernos en el papel de una agencia de compra-venta de inmuebles de lujo. Nuestro objetivo es, dado todo el conjunto de inmuebles en venta de Melbourne, intentar detectar el mayor número de casas “caras”,  para poder así comprarlas por un precio asequible y posteriormente venderlas obteniendo el mayor beneficio posible.

Ahora bien, nos interesa también que la precisión de nuestro modelo también sea alta. No queremos invertir en una casa según  lo que nos dice nuestro modelo, y que luego al no ser una casa realmente “cara”, no podamos sacar beneficio de su venta e incluso hasta perder dinero.  

Dicho esto,  se ha decidido que la métrica a utilizar para comparar la performance de los sucesivos modelos que se van a probar en las subsiguientes secciones, ha de ser la F1_score. Con ella, se pretende alcanzar un compromiso entre recall y precision, y así poder cumplir con los objetivos anteriormente descritos. 

# Regresión logística

Como primera aproximación, se ha probado el modelo de regresión logística con todas las variables disponibles en el dataset. El resultado obtenido es el siguiente:


```{r}
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label', "Method", "log_landsize_std", "rooms_cat")
housesTrain_lr <- housesTrain %>% select(-filter_cols)
housesTrain_lr$may_have_water <- factor(housesTrain_lr$may_have_water, levels = c(TRUE,FALSE))

lr_model <- readRDS("./models/lr_all_variables.rds")
#lr_model <- glm(price_label_high ~ ., family = binomial(link = 'logit'), data = housesTrain_lr)
summary(lr_model)
```

Se puede ver a partir de los coeficientes obtenidos, que las existen unas cuantas variables cuyos coeficientes no son siginificativamente diferentes de 0, por lo tanto lo más inteligente sería eliminarlas. Destacar que las más relevantes para el modelo son las numéricas relativas a la situación de las viviendas (latitud, longitud, distancia al centro...), así como las relativas al tamaño de la casa (número de baños, de plazas de garaje etc). También, y tal como habíamos visto en la parte de análisis exploratorio, la variable categórica tipo de vivienda es bastante relevante también para el modelo. 

De todas maneras, vamos a obtener las métricas de performance más relevantes de este modelo de cara a compararlo con uno más sencillo que construiremos a posteriori con menos variables:

```{r}
prob_lr <- lr_model %>% predict(housesTrain_lr, type = "response")
opt_f1_lr <- opt_f1_function_v2(prob_lr, housesTrain_lr, "price_label_high")
c(optimal_threshold = opt_f1_lr$threshold, precision = opt_f1_lr$precision, recall =  opt_f1_lr$recall, f1_score = opt_f1_lr$f1_opt)
```

Al igual que ocurre para el caso de regresión lineal, para el caso de regresión logística también se pueden aplicar las técnicas de regularización sobre los coeficientes de la misma. En concreto, se va a utilizar una regresión logística Lasso y observar los coeficientes obtenidos para cada una de las variables:

```{r}
set.seed(123)
lasso <- glmnet(price_label_high~., data = housesTrain_lr, alpha = 1, type.measure = "deviance", family = "binomial") 
cv.lasso <- cv.glmnet(price_label_high~., data = housesTrain_lr, alpha = 1, type.measure = "deviance", family = "binomial") # Con 10-fold cross-validation
#plot(cv.lasso)
par(mar=c(4.5,4.5,1,4))
plot(lasso)
coefs_lasso <- coef(lasso, lasso$lambda.min)
coefs_lasso <- coefs_lasso[-1, ncol(coefs_lasso)]
axis(4, at=coefs_lasso,line=-.5,label=names(coefs_lasso),las=1,tick=FALSE, cex.axis=0.5)
```

```{r}
coef(cv.lasso, cv.lasso$lambda.1se)
```


Con la aplicación realizada mediante lasso, confirmamos un poco las sospechas que teníamos. Las variables numéricas relativas a la situación siguen siendo relevantes para lasso. Lo que ocurre es que en algunos casos el valor de los coeficientes asociados a ciertos niveles dentro de una misma variable categórica, son muy dispares. Esto se debe o bien a que hay un nivel muy predominante, o bien a que hay ciertos niveles que no son nada relevantes (y quizás podrían agruparse) pero otros sí. Un ejemplo claro de esto es el caso de la variable Regionname. Tal y como vimos en el análisis exploratorio, en la región Southern Metropolitan se encuentran una gran cantidad de casas "caras", mientras que en otras regiones no aparece ninguna y no van a ser relevantes para el modelo. 

En vista a estos resultados, se va a probar un modelo con muchas menos variables, y poder así comparar con el modelo más complejo:

```{r}
lr_regularized <- readRDS("./models/lr_regularized.rds")
#lr_regularized <- glm(price_label_high ~. -sell_rate_cat - bed_cat - car_cat - kmeans_cluster - may_have_water - lattitude_std, family = binomial(link = 'logit'), data = train)
summary(lr_regularized)
```


```{r}
prob_lr_regularized <- lr_regularized %>% predict(housesTrain_lr, type = "response")
opt_f1_lr_regularized <- opt_f1_function_v2(prob_lr_regularized, housesTrain_lr, "price_label_high")
c(optimal_thrshold = opt_f1_lr_regularized$threshold, precision = opt_f1_lr_regularized$precision, recall =  opt_f1_lr_regularized$recall, f1_score = opt_f1_lr_regularized$f1_opt)
```

Se puede ver que la f1_score no se ve especialmente perjudicada pasando de 13 a 7 variables. A continuación se muestra por un lado la curva PR (Precision-Recall) junto con el área bajo la misma, y por otro, una comparativa de Precision, Recall y F1 score para los diferentes thresholds:

```{r}
opt_f1_lr_regularized$p1
opt_f1_lr_regularized$p2
```
Como se puede comprobar, sobre todo a partir del área bajo la curva PR, que los resultados obtenidos por este modelo no son demasiado favorables. A través de los sigbuietes gráficos de distribuciondes de las predicciones del modelo, podemos corroborar también que la precisión del modelo no es muy buena, ya que aparecen bastantes falsos positivos más alla del threshold:

```{r}
predictions <- data.frame(price_label_high=housesTrain_lr$price_label_high, pred=prob_lr_regularized)
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_lr_regularized$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_lr_regularized$threshold, linetype='dashed', color=palette34[4])
```

A continuación, se procede a medir la performance del modelo sobre los datos pertencientes al conjunto de validación:

```{r}
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label', "Method", "log_landsize_std", "rooms_cat")
housesVal_lr <- housesVal %>% select(-filter_cols)
housesVal_lr$may_have_water <- factor(housesVal_lr$may_have_water, levels = c(TRUE,FALSE))
prob_values_test <- lr_regularized %>% predict(housesVal_lr, type = "response")
preds_validation_lr <- as.factor(ifelse(prob_values_test > opt_f1_lr_regularized$threshold ,1,0))


housesVal_lr$price_label_high <- as.factor(ifelse(housesVal_lr$price_label_high==TRUE,1,0))
metrics_function_num(preds_validation_lr, housesVal_lr, 'price_label_high')
```

Como podemos ver, al simpolificar el modelo se ha conseguido que no sobreajuste en absoluto a los datos de entrenamiento. A pesar de esto, tal y como ya se ha comentado, los resultados obtenidos con este algoritmo no son excesivamente satisfactorios.

## Árboles de decisión
Se van a probar árboles de decisión sobre nuestro conjunto de datos. En primer lugar, se utilizarán las variables que se consideraron importantes en el análisis exploratorio, para después probar distintas formas de tuning e intentar evitar el sobreaprendizaje del árbol.

### Utilizando las variables estudiadas
Se utilizarán directamente las variables estudiadas en el análisis exploratorio, dejando de lado transformaciones y estandarizaciones, además de variables categóricas con muchas categorías. Además, si recordamos, para la regresión lineal tuvimos que convertir varias features numéricas en categóricas (número de habitaciones, año de construcción...). Podemos probar a introducir las variables originales para ver qué cortes propone el árbol, y ahorrarnos problemas con variables dummy.


En los cortes, se mira primero la región en la que está la casa, viendo si está o no en Southern Metropolitan (la zona con casas más caras). Después, utiliza variables como el número de habitaciones, el tipo de vivienda y la distancia para discriminar. Se ve que hay un grupo que contiene a cerca de la mitad de las muestras con un 9% de probabilidades de ser cara, solo teniendo en cuenta que no está en Southern Metropolitan y que tiene menos de 4 habitaciones.

Algo que también se puede ver es que hay varios nodos con pocas muestras (los que vienen de la distinción de tipo de casa es Southern Metropolitan). Vemos además que hay un bastantes falsos negativos (casas caras que se etiquetan como baratas).



```{r message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat", "may_have_water", "kmeans_cluster_woe"), "price_label_high")
model_metrics$metrics
model_metrics$dt
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Observamos que el árbol solo discrimina por las variables *Rooms*, *Distance*, *kmeans_cluster_woe*, *Regionname*. Si probamos un árbol con solo estas variables, obtenemos un modelo muy parecido:

```{r, message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Distance", "Regionname", "kmeans_cluster_woe"),"price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Sin embargo, se ha visto que utilizando la variable *Type* en vez de la variable generada por el cluster, se obtiene un modelo con una f1 ligeramente menor, pero que parece un poco más interpretable.
```{r, message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Distance", "Regionname", "Type"),"price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Al ver que el árbol se fija en la localización, se prueba a entrenar un modelo solo con el número de habitaciones y las coordenadas, que presumiblemente dan información más detallada, además del tipo de casa. Se intenta evitar meter variables que pueden estar relaciones entre sí, ya que el árbol solo elegiría una de ellas para discriminar. Observamos que el f1 score mejora: por un lado, el recall ha aumentado (devolvemos más casas caras) pero el precision ha empeorado (más falsos positivos, el modelo dice que casas que no son caras lo son). Además, si comparamos ambas curvas ROC, vemos que este modelo tiene un AUC mejor (distingue mejor entre clases). 

Por otro lado, el árbol, aunque ofrece mejores métricas, resulta ser menos interpretable, ya que la mayoría de los cortes se basa en valores numéricos de las coordenadas.

```{r message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Lattitude", "Longtitude", "Type"), "price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
```


Se puede realizar la misma prueba con las variables transformadas: tomando **rooms_cat** en vez de **Rooms**. Se obtienen las mismas métricas, lo que nos lleva a pensar que se realizó una buena separación a la hora de categorizar el número de habitaciones.
```{r message=FALSE}
model_metrics <- dt_metrics(housesTrain, c("rooms_cat", "Lattitude", "Longtitude", "Type"), "price_label_high")
model_metrics$metrics
prob_values <- predict(model_metrics$dt, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
```


En ambos casos, se puede ver que hay nodos con muy pocos casos (1%,2%...) que probablemente se puedan juntar en otro nodo sin perder poder predictivo. 

### Podando el árbol 
Intentamos eliminar esas últimas hojas que hemos comentado en el último modelo probado.

```{r}
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Lattitude", "Longtitude", "Type"), "price_label_high")
plotcp(model_metrics$dt)
```

Con 11 hojas, lel f1 baja hasta 0.726.

```{r pruneTree2, message=FALSE}
pruneTREE <- rpart::prune(model_metrics$dt, cp = model_metrics$dt$cptable[5, "CP"])
rpart.plot(pruneTREE, type=3, fallen.leaves = FALSE, roundint = FALSE)
preds <- predict(pruneTREE, type = "class")
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
prob_values <- predict(pruneTREE, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```


Bajando la complejidad hasta quedarnos solo con 5 hojas, observamos que el accuracy es parecido, pero que el f1-score disminuye hasta 0.56. Por otro lado, el árbol es mucho más fácil de interpretar. Las casas con menos de 4 habitaciones directamente son una hoja con las casas baratas (83% de las casas de este nodo son baratas), con el 75% de los datos. Las casas con más de 4 habitaciones en se distribuyen en función de las coordenadas.

```{r message=FALSE}
pruneTREE <- rpart::prune(model_metrics$dt, cp = model_metrics$dt$cptable[4, "CP"])
rpart.plot(pruneTREE, type=3, fallen.leaves = FALSE, roundint = FALSE)
preds <- predict(pruneTREE, type = "class")
metrics_function(preds, housesTrain, 'price_label_high', bool=TRUE)
prob_values <- predict(pruneTREE, housesTrain, type='prob')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
```

### Cross validation
Utilizaremos las variables que mejor resultados nos han dado en una etapa de validación cruzada para obtener el mejor parámetro de complejidad.


```{r}
set.seed(10)
housesTrainCV <- housesTrain %>% select("Rooms", "Lattitude", "Longtitude", "kmeans_cluster_woe", "price_label_high")
housesTrainCV$price_label_high <- as.factor(housesTrainCV$price_label_high)
dt_model <- decision_tree(cost_complexity = tune()) %>% set_mode('classification') %>% set_engine("rpart")
houses_rec <- recipe(price_label_high ~ ., data=housesTrainCV)
# Create workflow
dt_workflow <- workflow() %>% add_model(dt_model) %>% add_recipe(houses_rec)
dt_param <- dt_workflow %>% parameters()
# Create grid
dt_grid <- grid_regular(dt_param, levels=10)
```


```{r}
# Este chunk solo hace el cv cuando el fichero no se encuentra
path = "./models/dt_bayes_search_woe.rds"
houses_folds <- vfold_cv(housesTrainCV, v = 10, repeats = 3)
dt_bayes_search <- try_bayes_cv_train(path, dt_workflow, houses_folds, dt_param)
```


Fijándonos en la gráfica, quizás querramos sacrificar un poco de roc a costa de que nuestro árbol sea menos complejo. Viendo el resultado del mejor modelo, es un árbol con muchos nodos, que quizás esté sobre ajustando.

```{r}
autoplot(dt_bayes_search, type='marginals')
autoplot(dt_bayes_search, type='performance')
autoplot(dt_bayes_search, type='parameters')
```

```{r}
show_best(dt_bayes_search, metric='roc_auc', 20)
```


```{r, message=FALSE}
train_summary <- finish_workflow_train(housesTrainCV, dt_bayes_search, dt_workflow)
train_summary$metrics
train_summary$roc_plot
train_summary$opt_f1_obj
```


En validación, observamos que el modelo sobreajusta en términos de f1. Pasamos de una f1 de 0.835 en train a 0.786 en validación. Esto puede deberse a que en la etapa de validación cruzada se está optimizando el AUC ROC (que vemos que cae de 0.95 a 0.93) mientras que el AUC PR sí que es sensiblemente peor. Habría que intentar optimizar esta métrica.

Además, quizás se esté permitiendo al árbol tener demasiados nodos. Podemos probar a quedarnos con un parámetro de complejidad un poco mayor (reduciendo el número de nodos).


```{r message=FALSE}
eval_val <- evaluate_validation(housesVal, train_summary$wflow_final, train_summary$opt_f1_obj$threshold)
eval_val
```



Como prueba final, elegimos el parámetro de complejidad cp=0.001033845, y probamos sobre train y validación para ver si se produce sobreajuste.

```{r}
cp=0.001033845
model_metrics <- dt_metrics(housesTrain, c("Rooms", "Lattitude", "Longtitude", "Type"), "price_label_high")
#model_metrics$metrics
pruneTREE <- rpart::prune(model_metrics$dt, cp = cp)
preds_class <- predict(pruneTREE, type = "class")
metrics_function(preds_class, housesTrain, 'price_label_high', bool=TRUE)
preds <- predict(pruneTREE, type = "prob")
opt_f1 <- opt_f1_function_v2(preds[,2], housesTrain, 'price_label_high')
opt_th_dt <- opt_f1$threshold
opt_f1$f1_opt
opt_f1$p2
```

Sobre validación, observamos que las métricas empeoran un poco, aunque en este caso no se produce sobre ajuste. Se opta por sacrificar un poco de f1 para que el modelo no sobre ajuste y además sea más sencillo

```{r message=FALSE}
val_dt <- housesVal
preds_dt_val <- predict(pruneTREE, val_dt, type='prob')[,2]
preds_dt_val <- as.factor(ifelse(preds_dt_val > opt_th_dt,1,0))
val_dt$price_label_high <- factor(ifelse(val_dt$price_label_high==TRUE,1,0))
metrics_function_num(preds_dt_val, val_dt, 'price_label_high')
```


```{r}
#saveRDS(pruneTREE, "./models/dt_best_model.rds")
```


A modo de nota final, se han probado las variables generadas (kmeans, agua y habitaciones/landsize) sin ningún efecto a aparente en estos modelos.


## Random Forest

### Selección de variables
En este apartado se van a utilizar random forests para el problema de clasificación. Al no estar demasiado expuesto a los outliers, y aceptar variables categóricas como entrada, la primera prueba que se realizará será con la mayoría de variables del conjunto de datos, y con unos parámetros iniciales que después se modificarán en una etapa de selección de hiperparámetros. Se busca en esta primera prueba un modelo con muchos árboles, de tal forma que se pueda ver la importancia de las variables, y pueda servir de etapa previa de selección de las mismas. Aquí, hay que tener varias cosas en cuenta:

- Si se introducen variables muy correladas, solo una tendrá mucha importancia, y el efecto de la otra quedará "enmascarado". Esto puede afectar a la interpretación de los datos.
- Los random forest tienden a estar sesgados hacia variables categóricas con muchas categorías. Se introducirán por tanto la mayoría de variables que se convirtieron a categorías como numéricas.


```{r}
set.seed(10)
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat", "kmeans_cluster_woe",
                                     "price_label_high")]
housesTrainOrigVar$price_label_high <- as.factor(housesTrainOrigVar$price_label_high)
rf2 <- randomForest(price_label_high ~ ., data=housesTrainOrigVar, ntree=500)
```

Observamos que ya de primeras se obtienen buenas métricas.

```{r message=FALSE}
preds <- predict(rf2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
prob_values <- rf2$votes
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
```


Si observamos la importancia de las variables del anterior (medida como la caída media en el índice gini que produce cada variable), vemos que las más importantes son aquellas relacionadas con la situación geográfica de la vivienda (cómo ya se había visto anteriormente): coordenadas, distancia al centro y región están en las 6 primeras posiciones, además de variables que informan sobre el tamaño de la vivienda (número de habitaciones y tamaño de la parcela). 


```{r}
# Extracts variable importance (Mean Decrease in Gini Index)
# Sorts by variable importance and relevels factors to match ordering
var_importance <- data_frame(variable=setdiff(colnames(housesTrainOrigVar), "price_label_high"),
                             importance=as.vector(importance(rf2)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)
p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p+ scale_fill_manual(values = palette34, name = "Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```

Podemos probar si seleccionado las variables más importantes y volviendo a entrenar un modelo con los mismos parámetros, obtenemos resultados parecidos.

Si nos quedamos con las 6 primeras, el accuracy desciende ligeramente hasta 0.89, pero el f1 score baja hasta 0.77. Si se incluye la variable *Type* adicionalmente, se consiguen resultados muy parecidos a los del primer modelo, reduciendo la complejidad. Si además quitamos la variable *Regionname*, por tener bastantes categorías, conseguimos un f1-score de 0.8.


Otras pruebas:
- Rooms por rooms_cat: de nuevo no influye demasiado
- Variables numéricas transformadas estandarizadas: el resultado es un poco mejor, pero no afecta demasiado, y es más interpretable tener las variables originales sin transformaciones (mejor f1 de 0.8, auc 0.96->0.955)
- Variables nuevas: no ayudan a discriminar mejor.

```{r}
set.seed(10)
housesTrainImpVars <- housesTrain[c("Rooms", "Distance", "LandsizeImp", "Type",
                                    "Lattitude", "Longtitude",
                                     "price_label_high")]
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
rf3 <- randomForest(price_label_high ~ ., data=housesTrainImpVars, ntree=500)
```


```{r message=FALSE}
preds <- predict(rf3, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high', bool=TRUE)
prob_values <- rf3$votes
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
```

```{r}
var_importance <- data_frame(variable=setdiff(colnames(housesTrainImpVars), "price_label_high"),
                             importance=as.vector(importance(rf3)))
var_importance <- arrange(var_importance, desc(importance))
var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)
p <- ggplot(var_importance, aes(x=variable, weight=importance, fill=variable))
p <- p + geom_bar() + ggtitle("Importancia de variables")
p <- p + xlab("Features") + ylab("Decrecimiento medio índice Gini")
p <- p + scale_fill_manual(values = palette34, name="Variable")
p + theme(axis.text.x=element_blank(),
          axis.text.y=element_text(size=12),
          axis.title=element_text(size=16),
          plot.title=element_text(size=18),
          legend.title=element_text(size=16),
          legend.text=element_text(size=12))
```



## Utilizando cross validation
Como primera prueba, y teniendo en cuenta que las variables importantes el modelo son bastante lógicas, los resultados de este modelo son bastante buenos. Ahora se va a intentar tunear el número de árboles y la profundidad máxima de los mismos con grid search y CV, para intentar obtener unos mejores resultados, además de evaluar el modelo con conjuntos de validación. Los parámetros a tunear son:

- Número de árboles
- Número de variables que se muestrean aleatoriamente al crear los árboles
- Mínimo número de puntos para que un nodo se divida.

Realizamos 10 Fold cross validation con 3 iteraciones.

Se utilizan las variables que se han comentado en el apartado anterior.


```{r}
set.seed(10)
rf_model <- rand_forest(trees=tune(), mtry=tune(), min_n=tune()) %>% set_mode('classification') %>% set_engine('randomForest')
houses_rec <- recipe(price_label_high ~ ., data=housesTrainImpVars)
# Create workflow
rf_workflow <- workflow() %>% add_model(rf_model) %>% add_recipe(houses_rec)
rf_param <- rf_workflow %>% parameters() %>% update(trees = trees(c(50,500)), mtry=mtry(c(1,5)), min_n=min_n(c(2,20)))
# Create grid
rf_grid <- grid_regular(rf_param, levels=3)
```


```{r}
# Este chunk solo hace el cv cuando el fichero no se encuentra
path = "./models/rf_grid_search.rds"
houses_folds <- vfold_cv(housesTrainImpVars, v = 10, repeats = 3)
#rf_search <- tune_grid(rf_workflow, grid = rf_grid, resamples=houses_folds, param_info = rf_param)
rf_search <- try_cv_train(path, rf_workflow, houses_folds, rf_grid, rf_param)
```


```{r}
autoplot(rf_search) +
    labs(title = "Results of Grid Search for Two Tuning Parameters of a Random Forest")
```

```{r}
show_best(rf_search, metric='roc_auc', 10)
```
Observamos que se consiguen buenos resultados con muchas combinaciones de los parámetros. Podemos reducir la complejidad a costa de bajar un poco la métrica de clasificación.
Para el mejor modelo, se obtienen los siguientes resultados en train y validación:

Train:

```{r, message=FALSE}
train_summary <- finish_workflow_train(housesTrainImpVars, rf_search, rf_workflow)
train_summary$metrics
train_summary$roc_plot
train_summary$opt_f1_obj
```



En validación, observamos que el modelo sobreajusta bastante, sobretodo en términos de f1. Pasamos de una f1 de 0.92 en train a 0.81 en validación, solo un poco mejor de lo obtenido anteriormente con parámetros por defecto. Esto puede deberse a que en la etapa de validación cruzada se está optimizando el AUC ROC (que vemos que cae de 0.99 a 0.96) mientras que el AUC PR sí que es sensiblemente peor. Habría que intentar optimizar esta métrica.


```{r message=FALSE}
eval_val <- evaluate_validation(housesVal, train_summary$wflow_final, train_summary$opt_f1_obj$threshold)
eval_val
```


### Optimización bayesiana de hiperparámetros


```{r}
set.seed(10)
housesTrainImpVars <- housesTrain[c("Rooms", "Distance", "LandsizeImp",
                                    "Lattitude", "Longtitude", "Type",
                                     "price_label_high")]
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
rf_model <- rand_forest(trees=tune(), mtry=tune(), min_n=tune()) %>% set_mode('classification') %>% set_engine('randomForest')
houses_rec <- recipe(price_label_high ~ ., data=housesTrainImpVars)
# Create workflow
rf_workflow <- workflow() %>% add_model(rf_model) %>% add_recipe(houses_rec)
# mtry en función del número de features
rf_param <- rf_workflow %>% parameters() %>% update(trees = trees(c(50,500)), mtry=mtry(c(1,6)), min_n=min_n(c(2,30)))
```


```{r, echo=F}
set.seed(10)
path = "./models/rf_bayes_search.rds"
rf_bayes_folds <- vfold_cv(housesTrainImpVars, v = 10, repeats = 1)
rf_bayes_search <- try_bayes_cv_train(path, rf_workflow, rf_bayes_folds, rf_param)
```

```{r}
show_best(rf_bayes_search, metric = "roc_auc", 10)
```


```{r}
autoplot(rf_bayes_search, type='performance')
autoplot(rf_bayes_search, type='parameters')
```

Para el mejor modelo, se obtienen los siguientes resultados en train y validación:

TODO 12/05 METER ESTO EN UNA FUNCION PARA EVITAR POSIBLES ERRORES


Train: observamos que se consiguen mejores resultados que con el grid search (con un f1 de casi 0.94)

```{r message=FALSE}
train_summary <- finish_workflow_train(housesTrainImpVars, rf_bayes_search, rf_workflow)
train_summary$metrics
train_summary$roc_plot
train_summary$opt_f1_obj
```


De nuevo, vemos que el modelo sobreajusta en términos de f1 (de 0.93 a 0.809 en validación).

```{r message=FALSE}
eval_val <- evaluate_validation(housesVal, train_summary$wflow_final, train_summary$opt_f1_obj$threshold)
eval_val
```


### Modelo final

Probamos un modelo con unos parámetros más simples (de los que aparecen en el top10 del cv de la búsqueda bayesiana):

```{r}
set.seed(10)
housesTrainImpVars <- housesTrain[c("Rooms", "Distance", "LandsizeImp",
                                    "Lattitude", "Longtitude", "Type",
                                     "price_label_high")]
housesTrainImpVars$price_label_high <- as.factor(housesTrainImpVars$price_label_high)
#rf4 <- randomForest(price_label_high ~ ., data=housesTrainImpVars, ntree=50, mtry=3, nodesize=11)
rf4 <- randomForest(price_label_high ~ ., data=housesTrainImpVars, ntree=80, mtry=4, nodesize=15)
```

En train:

```{r message=FALSE}
preds <- predict(rf4, type = "class")
table(pred = preds, obs = housesTrainImpVars$price_label_high)
metrics_function(preds, housesTrainImpVars, 'price_label_high', bool=TRUE)
prob_values <- rf4$votes
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)
opt_th_metrics <- opt_f1_function_v2(prob_values[,2], housesTrain, 'price_label_high')
opt_th_metrics$p1
opt_th_metrics$f1_opt
opt_th_rf <- opt_th_metrics$threshold
```

En validación, vemos como ya no se produce ese sobre ajuste.

```{r message=FALSE}
val_rf <- housesVal
preds_rf_val <- predict(rf4, val_rf, type='prob')[,2]
#prob_svm_val <- predict(best_svm_gaussian, new_data = validation_svm, type = "prob")$.pred_1
preds_rf_val <- as.factor(ifelse(preds_rf_val > opt_th_rf,1,0))
val_rf$price_label_high <- factor(ifelse(val_rf$price_label_high==TRUE,1,0))
metrics_function_num(preds_rf_val, val_rf, 'price_label_high')
```


```{r}
prob_values <- predict(rf4, housesVal, type='prob')
predictions <- data.frame(price_label_high=housesVal$price_label_high, pred=prob_values[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_th_rf, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_th_rf, linetype='dashed', color=palette34[4])
```

```{r}
#saveRDS(rf4, "./models/rf_best_model.rds")
```

## kNN
El siguiente algoritmo que se va a probar es kNN. Para ello, necesitamos que los datos estén de nuevo estandarizados, ya que es un método basado en distancias, y las variables necesitan estar en el mismo rango de valores. Es importante que no se utilicen muchas variables en este método, ya que al estar basado en distancias, puede sufrir de la "maldición de la dimensionalidad".


```{r}
housesTrain <- read.csv('base_train.csv')
# Para que funcione confusionMatrix de caret, el target debe ser un factor
housesTrain$price_label_high <- as.factor(housesTrain$price_label_high)
```

En primer lugar, se probarán las variables numéricas con las que se trabajó en el anterior informe: distancia al centro, coordenadas y tamaño de la parcela. Se utiliza un k=3 para esta primera prueba. Para evaluar los modelos, se utiliza la función knn.cv, que utiliza leave one out cross validation. Para cada fila del dataset de entrenamiento, se utilizan los K vecinos más cercanos para decidir la clase.

Se obtiene un recall bastante bajo, lo que hace que el f1 score sea mediocre. Para intentar mejorar el modelo, se intentará elegir el valor óptimo de k.

```{r knn1}
num_std_cols <- c('sqrt_distance_std', 'log_landsize_std', 'lattitude_std', 'longtitude_std')
pred_knn <- knn.cv(housesTrain[,num_std_cols], k=3, cl=housesTrain[,"price_label_high"], prob = T)
probs_knn <- reverse_probs(pred_knn)
opt_f1_knn <- opt_f1_function(probs_knn, housesTrain, "price_label_high")
c(optimal_threshold = opt_f1_knn$threshold, precision = opt_f1_knn$precision, recall =  opt_f1_knn$recall, f1_score = opt_f1_knn$f1_opt)
```

### k óptimo
Se muestra en la gráfica los distintos valores de F1-score que se obtendrían cambiando los valores de k. Se observa que el f1 score es bastante bajo, dejando de mejorar a partir de k=5 o k=7.

```{r}
plot_acc_f1_k_v2(housesTrain, 'price_label_high', num_std_cols)
```


Si probamos con k=7, los resultados mejoran ligeramente, pero el recall sigue siendo bastante flojo (hay muchos FP).


```{r knn7high}
#knn_metrics <- perform_knn_cv(housesTrain, 'price_label_high', 7, num_std_cols)
#knn_metrics
pred_knn_7 <- knn.cv(housesTrain[,num_std_cols], k=7, cl=housesTrain[,"price_label_high"], prob = T)
probs_knn_7 <- reverse_probs(pred_knn_7)
opt_f1_knn_7 <- opt_f1_function(probs_knn_7, housesTrain, "price_label_high")
c(optimal_threshold = opt_f1_knn_7 $threshold, precision = opt_f1_knn_7 $precision, recall =  opt_f1_knn_7 $recall, f1_score = opt_f1_knn_7 $f1_opt)
```

### Usando otras variables
Hasta ahora, se han utilizado las 4 variables numéricas que teníamos identificadas. Se puede probar a crear otras variables adicionales como combinaciones de ordinales y numéricas. Por ejemplo, habitaciones por unidad de parcela.

```{r}
housesTrain <- housesTrain %>% mutate(log_room_land = log10(Rooms/LandsizeImp*1000))
housesTrain$log_room_land_std <- housesTrain %>% select(log_room_land) %>% scale()
housesTrain %>% ggplot(aes(y=log_room_land_std)) + geom_boxplot()
housesTrain %>% ggplot(aes(x=log_room_land_std)) + geom_histogram()

p<-housesTrain %>%
  select(log_room_land, price_label_high) %>%
  ggpairs(ggplot2::aes(colour=price_label_high))

for(i in 1:p$nrow) {
  for(j in 1:p$ncol){
    p[i,j] <- p[i,j] + 
        scale_fill_manual(values= palette34) +
        scale_color_manual(values= palette34)  
  }
}

p
```

```{r}
summary(aov(price_label_high~log_room_land, data=housesTrain))
```

```{r}
num_std_cols <- c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'log_room_land_std')
plot_acc_f1_k_v2(housesTrain, 'price_label_high', num_std_cols)
```

Con esta nueva variable, podemos elegir un k=3, consiguiendo un f1 de 0.75, y además eliminar una variable (**sqrt_distance_std**).

```{r}
pred_knn_3 <- knn.cv(housesTrain[,num_std_cols], k=3, cl=housesTrain[,"price_label_high"], prob = T)
probs_knn_3 <- reverse_probs(pred_knn_3)
opt_f1_knn_3 <- opt_f1_function(probs_knn_3, housesTrain, "price_label_high")
c(optimal_threshold = opt_f1_knn_3$threshold, precision = opt_f1_knn_3$precision, recall =  opt_f1_knn_3$recall, f1_score = opt_f1_knn_3$f1_opt)
```

El resto de variables son categóricas, pero algunas además son ordinales, por lo que tendría sentido aplicar distancias utilizando también esas variables.


```{r}
housesTrainKnn <- housesTrain %>% dplyr::select(sqrt_distance_std, log_room_land, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)


#las variables categóricas pasan a ser factores ordenados
housesTrainKnn['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric()

housesTrainKnn['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()

housesTrainKnn['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric()

housesTrainKnn['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric()

housesTrainKnn['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric()

# Norm
normParam <- preProcess(housesTrainKnn)
housesTrainKnnNorm <- predict(normParam, housesTrainKnn)

housesTrainKnnNorm$price_label_high <- as.factor(housesTrain$price_label_high)
```

Observamos que con estas variables no conseguimos incrementar el f1 score, y que además a partir de k=3 se mantendrian constantes:

```{r}
std_cols <- c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'rooms_cat', 'car_cat', 'sell_rate_cat')
plot_acc_f1_k_v2(housesTrainKnnNorm, 'price_label_high', std_cols)
```


Introduciendo todas las variables, el modelo no consigue mejorar nada apenas. La combinación que ofrece mejores resultados es el uso del tamaño de la parcela, coordenadas, habitaciones, plazas de aparcamiento y popularidad del barrio:

```{r}
pred_knn_ordinals <- knn.cv(housesTrainKnnNorm[,std_cols], k=3, cl=housesTrainKnnNorm[,"price_label_high"], prob = T)
probs_knn_ordinals <- reverse_probs(pred_knn_ordinals)
opt_f1_knn_ordinals <- opt_f1_function(probs_knn_ordinals, housesTrain, "price_label_high")
c(optimal_threshold = opt_f1_knn_ordinals$threshold, precision = opt_f1_knn_ordinals$precision, recall =  opt_f1_knn_ordinals$recall, f1_score = opt_f1_knn_ordinals$f1_opt)

```

Por último, vamos a probar el mejor modelo que hemos obtenido sobre nuestro conjunto de validación, con el objetivo de comprobar su funcionamiento:

```{r}
#housesVal <- read.csv('base_val.csv')
# Para que funcione confusionMatrix de caret, el target debe ser un factor
#housesVal$price_label_high <- as.factor(housesVal$price_label_high)
housesValKnn <- housesVal

num_std_cols <- c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'log_room_land_std')

pred_knn_val <- knn(train = housesTrain[,num_std_cols], test = housesValKnn[,num_std_cols], cl=housesTrain[,"price_label_high"], k = 3,prob = T)
final_preds_val <- rep(0, dim(housesValKnn)[1])
probs_knn_val <- reverse_probs(pred_knn_val)
# Para hacer las predcciones utilizando el threshold óptimo
for(i in 1:length(probs_knn_val)){
  
  if(probs_knn_val[i] >= opt_f1_knn_3$threshold){
    final_preds_val[i] = T
  }else{final_preds_val[i] = F}
}

housesValKnn$price_label_high <- as.factor(ifelse(housesValKnn$price_label_high==T,1,0))
metrics_function_num(factor(final_preds_val), housesValKnn, "price_label_high")
```

##SVM

A continuación se va aplicar el algoritmo de SVM (con diferentes hiperparámetros) sobre nuestros datos. Este algoritmo se fundamenta en la idea de encontrar un hiperplano óptimo que separe perfectamente los puntos. Además, para la resolución de este problema se basa en el teorema de Cover, el cual afirma que para cualquier dataset, la separación lineal de las clases se hace más latente a medida que las dimensiones aumentan. 

Para mapear los datos de entrada a un espacios de mayores dimensiones, se hace uso de un tipo de funciones llamadas funciones de kernel. Los kernels pueden ser de 3 tipos:

- Lineales.
- Polinómicos.
- Gaussianos (basados en funciones de base radial).

Lo adecuado que va a ser utilizar un tipo de kernel sobre otro va a estar totalmente determinado por el tipo de datos. En espacios con baja dimensionalidad (como mucho hasta 3 dimensiones y ni siquiera), resulta interesante darle un primer vistazo a cómo se reparten las clases en el espacio, para principalemente observar si son linearmente separables o no. 

En nuestro conjunto de datos no contamos con esa posibilidad, debido a la alta dimensionalidad del mismo. Lo único que se puede hacer, es aplicar de nuevo alguna técnica de dimensionalidad (PCA), por si se pudiese intuir algo en dos dimensiones:


```{r, echo=F}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, log_room_land_std)
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label', 'sell_rate_cat', "may_have_water", "bed_cat", "car_cat","year_built_cat", "Regionname", "Type", "Method", "kmeans_cluster", "log_room_land_std")
housesTrainSVM <- housesTrain %>%  select(-filter_cols)

# Estandarización de variables categóricas ordinales, quitando sell_rate_cat, bed_cat y car_cat que en principio no aporta nada al modelo (las dos últimas por ser redundantes)
cat_with_order = c("rooms_cat", "bath_cat")

housesTrainSVM['bath_cat'] <- housesTrainSVM %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() 
housesTrainSVM['rooms_cat']<- housesTrainSVM %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() 

ordinal_variables <- housesTrainSVM %>% select(cat_with_order) 
normParam <- preProcess(ordinal_variables)
ordinal_variables_norm <- predict(normParam, ordinal_variables)

for(cat in cat_with_order){
  housesTrainSVM[cat] <- ordinal_variables_norm[cat]
}


housesTrainSVM$price_label_high<-as.factor(ifelse(housesTrainSVM$price_label_high==TRUE,1,0))
    
```

```{r}
housesPCA <- prcomp(housesNum, center = TRUE, scale = TRUE)
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca$price_label <- train$price_label_high
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=housesTrain$price_label)) + geom_point() + scale_color_manual(values = palette34)
```

Con esta represntación podemos ver que claramente nuestros datos no son linearmente separables si los reducimos a dos dimensiones. Aun así, no se va a descartar la realización de pruebas utilizando un kernel lineal, ya que como se ha dicho nuestros datos no están en dos, sino en 9 dimensiones. 

Debido a que las SVM están basadas en el cálculo de distancias, las variables categóricas codificadas numéricamente primarían sobre el resto de variables numéricas, las cuales se encuentran estandarizadas. Lo que sí se ha hecho es codificar las variables categóricas que pueden estar ordenadas (por ejemplo el número de habitaciones o de plazas de garage) y después se han estandarizado para poder incluirlas en el modelo.

## Kernel Lineal 

Inicialmente se va a probar utilizando un kernel lineal para la SVM. En el caso de la utilización de un kernel lineal, aparece un hiperparámetro a utilizar, denominado parámetro de coste (C). Se van a probar diferentes valores de este parámetro:


```{r pressure, echo=FALSE}

# Tarda aproximadamente 7 minutos
svm_linear <- e1071::tune("svm", price_label_high ~ ., data = train,
               kernel = 'linear',
               ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))


summary(svm_linear)

```


Como se puede el menor error de clasificación utilizando este kernel lineal se obtiene para C=5. Sin embargo a partir de 0.1 apenas varia de forma significativa este error. Además, las SVM tienen un cierto caracter aleatorio, que hace que en otra ejecución el mejor modelo podría conseguirse con un C distinto. Aún así, vamos a seleccionar como este modelo como el mejor con kernel linear y obtener algunas métricas adicionales de rendimiento:

```{r, echo=F}
#best_linear <- svm(price_label_high~., data = train, kernel = "linear", cost = svm_linear$best.parameters$cost, probability = T)
#saveRDS(best_linear, "./models/svm_linear_kernel.rds")
```


```{r}
best_svm_linear <-  readRDS("./models/svm_linear_kernel.rds")
prob_train_linear <- best_svm_linear %>% predict(housesTrainSVM[,1:6], probability = T) 
prob_svm_linear <- attr(prob_train_linear, "probabilities")[,1] # La probabilidad de pertenecer a la clase "cara" (1)
opt_svm_linear <- opt_f1_function_v2(prob_svm_linear, housesTrainSVM, "price_label_high")
c(optimal_threshold = opt_svm_linear$threshold, precision = opt_svm_linear$precision, recall = opt_svm_linear$recall, f1_score = opt_svm_linear$f1_opt)
```


Se ha alcanzado una f1_score de 0.67 aproximadamente sobre el conjunto de entrenamiento utilizando la svm con kernel lineal. Sin embargo, se puede intuir que utilizando un tipo de kernel no lineal se podrían mejorar estos resultados. Para comprobarlo, se procede a probar el funcionamiento de una SVM haciendo uso de un kernel gaussiano. En este caso, además del parámetro de coste que ya conocíamos, aparece otro hiperparámetro que será necesario optimizar (gamma). Esta vez, para buscar los mejores hiperparámetros posibles para el modelo, se va a hacer una búsqueda bayesiana. Los resultados obtenidos se muestan a continuación:

```{r}
svm_mod <- svm_rbf(mode = "classification", cost = tune(), rbf_sigma = tune()) %>% set_engine("kernlab")
svm_rec <- recipe(price_label_high ~ ., data=housesTrainSVM)
svm_wflow <- workflow() %>% add_model(svm_mod) %>% add_recipe(svm_rec)
svm_param <- svm_wflow %>% parameters() #%>% update(cost = cost(range = c()), rbf_sigma = rbf_sigma(range = c()))
svm_param
```

```{r, echo=F}
#tiempò aproximado de ejecución: 15 minutos
set.seed(1291)

path = "./models/svm_bayes_search.rds"

svm_folds <- vfold_cv(housesTrainSVM, v = 10, repeats = 1)
svm_search <- try_bayes_cv_train(path, svm_wflow, svm_folds, svm_param)


show_best(svm_search, metric = "roc_auc")

```

En le anterior tabla se muestran los 5 mejores resultados obtenidos (en cuanto a área bajo la curva ROC). Para el cáculo de métricas y búsqueda del mejor threshold, no se va a utilizar el mejor modelo devuelto, sino que se va a hacer uso de la "one-standard error rule" propuesta por Breinman y seleccionar el modelo cuya área bajo la curva se encuentra a una desviación estándar de distancia del modelo con mejor auc. Con esto, estaremos intentando reducir en la medida de lo posbile el sobreajuste a los datos de entrenamiento: A continuación se muestran los resultados obtenidos tras el proceso de búsqueda del threshold óptimo para el modelo:


```{r, echo = F}
svm_param_best <- select_by_one_std_err(svm_search, rbf_sigma, cost, metric = "roc_auc")
svm_best <- finalize_workflow(svm_wflow, svm_param_best)
svm_best_fit <- fit(svm_best, data = housesTrainSVM)
#saveRDS(svm_best_fit, "./models/svm_gaussian_kernel.rds")
```


Una vez seleccionado el modelo, se pretende buscar el threshold óptimo. Se entiende por threshold óptimo, el que hace que el modelo alcanze la mayor f1-score. Los resultados han sido los siguientes:

```{r}
best_svm_gaussian <- readRDS("./models/svm_gaussian_kernel.rds")
prob_svm_gaussian <- predict(best_svm_gaussian, new_data = housesTrainSVM, type = "prob")$.pred_1
opt_svm_gaussian <- opt_f1_function_v2(prob_svm_gaussian, housesTrainSVM, "price_label_high")
c(optimal_threshold = opt_svm_gaussian$threshold, precision = opt_svm_gaussian$precision, recall = opt_svm_gaussian$recall, f1_score = opt_svm_gaussian$f1_opt)
```

La mejor f1 score obtenida con este Kernel es cercana a 0.82, bastante superior al caso del kernel lineal (como era de esperar). En el siguiente gráfico se muestran la curva PR (con su área sobreimpresionada): y la matriz de confusión del modelo sobre los datos de entrenamiento:

```{r}
opt_svm_gaussian$p1
```

A continuación se muestra un gráfico en el que se pueden ver la variación f1_score, la precision y el recall para diferentes thresholds. Además se muestra también con línea discontinua el threshol óptimo seleccionado:

```{r}
opt_svm_gaussian$p2
```

También se ha querido la distribución de las probabilidades predichas:

```{r}
predictions <- data.frame(price_label_high=housesTrainSVM$price_label_high, pred=prob_svm_gaussian)
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_svm_gaussian$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_svm_gaussian$threshold, linetype='dashed', color=palette34[4])
```

Cabe destacar que eL algoritmo de SVM, en un principio solo proporciona las etiquetas predichas. Sin embargo, de cara a la optimización de ciertas métricas puede ser útil obtener probabilidades. Para obtenerlas, se aplica una técnica llamada calibrado de Platt, que a grandes rasgos consiste en aplicar una regresión logística sobre las etiquetas calculadas por la SVM. Por lo tanto, al no estar diseñadas las SVM para proporcionar outputs probabilísticos, tampoco hay que darle mayor importancia a esta gráfica.

A continuación, se procede a validar el modelo utilizando el conjunto de validación:

```{r, echo=F}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, log_room_land_std)
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label', 'sell_rate_cat', "may_have_water", "bed_cat", "car_cat","year_built_cat", "Regionname", "Type", "Method", "kmeans_cluster", "log_room_land_std")
housesValSVM <- housesVal %>%  select(-filter_cols)

# Estandarización de variables categóricas ordinales, quitando sell_rate_cat, bed_cat y car_cat que en principio no aporta nada al modelo (las dos últimas por ser redundantes)
cat_with_order = c("rooms_cat", "bath_cat")

housesValSVM['bath_cat'] <- housesValSVM %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric() 
housesValSVM['rooms_cat']<- housesValSVM %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric() 

ordinal_variables <- housesValSVM %>% select(cat_with_order) 
#normParam <- preProcess(ordinal_variables)
ordinal_variables_norm <- predict(normParam, ordinal_variables)

for(cat in cat_with_order){
  housesValSVM[cat] <- ordinal_variables_norm[cat]
}


housesValSVM$price_label_high<-as.factor(ifelse(housesValSVM$price_label_high==TRUE,1,0))
    
```



```{r}
prob_svm_val <- predict(best_svm_gaussian, new_data = housesValSVM, type = "prob")$.pred_1
preds_svm_val <- as.factor(ifelse(prob_svm_val > opt_svm_gaussian$threshold,1,0))
metrics_function_num(preds_svm_val, housesValSVM, 'price_label_high')
```


## Naive Bayes

Como simplificación de redes bayesianas, vamos a entrenar un modelo utilizando Naive Bayes.

### Primer modelo

Como primera aproximación, utilizaremos las mismas variables utilizadas en el modelo de regresión lineal múltiple (aunque el escalado debería ser indiferente). Para variables numéricas asume una distribución gausiana para calcular la verosimilitud (likelihood). Lo que saca el mod es la media en la primera columna y la varianza en la segunda para las distribuciones condicionadas.

```{r}
mod_1 <- naiveBayes(price_label_high ~ sqrt_distance_std + log_landsize_std + lattitude_std + longtitude_std + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data=housesTrain)
```

Métricas en train. El f1 es algo bajo, ya que ni el recall ni el precision son demasiado buenos. Sin embargo, el AUC ROC es bastante bueno (0.9)

```{r, warning=F}
prob_values <- predict(mod_1, housesTrain, type='raw')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)

opt_f1_nv1<- opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
opt_f1_nv1$f1_opt
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=prob_values[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_nv1$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_nv1$threshold, linetype='dashed', color=palette34[4])
```

Probamos a reducir el número de variables, utilizando por ejemplo las variables más importantes seleccionadas por el random forest y la variable extrída del clustering kmeans. El modelo empeora ligeramente.

```{r}
mod_best <- naiveBayes(price_label_high ~ sqrt_distance_std + Regionname + Type + rooms_cat + kmeans_cluster, data=housesTrain)
#saveRDS(mod_best, "./models/nb_best_model.rds")
```


```{r, warning=F}
mod_best <- readRDS("./models/nb_best_model.rds")
prob_values <- predict(mod_best, housesTrain, type='raw')
roc_dt <- roc(housesTrain$price_label_high, prob_values[,2])
ggplotROCCurve(roc_dt)

opt_f1_nv<- opt_f1_function_v2(prob_values, housesTrain, 'price_label_high')
opt_f1_nv$f1_opt
predictions <- data.frame(price_label_high=housesTrain$price_label_high, pred=prob_values[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=opt_f1_nv$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=opt_f1_nv$threshold, linetype='dashed', color=palette34[4])
```

Probando algún modelo con menos variables. A modo de resumen:

- Utilizando solo las categóricas, los resultados empeoran, con un accuracy de 0.8079153, un f1 de 0.6207523 y una auc roc de 0.8552.
- Utilizando únicamente las numéricas, el resultado lógicamente empeora, ya que Naive Bayes asume una distribución normal de las entradas numéricas, y trabaja mejor con categóricas al calcular las probabilidades de una forma más precisa conociendo las distribuciones. En este caso, tenemos un accuracy de 0.7727566, un f1 de 0.4759075 y una auc roc de 0.7913

El modelo elegido como el mejor aquel que utiliza las variables sqrt_distance_std + Regionname + Type + rooms_cat + kmeans_cluster. No presenta el mejor f1 pero no se aleja mucho de los mejores según esta métrica y emplea menos variables.

### Validación

Ahora probamos el modelo con menos variables sobre los datos de validación. El F1 obtenido es ligeramente inferior al obtenido en train (0,68) y una auc roc de 0,89.

Finalmente, buscamos el corte óptimo con el mejor modelo.

```{r message=FALSE}
validation_nv <- housesVal
validation_nv$kmeans_cluster <- factor(validation_nv$kmeans_cluster, levels = c(1,2,3,4,5))

preds_nv_val <- predict(mod_best, validation_nv, type='raw')[,2]
preds_nv_val <- as.factor(ifelse(preds_nv_val > opt_f1_nv$threshold ,1,0))
validation_nv$price_label_high <- factor(ifelse(validation_nv$price_label_high==TRUE,1,0))
metrics_function_num(preds_nv_val, validation_nv, 'price_label_high')
```


# Evaluación

Tal y como se ha comentado en el apartado de aprendizaje supervisado, nuestro problema de clasificación busca alcanzar un compromiso entre recall y precission respecto a las casas caras.

A continuación se muestra una tabla de comparación de todos los modelos aplicados sobre el conjunto de validación según su F1 score. Para tener una información más completa a la hora de comparar, en la tabla también podemos ver las variables empleadas en cada modelo y el valor de los parámetros en caso de tenerlos.

```{r}
#Logistic regression
data_lr = validation
model_lr = readRDS("./models/lr_regularized.rds")
values_lr <- predict(model_lr, data_lr, type='response')
f1_lr = round(opt_f1_function(values_lr, data_lr, 'price_label_high')$f1_opt, 3)
var_lr = 'sell_rate_cat - bed_cat - car_cat - kmeans_cluster - may_have_water - lattitude_std'
p2_lr = opt_f1_function(values_lr, data_lr, 'price_label_high')$p2
p2_lr$data <- p2_lr$data %>% filter(metric == 'f1') %>%  mutate(model = 'lr')


#Árbol de decision
data_dt = housesVal
model_dt = readRDS("./models/dt_best_model.rds")
values_dt = predict(model_dt, data_dt, type='prob')
f1_dt = round(opt_f1_function(values_dt, data_dt, 'price_label_high')$f1_opt, 3)
val_dt = 'Longtitude - Rooms - Lattitude - Type'
parameters_dt = 'cp = 0.013981636'
p2_dt = opt_f1_function(values_dt, data_dt, 'price_label_high')$p2
p2_dt$data <- p2_dt$data %>% filter(metric == 'f1') %>%  mutate(model = 'dt')


#Random forest
data_rf = housesVal
model_rf = readRDS("./models/rf_best_model.rds")
values_rf = predict(model_rf, data_rf, type='prob')
f1_rf = round(opt_f1_function(values_rf, data_rf, 'price_label_high')$f1_opt, 3)
val_rf = 'Rooms - Distance - LandsizeImp - Lattitude - Longtitude - Type'
parameters_rf = 'ntree = 50 / mtry = 3 / nodesize = 11'
p2_rf = opt_f1_function(values_rf, data_rf, 'price_label_high')$p2
p2_rf$data <- p2_rf$data %>% filter(metric == 'f1') %>%  mutate(model = 'rf')


#SVM
data_svm = validation_svm
model_svm = readRDS("./models/svm_gaussian_kernel.rds")
values_svm <- predict(model_svm, data_svm, type = "prob")$.pred_1
f1_svm = round(opt_f1_function(values_svm, data_svm, "price_label_high")$f1_opt, 3)
val_svm = 'sqrt_distance_std - log_landsize_std - lattitude_std - longtitude_std - rooms_cat -      bath_cat - price_label_high'
parameters_svm = 'cost = 30.9136 / rbf_sigma = 0.1832'
p2_svm = opt_f1_function(values_svm, data_svm, 'price_label_high')$p2
p2_svm$data <- p2_svm$data %>% filter(metric == 'f1') %>%  mutate(model = 'svm')


#Naive Bayes
data_nv = housesVal
model_nv = readRDS("./models/nb_best_model.rds")
values_nv <- predict(model_nv, data_nv, type='raw')
f1_nv = round(opt_f1_function(values_nv, data_nv, 'price_label_high')$f1_opt, 3)
val_nv = 'sqrt_distance_std + Regionname + Type + rooms_cat + kmeans_cluster'
p2_nv = opt_f1_function(values_nv, data_nv, 'price_label_high')$p2
p2_nv$data <- p2_nv$data %>% filter(metric == 'f1') %>%  mutate(model = 'nv')



#KNN
data_knn = housesVal
num_std_cols = c('log_landsize_std', 'lattitude_std', 'longtitude_std', 'log_room_land_std')
pred_knn_val <- knn(train = housesTrain[,num_std_cols], test = housesVal[,num_std_cols], cl=housesTrain[,"price_label_high"], k = 3,prob = T)
values_knn <- reverse_probs(pred_knn_val)
f1_knn <- round(opt_f1_function(values_knn, data_knn, "price_label_high")$f1_opt, 3)
val_knn = 'log_landsize_std - lattitude_std - longtitude_std - log_room_land_std'
parameters_knn = 'k = 3'
p2_knn = opt_f1_function(values_knn, data_knn, 'price_label_high')$p2
p2_knn$data <- p2_knn$data %>% filter(metric == 'f1') %>%  mutate(model = 'knn')


#Tabla
dfMetricas <- data.frame(
  Modelos = c('Regresión logística', 'Árbol de decisión', 'Random forest', 'KNN', 'SVM','Naive Bayes'),
  F1 = c(f1_lr, f1_dt, f1_rf, f1_knn,f1_svm, f1_nv),
  Variables = c(var_lr, val_dt, val_rf, val_knn,val_svm, val_nv ),
  Parámetros = c('/', parameters_dt, parameters_rf, parameters_knn,parameters_svm, '/'))

dfMetricas %>% kable() %>% kable_styling(bootstrap_options = c('hover'), position = 'center') %>%  row_spec(3, bold = T, color = "white", background = palette34[1])

#Gráfica F1
concat = rbind(p2_lr$data, p2_dt$data, p2_rf$data, p2_knn$data, p2_svm$data, p2_nv$data)
concat %>% ggplot(aes(x=th,y=value,colour=model)) + geom_line() + scale_color_manual(values=c("#1B9E77","#D95F02","#7570B3","#66A61E","#E7298A","#E6AB02")) + ggtitle("F1 Score")+ theme(plot.title = element_text(hjust = 0.5))
```

La performance con un mayor F1 ha sido la que emplea el modelo *Random Forest*. Además, si atendemos a la explicabilidad del modelo, no es el que más variables emplea. Solamente supera en número de variables a los modelos de Naive Bayes y Árbol de decisión. Sin embargo, estos modelos tiene un F1 muy por debajo.

A continuación, vamos a probar el mejor modelo sobre el conjunto de test. 
```{r}
#Random forest
data_rf_test = housesTest
model_rf = readRDS("./models/rf_best_model.rds")
values_rf_test = predict(model_rf, data_rf, type='prob')



test_metrics <- val_metrics(model_rf, data_rf_test, c("Rooms", "Distance", "LandsizeImp",
                                    "Lattitude", "Longtitude", "Type"), "price_label_high")
test_metrics$metrics
test_metrics$roc_plot
test_metrics$opt_f1


predictions <- data.frame(price_label_high=housesVal$price_label_high, pred=values_rf_test[,2])
predictions %>% ggplot(aes(x=pred, fill=price_label_high)) + geom_histogram(alpha=0.5) + scale_fill_manual(values = palette34) + geom_vline(xintercept=test_metrics$opt_f1$threshold, linetype='dashed', color=palette34[4])
predictions %>% ggplot(aes(x=pred, color=price_label_high)) + geom_density(alpha=0.5) + scale_color_manual(values = palette34) + geom_vline(xintercept=test_metrics$opt_f1$threshold, linetype='dashed', color=palette34[4])
```
 
