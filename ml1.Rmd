---
title: "Machine Learning 1"
author: 
- name: Paula Santamaría Villaverde
- name: Manuel Jesús Pertejo Lope
- name: Carlos Gómez Sánchez
date: "21 de marzo de 2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      section_divs: true
    theme: "sandstone"
    highlight: "zenburn"
    code_folding: "hide"
---

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(cowplot)
library(scales)
library(tidyr)
library(InformationValue)
library(rpart)
library(rpart.plot)
library(e1071)
library(ROCR)
library(pROC)
library(gridExtra)
library(glmnetUtils)

set.seed(10)
```


```{r, echo = F}
# Función para el cáculo de las métricas
#Input necedario son las predicciones del modelo, el conjunto de datos y el nombre de la columna a predecir
metrics_function_num <- function(preds, test, label){
  accuracy = sum(preds == test[,label]) / nrow(test)
  recall = sum(preds == test[,label] & test[,label] == 1) / sum(test[,label] == 1)
  precision = sum(preds == test[,label] & test[,label] == 1) / sum(preds == 1)
  f1_score = 2*recall*precision/(recall+precision)
  specificity = sum(preds == test[,label] & test[,label] == 0) / sum(test[,label] == 0)
  metrics <- c(accuracy = accuracy, recall = recall, specificity = specificity, precision = precision, f1=f1_score)
  cm <- caret::confusionMatrix(preds, test[,label], positive = '1')
  return(list(metrics, cm))
}


# Funciones para dibujar matriz de confusión y curva ROC
#Input necesario es una confusionMatrix de caret
ggplotConfusionMatrix <- function(m){
  mytitle <- paste("Accuracy", percent_format()(m$overall[1]))
  p <-
    ggplot(data = as.data.frame(m$table) ,
           aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = log(Freq)), colour = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
    theme(legend.position = "none") +
    ggtitle(mytitle)
  return(p)
}

# Input necesario es un pROC object
ggplotROCCurve <- function(roc_object, interval = 0.2, breaks = seq(0, 1, interval)){
  require(pROC)
  if(class(roc) != "roc")
    simpleError("Please provide roc object from pROC package")
  ggroc(roc_object) + 
              scale_x_reverse(name = "Specificity",limits = c(1,0), breaks = breaks, expand = c(0.001,0.001)) + 
              scale_y_continuous(name = "Sensitivity", limits = c(0,1), breaks = breaks, expand = c(0.001, 0.001)) +
              geom_segment(aes(x = 0, y = 1, xend = 1,yend = 0), alpha = 0.01) + 
              theme_bw() + 
              theme(axis.ticks = element_line(color = "grey80")) +
              coord_equal() + 
              annotate("text",x=-Inf,y=-Inf, label = paste("AUC =",sprintf("%.3f",roc_object$auc)), hjust= 1.2, vjust = -7)
}


```


NOTA IMPORTANTE: Se han generado los datasets base_train y base_tes con:
- Variables originales
- Variable log_price
- Variables para clasificación (price_label con mediana y price_label_high con percentil 75)
- Variables generadas en Fundamentos para regresión lineal. Incluye variables numéricas estandarizadas (con _std) y numéricas discretizadas (con _cat)
- Variables imputadas son LandsizeImp, CarImp

Se recomienda leerlos cada vez que se vaya a probar un algoritmo nuevo.


# PCA
Se debe aplicar sobre datos numéricos que hayan sido estandarizados antes (deben estar en el mismo rango de valores). En una primera instancia, se va a probar a aplicar PCA sobre las variables numéricas sobre las que se trabajó en la parte de regresión: raíz cuadrada de la distancia, logaritmo de la parcela, latitud y longitud, todas ellas debidamente estandarizadas. Se observa que con las 3 primeras componentes, se captura cerca del 85% de la varianza de los datos.

```{r}
housesTrain <- read.csv('base_train.csv')
```

```{r}
housesNum <- housesTrain %>% select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std)
housesPCA <- prcomp(housesNum)
summary(housesPCA)
```


Probamos a lanzar un modelo de regresión lineal múltiple con las variables numéricas después de aplicar PCA. El objetivo de esto es comprobar si, con estas componentes (que son independientes entre sí, por lo que no habría correlación lineal entre ellas) se podría mejorar la regresión.
```{r}
train <- housesTrain %>% select(-Price)
```

```{r}
train_pca <- data.frame(housesPCA$x[,1:3])
train_cat <- train %>% select(Type, rooms_cat, Regionname, bath_cat, year_built_cat, sell_rate_cat, Method, car_cat)
train_pca_cat <- merge(train_pca, train_cat, by.x=0, by.y=0)
train_pca_cat$log_price <- train$log_price
```

Hemos conseguido que las variables de entrada sean independientes, pero estas nuevas variables no están correladas linealmente con la salida, por lo que el modelo no debería ser demasiado bueno.
```{r, message=FALSE}
train_pca_cat %>% select(PC1, PC2, PC3, log_price) %>%
    na.omit() %>%
    ggpairs(columns=1:4)
```

Efectivamente, estas variables no ayudan a la regresión, al estar completamente no correladas con la salida.

```{r}
lm_model <- lm(log_price~PC1 + PC2 + PC3 + Type + rooms_cat + Regionname + bath_cat + year_built_cat + sell_rate_cat + Method + car_cat, data = train_pca_cat)
summary(lm_model)
```

## Representación de PCA
Una de las aplicaciones más directas de PCA es la elección de las dos primeras componentes de la descomposición para representar el problema en dos dimensiones. Esto puede ayudar en tareas como clustering. Como primera aproximación, se va a realizar una descomposición de las variables numéricas distancia, tamaño de parcela, latitud y longitud, representando las dos primeras componentes, visualizando el precio como tercera variable.

```{r}
housesNum <- housesTrain %>% select(Distance, LandsizeImp, Lattitude, Longtitude)
housesPCA <- prcomp(housesNum, center = TRUE, scale = TRUE)
summary(housesPCA)
```

No se ve absolutamente nada, principalmente por los puntos apartados con un valor de PC2 más alto.
```{r}
df_houses_pca <- data.frame(housesPCA$x[,1:2])
df_houses_pca$price_label <- housesTrain$price_label_high
df_houses_pca %>% ggplot(aes(x=PC1, y=PC2, color=price_label)) + geom_point()
```

```{r}
df_houses_pca_fil<- df_houses_pca %>% filter(PC2<5 & PC1<5)
df_houses_pca_fil %>% ggplot(aes(x=PC1, y=PC2, color=price_label)) + geom_point()
```


Mapa de Melbourne :) (lo podemos poner el algún sitio)
```{r}
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=price_label_high)) + geom_point()
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Regionname)) + geom_point()
housesTrain %>% ggplot(aes(x=Longtitude, y=Lattitude, color=Type)) + geom_point()
```


# kNN
El siguiente algoritmo que se va a probar es kNN. Para ello, necesitamos que los datos estén de nuevo estandarizados, ya que es un método basado en distancias, y las variables necesitan estar en el mismo rango de valores.

PARA HACER: intentar probar otras distancias

```{r}
housesTrain <- read.csv('base_train.csv')
housesTest <- read.csv('base_test.csv')
# Para que funcione confusionMatrix de caret, el target debe ser un factor
housesTrain$price_label <- as.factor(housesTrain$price_label)
housesTrain$price_label_high <- as.factor(housesTrain$price_label_high)
housesTest$price_label <- as.factor(housesTest$price_label)
housesTest$price_label_high <- as.factor(housesTest$price_label_high)
```

```{r knn_functions}
# Función que calcula distintas métricas de clasificación
metrics_function <- function(prediccion, test, label){

  # Medidas de precisión
  
  accuracy = sum(prediccion == test[,label]) /nrow(test)
  error = 1-accuracy
  
  # Acierto sobre el total de las casas CARAS, sensitivity o recall
  sensitivity = sum(prediccion == test[,label] & test[,label] == TRUE) / sum(test[,label] == TRUE)
  recall = sensitivity
  
  # Acierto sobre el total de las casas BARATAS
  specificity =  sum(prediccion == test[,label] & test[,label] == FALSE) / sum(test[,label] == FALSE)
  
  # Acierto cuando el predicho es CARO
  precision = sum(prediccion == test[,label] & prediccion == TRUE) / sum(prediccion == TRUE)
  
  # Acierto cuando el predicho es BARATO
  npv = sum(prediccion == test[,label] & prediccion == FALSE) / sum(prediccion == FALSE)
  
  # F1_score
  f1score = 2*precision*recall /(precision+recall)
  conf_mat <- caret::confusionMatrix(table(prediccion, test[,label]), positive="TRUE")
  metrics <- c(accuracy = accuracy, error = error, sensitivity = sensitivity, specificity = specificity, precision = precision, npv = npv, f1=f1score)
  return(list(metrics, conf_mat))
  
}

perform_knn <- function(train, test, label, k, cols) {
  predictions <- knn(train[,cols], test[,cols], k=k, cl=train[,label])
  test_metrics <- metrics_function(predictions, test, label)
  return(test_metrics)
  
}
```


```{r knn1}
num_std_cols <- c('sqrt_distance_std', 'log_landsize_std', 'lattitude_std', 'longtitude_std')
metrics <- perform_knn(housesTrain, housesTest, 'price_label_high', 1, num_std_cols)
metrics
```

## k óptimo
ESTO HABRIA QUE HACERLO SOBRE VALIDACIÓN NO?
Para price_label:

```{r}
plot_acc_f1_k <- function(train, label, cols){
  long = 15
  accuracy = rep(0,long)
  f1score = rep(0,long)
  recall = rep(0,long)
  precision = rep(0,long)
  for (i in 1:long)
  {
    prediccion_knn_cv =knn.cv(train[,cols], 
                              k=i, cl=train[,label])
    accuracy[i] = sum(prediccion_knn_cv == train[,label]) /nrow(train)
    recall[i] = sum(prediccion_knn_cv == train[,label] & train[,label] == TRUE) / sum(train[,label] == TRUE)
    precision[i] = sum(prediccion_knn_cv == train[,label] & prediccion_knn_cv == TRUE) / sum(prediccion_knn_cv == TRUE)
    f1score[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])
  }
  resultados_knn = as.data.frame(cbind(accuracy,f1score,precision,recall))
  resultados_knn = resultados_knn %>% mutate(index=as.factor(seq(1:long)))
  
  max(resultados_knn$f1score)
  which.max(resultados_knn$f1score)
  
  
  p1 <- ggplot(data=resultados_knn,aes(x=index,y=accuracy)) + 
    geom_col(colour="cyan4",fill="cyan3")+
    ggtitle("Accuracy")
  
  
  p2 <- ggplot(data=resultados_knn,aes(x=index,y=f1score)) + 
    geom_col(colour="orange4",fill="orange3") +
    ggtitle("F1_score values")
  
  plot_grid(p1, p2, rel_heights = c(1/2, 1/2))
  }

plot_acc_f1_k(housesTrain, 'price_label', num_std_cols)
```
Para k=5 obtenemos un accuracy y un f1 score entre 0.75 y 0.8. Se observa que a medida que k va aumentando, las métricas no incrementan.

HABRIA QUE SACAR F1
```{r knn5}
# En train
prediccion_knn5_train =knn.cv(housesTrain[,num_std_cols], 
                              k=5, cl=housesTrain$price_label)
metrics_function(prediccion_knn5_train, housesTrain, 'price_label')

#En test
prediccion_knn5_test=knn(housesTrain[,num_std_cols], housesTest[,num_std_cols],
                         k=5, cl=housesTrain$price_label)
metrics_function(prediccion_knn5_test, housesTest, 'price_label')
```


Para price_label_high, vemos de nuevo que el f1-score es menor. Los mejores valores de k podrían ser 5 o 7.

```{r}
plot_acc_f1_k(housesTrain, 'price_label_high', num_std_cols)
```
```{r knn7high}
# En train
prediccion_knn7_train =knn.cv(housesTrain[,num_std_cols], 
                              k=7, cl=housesTrain$price_label_high)
metrics_function(prediccion_knn7_train, housesTrain, 'price_label_high')

#En test
prediccion_knn7_test=knn(housesTrain[,num_std_cols], housesTest[,num_std_cols],
                         k=7, cl=housesTrain$price_label_high)

metrics_function(prediccion_knn7_test, housesTest, 'price_label_high')
```

# Regresión logística

En este apartado, se va imlementar un modelo regresión logística sobre los datos de entrenamiento. 

```{r}
# Se creea la variable objetivo (75-25) y se eliminan los antiguos targets
train <- read.csv('base_train.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
train <- train %>%  select(-filter_cols)
train$price_label_high <- as.factor(ifelse(train$price_label_high==TRUE,1,0))


lr_model <- glm(price_label_high ~ ., family = binomial(link = 'logit'), data = train)
summary(lr_model)
```

El resultado óptimo para este modelo, en cuanto a optimización de la f1_score se refiere es la siguiente:

```{r}
beta <- 1
prob_values <- lr_model %>% predict(train[,1:13], type = "response")
pred_lr <- prediction(prob_values, train$price_label_high)
perf_lr <- performance(pred_lr, "prec", "rec")

f1_best_lr <- (1+beta^2)*perf_lr@y.values[[1]]*perf_lr@x.values[[1]]/(beta^2*perf_lr@y.values[[1]]+perf_lr@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_lr) # Mejor f1 score para este modelo
prec_lr_opt=perf_lr@y.values[[1]][optimo]
rec_lr_opt=perf_lr@x.values[[1]][optimo]
f1_measure_lr_opt <- (1+beta^2)*prec_lr_opt*rec_lr_opt/(beta^2*prec_lr_opt+rec_lr_opt)
threshold_optimo_lr <- perf_lr@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_lr_opt, precision = prec_lr_opt, recall = rec_lr_opt, threshold = threshold_optimo_lr))
```


```{r,message=F}
preds_train <- as.factor(ifelse(prob_values > threshold_optimo_lr ,1,0))
cm <- caret::confusionMatrix(preds_train, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(train$price_label_high,prob_values)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)
```

Como se puede observar, el valor no es muy alto, por lo que este resultado claramente puede mejorarse.

Si echamos un ojo a la columna de los p-valores obtenidos durante el entrenamiento del modelo, podemos ver que algunos de ellos nos indican que no se puede asegurar que los coeficientes asociados a ciertas variables sean diferentes de 0. Sin embargo, estos p-valores tan bajos van asociados a ciertos niveles de variables categóricas, por lo que si eliminásemos la variable estaríamos perdiendo información del resto de niveles, la cual sí que es relevante. 

Sin embargo, se podría aplicar alguna medida de regularización a través de la cual obtener más información sobre las posibles variables que no aportan demasiado al modelo y podrían ser eliminadas. A continuación, se va a utilizar una regresión logística Lasso y observar los coeficientes obtenidos para cada una de las variables:

```{r}
set.seed(123)
cv.lasso <- cv.glmnet(price_label_high~., data = train, alpha = 1, type.measure = "deviance", family = "binomial") # Con 10-fold cross-validation
#plot(cv.lasso)
coef(cv.lasso, cv.lasso$lambda.1se)
```

```{r}
lr_regularized <- glm(price_label_high ~. -sell_rate_cat - Method - bed_cat, family = binomial(link = 'logit'), data = train)
summary(lr_regularized)
```

```{r}
beta <- 1
prob_values <- lr_regularized %>% predict(train[,1:13], type = "response")
pred_lr <- prediction(prob_values, train$price_label_high)
perf_lr <- performance(pred_lr, "prec", "rec")

f1_best_lr <- (1+beta^2)*perf_lr@y.values[[1]]*perf_lr@x.values[[1]]/(beta^2*perf_lr@y.values[[1]]+perf_lr@x.values[[1]]) # Se calcula la f1 score para todos los posibles thresholds

optimo <- which.max(f1_best_lr) # Mejor f1 score para este modelo
prec_lr_opt=perf_lr@y.values[[1]][optimo]
rec_lr_opt=perf_lr@x.values[[1]][optimo]
f1_measure_lr_opt <- (1+beta^2)*prec_lr_opt*rec_lr_opt/(beta^2*prec_lr_opt+rec_lr_opt)
threshold_optimo_lr <- perf_lr@alpha.values[[1]][optimo+1]
print(c(f1_opt= f1_measure_lr_opt, precision = prec_lr_opt, recall = rec_lr_opt, threshold = threshold_optimo_lr))
```

```{r, message=F}
preds_train <- as.factor(ifelse(prob_values > threshold_optimo_lr ,1,0))
cm <- caret::confusionMatrix(preds_train, train$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_gaussian <- roc(train$price_label_high,prob_values)
roc_plot <- ggplotROCCurve(roc_gaussian)


grid.arrange(roc_plot, cm_plot, ncol=2)
```

Vemos que, al eliminar ciertas variables que simplemente estaban introduciendo información redundante al modelo, el rendimiento obtenido es prácticamente el mismo (incluso mejora muy mínimamente).
  
```{r}

probs_labels <- data.frame("probs" = prob_values, "labels" = train$price_label_high)

probs_labels %>% ggplot(aes(x=probs, color=labels)) +
  geom_density() + geom_vline(aes(xintercept=threshold_optimo_lr),
            color="blue", linetype="dashed")


```

A continuación, se procede a comprobar el funcionamiento de este modelo de regresión lineal sobre los datos de test:

```{r}
test<- read.csv('base_test.csv', encoding = 'UTF-8')
filter_cols <- c('Suburb', 'Address', 'Rooms', 'Distance', 'Lattitude', 'Longtitude', 'SellerG',  'Date', 'Postcode', 'Bedroom2', 'Bathroom', 'YearBuilt', 'CouncilArea', 'CarImp' ,'LandsizeImp', 'Price', 'log_price', 'price_label')
test <- test %>%  select(-filter_cols)
test$price_label_high <- as.factor(ifelse(test$price_label_high==TRUE,1,0))


x_test <- test[,13]
prob_values_test <- lr_regularized %>% predict(test[,1:13], type = "response")
preds_test <- as.factor(ifelse(prob_values_test > threshold_optimo_lr ,1,0))

metrics_function_num(preds_test, test, 'price_label_high')
```


```{r, message=F}

cm <- caret::confusionMatrix(preds_test, test$price_label_high, positive = '1')
cm_plot <- ggplotConfusionMatrix(cm)

roc_lr_test <- roc(test$price_label_high,prob_values_test)
roc_plot <- ggplotROCCurve(roc_lr_test)


grid.arrange(roc_plot, cm_plot, ncol=2)
```


Se puede comprobar que el AUC se mantiene intacto. Si bien es cierto que la accuracy y la f1_score disminyen ligeramente, se puede afirmar que la regresión no sobreajusta para nada los datos.



# Árboles de decisión
Se van a probar árboles de decisión sobre nuestro conjunto de datos. En primer lugar, se utilizarán las variables que se consideraron importantes en el análisis exploratorio, para después probar distintas formas de tuning e intentar evitar el sobreaprendizaje del árbol.

## Utilizando directamente las variables estudiadas
Se utilizarán directamente las variables estudiadas en el análisis exploratorio, dejando de lado transformaciones y estandarizaciones, además de variables categóricas con muchas categorías. Además, si recordamos, para la regresión lineal tuvimos que convertir varias features numéricas en categóricas (número de habitaciones, año de construcción...). Podemos probar a introducir las variables originales para ver qué cortes propone el árbol.


### Con la variable price mediana

Como se puede ver con el árbol, la principal división se realiza por el tipo de vivienda (en este caso unifamiliar, se puede entender que separa casas grandes de pequeñas). Si la casa es u (pequeña), se clasificaría con buena tasa el 22% de los datos (92% de las veces serían 0, casas baratas). Por el otro lado del árbol, separa por localización (si se observa el mapa, tiene bastante sentido, las casas caras aparecen en el Noreste). Habría unos nodos con más fallos. Además, la última división por distancia al centro, nos deja un nodo con el 50% de la población, de los cuales el 84% se clasifican como casas caras, mientras que nos deja un nodo con solo un 3%. Esto quiźas se podría mejorar parando antes.



```{r load_variables}
housesTrain <- read.csv('base_train.csv')
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label")]
n = dim(housesTrainOrigVar)[1]
# Hay que factorizar el label para que se construya bien el árbol
housesTrainOrigVar$price_label <- as.factor(housesTrainOrigVar$price_label)
```

```{r tree1_price_label}
housesTree1 = rpart(price_label ~ ., data = housesTrainOrigVar)
rpart.plot(housesTree1)
#summary(housesTree1)
```

```{r metricsTree1}
preds <- predict(housesTree1, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label)
metrics_function(preds, housesTrainOrigVar, 'price_label')
```



### Podando el árbol 
Queremos quedarnos con el árbol más pequeño que tenga el menor error de cross validation. Nos quedamos con 4 hojas, aumentando un poco el error.

```{r}
printcp(housesTree1)
```
```{r}
plotcp(housesTree1)
```

```{r}
#pruneTREE1 = prune(housesTree1, cp = housesTree1$cptable[which.min(housesTree1$cptable[,"xerror"]), "CP"])
pruneTREE1 = prune(housesTree1, cp = housesTree1$cptable[4,"CP"])
rpart.plot(pruneTREE1)
```
Pondando el último nodo, se consiguen unas métricas ligeramente inferiores en train.

```{r metricsPruneTree1}
preds <- predict(pruneTREE1, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label)
metrics_function(preds, housesTrainOrigVar, 'price_label')
```


### Con la variable price 75%

Al aumentar el corte de la distinción entre barato y caro, observamos que el accuracy mejora ligeramente, pero el f1-score empeora (de 0.83 a 0.69). Esto se debe a la caída del recall (positivos acertados respecto al total de positivos reales), y es lógico, ya que ahora hay menos positivos, y es más difícil diferenciar entre las clases. 

En los cortes ahora se tiene en cuenta el número de habitaciones como corte principal (de nuevo diferenciando entre casas pequeñas y grandes). El siguiente corte se realiza en base a la región (de nuevo localización, estaría bien saber las regiones porque no se ve...). Parece que la región por la que se discrimina es Southern Metropolitan. Si la casa es pequeña y está en cualquier región que no sea esa, se clasifica como barata la mayoría de las veces. REVISAR!!

Algo que también se puede ver es que hay varios nodos con pocas muestras (los que vienen de la distinción de tipo de casa es Southern Metropolitan). Vemos además que hay un montón de falsos negativos (casas caras que se etiquetan como baratas).

```{r load_variables_high}
housesTrainOrigVar <- housesTrain[c("Rooms", "Type", "Method", "Distance", "Bathroom", "CarImp", "LandsizeImp", 
                                    "Lattitude", "Longtitude", "Regionname", "year_built_cat", "sell_rate_cat",
                                     "price_label_high")]
n = dim(housesTrainOrigVar)[1]
# Hay que factorizar el label para que se construya bien el árbol
housesTrainOrigVar$price_label_high <- as.factor(housesTrainOrigVar$price_label_high)
```

```{r tree2_price_label}
housesTree2 = rpart(price_label_high ~ ., data = housesTrainOrigVar)
rpart.plot(housesTree2, type=4, fallen.leaves = FALSE, tweak =1.75)
rpart.rules(housesTree2, clip.facs = TRUE)
```

```{r metricsTree2}
preds <- predict(housesTree2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high')
```


### Podando el árbol 
Intentamos eliminar esas últimas hojas que hemos comentado. Prácticamente no habría diferencia entre tener 7 hojas u 8. Podemos probar a ver si con 4 hojas el error aumenta mucho.

```{r}
printcp(housesTree2)
```
```{r}
plotcp(housesTree2)
```

Con 7 hojas, el error es prácticamente el mismo. Incluso mejora un poco el f1-score.

```{r pruneTree2}
pruneTREE2 = prune(housesTree2, cp = housesTree2$cptable[4,"CP"])
rpart.plot(pruneTREE2, type=4, fallen.leaves = FALSE, tweak =1.75)
```

```{r metricsPruneTree2}
preds <- predict(pruneTREE2, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high')
```


Bajando la complejidad hasta quedarnos solo con 4 hojas, observamos que el error aumenta en un 4% (hasta el 17%) y que el f1-score disminuye hasta 0.55, pero el árbol es mucho más fácil de interpretar. Las casas con menos de 4 habitaciones directamente son una hoja, con el 75% de los datos, y con una tasa de acierto aceptable (el 84% serían baratas). Las casas con más de 4 habitaciones en Southern Metropolitan (son solo el 9%) se clasifican como caras el 86% de las veces. La última división es en función de la distancia al centro, y ya falla más.

```{r pruneTree3}
pruneTREE3 = prune(housesTree2, cp = housesTree2$cptable[3,"CP"])
rpart.plot(pruneTREE3, type=4, fallen.leaves = FALSE, tweak =1)
```

```{r metricsPruneTree3}
preds <- predict(pruneTREE3, type = "class")
table(pred = preds, obs = housesTrainOrigVar$price_label_high)
metrics_function(preds, housesTrainOrigVar, 'price_label_high')
```


# Clustering

## k-means

```{r}
housesTrain <- read.csv('base_train.csv')
```

NOTA: Habría que revisar estas variables con el nuevo cambio de dataset. Además, lo de las siluetas a veces me peta

```{r}
#Antes:
#housesTrainKmeans <- housesTrain %>% dplyr::select(-Regionname, -Type, -Method, -year_built_cat)
housesTrainKmeans <- housesTrain %>% dplyr::select(sqrt_distance_std, log_landsize_std, lattitude_std, longtitude_std, rooms_cat, car_cat, bath_cat, bed_cat, sell_rate_cat)

#las variables categóricas pasan a ser factores ordenados
housesTrainKmeans['rooms_cat']<- housesTrain %>% pull('rooms_cat') %>% 
  ordered(levels = c('Pequeñas', 'Medianas', 'Grandes')) %>% as.numeric()

housesTrainKmeans['car_cat'] <- housesTrain %>% pull('car_cat') %>% 
  ordered(levels=c("Pocas_plazas","Muchas_plazas")) %>% as.numeric()

housesTrainKmeans['bath_cat'] <-housesTrain %>% pull('bath_cat') %>% 
  ordered(levels = c('Pocos_baños', 'Muchos_baños')) %>% as.numeric()

housesTrainKmeans['bed_cat'] <-housesTrain %>% pull('bed_cat') %>% 
  ordered(levels = c('Pocos_dormitorios', 'Muchos_dormitorios')) %>% as.numeric()

housesTrainKmeans['sell_rate_cat'] <-housesTrain %>% pull('sell_rate_cat') %>% 
  ordered(levels = c('Menos_populares', 'Más_populares')) %>% as.numeric()
```

```{r kmeans}
#nume <- housesTrainKmeans %>% dplyr::select(sqrt_distance, log_landsize, Lattitude, Longtitude, Price, log_price)

fac = as.matrix(housesTrainKmeans)

#Guardamos el número filas
n <- nrow(fac)

#Probamos un cluster de 2 grupos
clusters2=kmeans(fac,centers=2,nstart=25)

#Para ver la variabilidad dentro de los grupos correspondiente a esos dos clusters
clusters2$withinss

#Buscamos el número de clusters óptimo 

#Inicializamos el vector
SSW <- vector(mode = "numeric", length = 15)
#Variabilidad de todos los datos, es decir, todos los datos como un único cluster
SSW[1] <- (n - 1) * sum(apply(X = fac, MARGIN = 2, FUN = 'var'))
#Variabilidad de cada modelo, desde 2 clusters hasta 15 clusters
for (i in 2:15) SSW[i] <- sum(kmeans(fac,centers=i,nstart=25)$withinss)
#Dibujamos un gráfico con el resultado
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within
groups",pch=19, col="steelblue4")

#Otra manera de encontrar el número de clusters óptimo: silhouette coefficient 
#This coefficiente ranges from  −1  to  1 , so that:
  #Near +1 indicate that the sample is far away from the neighboring clusters.
  #A value of 0 indicates that the sample is on or very close to the decision boundary     between two neighboring clusters and
  #Negative values indicate that those samples might have been assigned to the wrong       cluster.

fviz_nbclust(fac, kmeans, method='silhouette')

avg_sil <- function(k) {
  km.res <- kmeans(fac, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(fac))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- sapply(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```

Según las tres gráficas obtenidas, el k óptimo es 2. (dudo??)



